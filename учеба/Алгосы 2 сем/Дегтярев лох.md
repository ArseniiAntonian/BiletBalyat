# 1. Поразрядная сортировка MSD

## **Суть поразрядной сортировки:**

Алгоритмы поразрядной сортировки рассматривают ключи, как числа, которые представлены в системе счисления с основанием R и работают с отдельными цифрами чисел

### MSD (most significant digit radix sort) - поразрядная сортировка по старшей цифре.

Идея состоит в том, чтобы выполнить следующие шаги для каждой цифры **i**, где значение **i** варьируется от старшей цифры до младшей:
- Храните элементы в разных корзинах в соответствии с их **i-м** разрядом.
- сортируйте каждую ячейку, содержащую более одного элемента.
- ![[Pasted image 20240611215607.png]]
https://youtu.be/bg-qnrQS82I?si=fuHn5zn2x3qZCY1z
### Проблемы настройки

- **если R принимает чрезмерно большое значение, то большая часть стоимости сортировки приходится на инициализацию и проверку корзин;**
- **если R недостаточно велико, то метод не использует своих потенциальных выгод, что достигается, если разделить исходный файл на максимально возможное число фрагментов.**

### Проблема пустых корзин
При сортировке случайных ключей количество ключей в каждом контейнере (размер подфайлов) после первого прохода в среднем будет равно N/R. На практике ключи могут не быть случайными (например, если ключи — это строки, представляющие собой слова на русском языке, то мы знаем, что лишь немногие из них начинаются с буквы й и совсем нет слов, начинающихся с буквы ъ), так что многие контейнеры окажутся пустыми, а некоторые из непустых контейнеров будут содержать больше ключей, чем остальные.
Пути решения:
- **Эвристика в масштабах корзины (bin-span-heuristics) - 
	- **First Fit (Первое подходящее)**:
	    - Элементы рассматриваются последовательно, и каждый элемент помещается в первый контейнер, который может его вместить во всех измерениях. Этот метод прост в реализации, но не всегда находит оптимальное решение.
	- **Best Fit (Лучшее подходящее)**:
	    - Элементы рассматриваются последовательно, и каждый элемент помещается в контейнер, который оставляет наименьшее свободное пространство после его добавления. Этот метод более вычислительно затратен, но часто дает лучшие результаты по сравнению с First Fit.
	- **Next Fit (Следующее подходящее)**:
	    - Элементы рассматриваются последовательно, и каждый элемент помещается в текущий контейнер, если он подходит, иначе открывается новый контейнер. Этот метод ограничивает количество открытых контейнеров, что может быть полезно в некоторых приложениях.**
- **Разработка более сложной реализации абстрактной операции доступа к конкретным байтам, которая учитывает любые специальные знания о сортируемых строках.**

### Сложность

Пусть значения разрядов меньше b, а количество разрядов — k. При сортировке массива из одинаковых элементов MSD-сортировкой на каждом шаге все элементы будут находится в неубывающей по размеру корзине, а так как цикл идет по всем элементам массива, то получим, что время работы MSD-сортировки оценивается величиной **O(nk)**, причем это время нельзя улучшить. Хорошим случаем для данной сортировки будет массив, при котором на каждом шаге каждая корзина будет делиться на b частей. Как только размер корзины станет равен 1, сортировка перестанет рекурсивно запускаться в этой корзине. Таким образом, асимптотика будет Ω(n logb(n)). Это хорошо тем, что не зависит от числа разрядов.

По памяти O(k), по сути O(n)
### **Условие поразрядной сортировки:**

Основное условие поразрядной сортировки состоит в том, чтобы мы могли разделить ключ, строку, последовательность байтов, которые мы могли бы сравнивать по отдельности, то есть выбирать какие-то определенные части и сравнивать их.

### **Абстракции для работы поразрядной сортировки (с чем она может работать):**

1. Байт - последовательность битов фиксированной длины

2. Строка - последовательность байтов переменной длины

3. Слово - последовательность байтов фиксированной длины

4. Ключ - число в системе счисления с основанием R, цифры которого пронумерованы. (организованы как последовательности байтов)

Все эти абстракции можно разделить на части, которые можно сравнивать

### Частный случай: двоичная поразрядная сортировка:

1. У каждого элемента изначально имеется ключ в двоичной системе счисления, выбираем самый большой разряд, по нему будем сортировать
2. Где единица - вниз, они больше, с 0 вверх, они меньше
3. Получили два блока, далее в каждом блоке берем второй разряд, он будет страшим и уже к каждому из блоков рекурсивно применяем алгоритм сортировки

Суть: Делим на 2 блока, с нулями и единицами в старшем разряде и рекурсивно сортируем по старшему разряду, пока не дойдем до конца ключа

Пример:
![[Pasted image 20240611222130.png]]

Суть: модификация двоичной поразрядной сортировки, только вместо двоичной СС, у нас СС, с основанием R, получается, что наш алгоритм делит на k частей, называемых корзинами и затем в каждой из этих корзин сортирует рекурсивно также разделяя на k частей, пока не дойдет до конца

### Преимущества и недостатки:

- Хорошо параллелится, при этом даже без этого является очень быстрой
- Требует дополнительную память и не всегда работает. Например, на знаковых числах (просто для разных знаков сортировать - неэффективно). Также не очень эффективен, когда много одинаковых элементов
# 2. Трех-путевая поразрядная быстрая сортировка
### Постановка задачи:

Допустим, мы выполняем сортировку строк.

Использовать qsort, который активно сравнивает элементы, выглядит слишком накладным — сравнение строк операция долгая. Да, мы можем написать свой компаратор, который будет несколько эффективнее. Но все же.

Использовать radix, который требует дополнительную память, тоже не слишком мотивирует — строки могут быть большими. Да и большая длина строк, т.е. число разрядов, удручающе сказываются на эффективности.
### Основная идея

**Приспособить быструю сортировку для MSD, используя трехпутевое разделение ключей по старшим байтам с переходом к следующему байту только в среднем подфайле.**

Это комбинация быстрой и поразрядной сортировки.

1. Берем опорный элемент.
2. Разделяем массив на три части, сравнивая элементы с опорным по старшему разряду — на меньшие, равные и большие.
3. В каждой из трех частей процедуру повторяем, начиная с шага №1, до тех пор, не дойдем до пустых частей или частей с 1 элементом.

Только в средней части (т.е. где старший разряд равен старшему разряду опорного элемента) переходим к следующему разряду. В остальных частях операция начинается без изменения «рабочего» разряда.

Пример:
![[Pasted image 20240611222618.png]]

### Сложность:

По сути работает схожим образом, как и быстрая сортировка, по методу “разделяй и властвуй”, поэтому сложность:

**Сложность сортировки — O(n*logn).**

**Дополнительная память — O(1).**

### Преимущества:

- По сравнению с быстрой в том, что нам не нужно выполнять сравнение каждого элемента целиком, что естественно ускоряет процесс
- По сравнению с поразрядной - не требует дополнительной памяти

### Недостатки:

- Более сложна в написании, чем быстрая сортировка
- Хуже параллелится по сравнению с поразрядной сортировкой 
# 3. Поразрядная сортировка LSD
### LSD (least significant digit radix sort) - поразрядная сортировка сначала по младшей цифре.

**Альтернативный метод поразрядной сортировки
› Сортируем по последней букве (используем метод подсчета индексных ключей)
› Сортируем по средней букве


**Работает только в том случае, если доказана устойчивость метода сортировки**

Идея: сортировать элементы не от большего разряда к меньшему, а наоборот

### Алгоритм:

1. Берем последние разряды чисел и поводим сортировку по ним, любой сортировкой, главное, чтобы она была устойчивой(наиболее эффективно, конечно, будет использовать сортировку подсчетом. Именно для нее будет потом рассчитана сложность)
2. Переходим к предпоследнему элементу, также сортируем
3. Производим операции до тех пор, пока не дойдем до конца.

### Почему сортировка должна быть устойчивой?

Если значения разрядов чисел не равны между собой, то элементы могут перемещаться между группами на каждом шаге. Поэтому при реализации LSD sort необходимо использовать стабильную сортировку внутри каждой группы.

Пояснение:

Предположим, что мы используем, неустойчивую сортировку, тогда элементы не сохраняют относительный порядок. Из этого следует, что относительный порядок, который мы задали, никак не будет задействован, из чего следует, что элементы, у которых, все одинаковы разряды, кроме, например, последнего, стоят так, что не удовлетворяют отсортированности, из чего можно сделать вывод, что сортировка не работает

### Сложность

Пусть m — количество разрядов, n — количество объектов, которые нужно отсортировать, T(n) — время работы устойчивой сортировки. Цифровая сортировка выполняет k итераций, на каждой из которой выполняется устойчивая сортировка и не более O(1) других операций. Следовательно время работы цифровой сортировки — O(kT(n)).

Рассмотрим отдельно случай сортировки чисел. Пусть в качестве аргумента сортировке передается массив, в котором содержатся n m-значных чисел, и каждая цифра может принимать значения от 0 до k−1. Тогда цифровая сортировка позволяет отсортировать данный массив за время O(m(n+k)), если устойчивая сортировка имеет время работы O(n+k). Если k небольшое, то оптимально выбирать в качестве устойчивой сортировки сортировку подсчетом.

Если количество разрядов — константа, а k=O(n), то сложность цифровой сортировки составляет O(n), то есть она линейно зависит от количества сортируемых чисел.

### Преимущества и недостатки:

1. Устойчива, довольно быстрая
2. Требует дополнительные затраты памяти и не всегда работает или делает это неэффективно(аналогично MSD)
# 4. Графы и их разновидности
### Граф - совокупность узлов и ребер, соединяющих эти узлы, как структура данных.

### Разновидности графов:

- **Неориентированный/ориентированный**

	G=(V,E) – неориентированный, если из (x,y)∈E следует, что (y,x) также является членом E. В противном случае граф – ориентированный.

- **Взвешенный/Невзвешенный**

	Каждому ребру/вершине взвешенного графа G присваивается числовое значение или вес.

- **Простые/Сложные**
	![[Pasted image 20240611223823.png]]
- **Разреженные/Плотные**
	![[Pasted image 20240611223930.png]]

- **Циклические/Ациклические**
	![[Pasted image 20240611224040.png]]

- **Явные/Неявные**
	![[Pasted image 20240611224112.png]]

- **Вложенные/Топологические**
	![[Pasted image 20240611224133.png]]

- **Помеченные/Непомеченные**
	![[Pasted image 20240611224258.png]]
- **Дерево**
	Дерево - любой связанный граф, не содержащий циклов
	
	Связанный граф - граф, между любой парой вершин которого существует хотя бы 1 путь
# 5. Обход графов, раскраска графов
## Обход графов
### **Обход графа - систематизированное посещение каждой вершины и каждого ребра графа.**

Каждая из вершин находится в одном из состояний: 
- Неоткрытая – первоначальное, нетронутое состояние вершины
- Открытая – вершина обнаружена, но не проверены все ее ребра
- Обработанная – все инцидентные данной вершине ребра были посещены

### Обход в ширину (breadth-first search, BFS)

Идея: берем, изначально, какую-нибудь вершину, помечаем ее. Далее мы берем для этой вершины все вершины, смежные с ней. Заходим в каждую. Если были в ней, то ничего не делаем, если в ней не были, то берем все вершины, смежные с данной. Далее то же самое повторяем, для вершин, которые взяли (т.е. мы каждый раз берем все смежные вершины данной, тем самым каждый раз расширяемся в ширину).

### **Обход в глубину (depth-first search, DFS)**

Идея: выбрать изначально любую вершину графа, и идти по какому-нибудь одному пути, помечая при этом все вершины, в которых были, пока не дойдем до вершины, которую уже посетили, или до вершины, из которой нельзя попасть в другие вершины. (т.е. постоянно спускаемся в глубь). Затем поднимаемся на одну вершину вверх, от той, которая оказалась конечной и повторяем то же самое

Замечание: оба алгоритма являются взаимозаменяемыми, но иногда какой-то из них использовать более выгодно чем другой.

## Раскраска графов

пусть G - некоторый граф, k - натуральное число. тогда раскраской графа называется функция f, которая каждой вершине графа G ставит в соответствие определенные номер {1,…, k}

Раскраска называется правильной, если любым двум смежным вершинам не соответствует одно и то же число, при этом k - минимальное число для этого графа.

_Пояснение:_ раскраска будет правильной, если у нас смежные вершины не раскрашены в один и тот же цвет, при этом мы задействовали минимальное количество цветов.(граф помечен определенным числом, можно визуализировать как граф раскрашен в определенный цвет)
![[Pasted image 20240611224857.png]]
## Двудольная раскраска графов

Задача: раскрасить граф в 2 цвета

Заметим, что если такая раскраска существует, и если зафиксировать цвет одной вершины, то все цвета всех достижимых из неё вершин определяются однозначно: пусть цвет этой вершины белый, тогда все её соседи будут иметь черный цвет, все вершины на расстоянии 2 будут иметь снова белый цвет, все вершины на расстоянии 3 снова черный, и так далее.

Раскрашивать граф можно обходом в глубину. На этот раз наш DFS будет принимать параметром цвет, в который нужно покрасить вершину, и он будет рекурсивно запускаться от всех соседей, крася их в противоположный цвет. По окончании работы алгоритма мы либо обнаружим, что граф не двудолен (мы когда-то посмотрели на две соседние вершины, которым нужно присвоить один и тот же цвет), либо найдём разбиение вершин графа на две доли.
**Пример**
 - Предположим, что в ВУЗе есть N преподавателей. Каждый должен прочитать определенное количество лекций по своему предмету, притом разные преподаватели могут читать свои лекции параллельно, один преподаватель так не может. Каждая лекция пусть занимает 1 час.
- Задача: составить расписание лекций так, чтобы они были прочтены для всех групп за минимальное время (т.е. по сути были прочтены за минимальное количество дней, значит надо сделать так, чтобы было как можно меньше окон)
- Пусть каждая лекция - это вершина графа, смежные вершины те, которые читает один и тот же преподаватель, смежные вершины раскрашиваем в разные цвета. Получаем некоторую раскраску, при этом, если вершины имеют 1 раскраску, то они могут читаться параллельно, значит их можно поставить в расписание на 1 время, если сделаем правильную раскраску, то по сути найдем максимальное количество лекций, которые можно ставить параллельно, при этом получим, что если мы так расставим лекции, то все лекции будут прочтены за минимальное количество часов

подгон от Коли:

[](https://www.youtube.com/watch?v=qoIEi0KA5Xc&t=336s&ab_channel=%D0%A3%D1%87%D0%B8%D0%BC%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%D0%B4%D0%B8%D1%81%D0%BA%D1%80%D0%B5%D1%82%D0%BD%D0%BE%D0%B9%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B8)[https://www.youtube.com/watch?v=qoIEi0KA5Xc&t=336s&ab_channel=Учималгоритмыдискретнойматематики](https://www.youtube.com/watch?v=qoIEi0KA5Xc&t=336s&ab_channel=%D0%A3%D1%87%D0%B8%D0%BC%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%D0%B4%D0%B8%D1%81%D0%BA%D1%80%D0%B5%D1%82%D0%BD%D0%BE%D0%B9%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B8)
# 6. Алгоритм обхода в ширину по сравнению с обходом в глубину
### **Обход в ширину (breadth-first search, BFS)**

**Идея**: берем, изначально, какую-нибудь вершину, помечаем ее. Далее мы берем для этой вершины все вершины, смежные с ней. Заходим в каждую. Если были в ней, то ничего не делаем, если в ней не были, то берем все вершины, смежные с данной. Далее то же самое повторяем, для вершин, которые взяли (т.е. мы каждый раз берем все смежные вершины данной, тем самым каждый раз расширяемся в ширину).

Получается, что при каждой операции мы проверяем, является ли вершина посещенной и, если нет, то собираем все вершины, смежные ей, и даем им самый низкий приоритет. Далее берем еще одну, с нее собираем смежные и даем им самый низкий приоритет, значит приоритет вершин, добавленных до этого надо увеличить. Таким образом, мы приходим к выводу, что для реализации обхода в ширину мы будем использовать структуру данных - очередь, в отличие от обхода в глубину, где мы использовать будем стек.

**Очередь** - обход в ширину. Помещая вершины в очередь типа FIFO, мы исследуем самые старые неисследованные вершины первыми. Таким образом, наше исследование медленно распространяется вширь, начиная от стартовой вершины.

### Итеративный алгоритм

1. Выбираем вершину и кладем ее в очередь
2. Достаем из очереди вершину, помечаем ее как открытую
3. Если вершина изначально не была открытой, то кладем в очередь все вершины, смежные ей
4. Повторяем шаги 2, 3 до тех пор, пока очередь не окажется пустой

### Свойства при обходе в ширину

- _Сохранение порядка, в котором мы открывали вершины графа, и пути позволяет определить минимальный путь от исходной вершины до любой заданной (поскольку у каждой вершины есть только один родитель)._
- Ребра графа, которые не включены в дерево обхода в ширину, также имеют особые свойства. Для неориентированных графов, не попавшие в дерево ребра могут указывать только на вершины на том же уровне, что и родительская вершина, или на вершины, расположенные на уровень ниже. Эти свойства естественно следуют из того факта, что каждое ребро в дереве должно быть кратчайшим путем в графе (есть вершина, если с вершиной, смежной с ней, изначально нет ребра в дереве вершин, то тогда значит, что смежная вершина или на том же уровне с деревом или на 1 уровень ниже).
- Для ориентированных графов ребро (u, v), указывающее в обратном направлении, может существовать в любом случае, когда вершина v расположена ближе к корню, чем вершина u.(у нас есть ребро (u,v). Если v ближе к корню чем u, то ребро (u, v) обязательно может существовать, хотя это не гарантируется)

### Преимущества обхода в ширину, по сравнению с обходом в глубину

- Одним из основных преимуществ обхода в ширину по сравнению с обходом в глубину является то, что он может быть использован для поиска кратчайшего пути в невзвешенном графе. Обход в ширину гарантирует, что кратчайший путь будет найден, если все ребра имеют одинаковый вес. Кроме того, обход в ширину может быть более эффективным при работе с графами, у которых много ребер, но мало уровней.
- Еще одним преимуществом обхода в ширину является то, что он может быть использован для поиска компонент связности в неориентированном графе. Обход в ширину позволяет найти все вершины, которые достижимы из заданной вершины, и таким образом определить компонент связности (максимального связного подграфа).
- Кроме того, обход в ширину может быть более удобным для работы с графами, у которых нет определенной структуры или которые содержат циклы. Обход в ширину позволяет посетить все вершины графа, не застревая в циклах или повторно посещая вершины.
# 7. Алгоритм обхода в глубину по сравнению с обходом в ширину
### **Обход в глубину (depth-first search, DFS)**

**Идея**: выбрать изначально любую вершину графа, и идти по какому-нибудь одному пути, помечая при этом все вершины, в которых были, пока не дойдем до вершины, которую уже посетили, или до вершины, из которой нельзя попасть в другие вершины. (т.е. постоянно спускаемся вглубь). Затем поднимаемся на одну вершину вверх, от той, которая оказалась конечной и повторяем то же самое.

Получается, что мы берем одну вершину, далее, собираем все смежные вершины и даем им максимальный приоритет, переходи-м в одну из них, если не открытая, то снова собираем все смежные вершины и даем им максимальный приоритет. Тем самым приходим к выводу, что нам необходимо использовать стек (или рекурсию, как замену ему)

**Стек** – обход в глубину. Помещая вершины в стек с порядком извлечения LIFO, мы исследуем их, отклоняясь от пути для посещения очередного соседа, и возвращаясь назад, только если оказываемся в окружении ранее открытых вершин. Таким образом, мы в своем исследовании быстро удаляемся от стартовой вершины.

### Итеративный алгоритм

1. Берем вершину и кладем (!!!) ее в стек
2. Вытаскиваем из стека и отмечаем, как открытую
3. Если до этого вершина была закрытой, то кладем в стек все вершины, смежные с ней
4. Повторяем шаги 2-3 до тех пор, пока стек не будет пуст

### Рекурсивный алгоритм

1. берем первую вершину, и запускаем для нее функцию проверки на открытость
2. Если вершина открытая, то завершаем, если нет, то помечаем, как открытую и для каждой вершины, смежной ей запускаем рекурсивно ту же функцию
3. Алгоритм завершится, когда стек рекурсий будет пуст

### Свойства при обходе в глубину

- Посещение предшественника. Если вершина х является предшественником вершины у в дереве обхода в глубину, то временной интервал посещения у должен быть корректно учтен его предшественником
- Количество потомков. Разница во времени выхода и входа для вершины у свидетельствует о количестве потомков этой вершины в дереве обхода в глубину.

**Обход в глубину разбивает ребра на два класса:**

- древесные (tree edges) - используются при открытии новых вершин и закодированы в родительском отношении
- обратные (back edges) - второй конец является предшественником расширяемой вершины

### Преимущество обхода в глубину по сравнению с обходом в ширину

- Один из главных преимуществ обхода в глубину по сравнению с обходом в ширину заключается в том, что он использует меньше памяти. В обходе в ширину необходимо хранить все вершины на текущем уровне, а также все вершины на предыдущих уровнях, что может потребовать значительных объемов памяти при работе с большими графами. В обходе в глубину же используется стек для хранения вершин, что позволяет эффективно использовать память и работать с графами большого размера.
- Кроме того, обход в глубину может быть более эффективен при поиске определенных типов путей или структур в графе, таких как циклы, деревья или компоненты связности. Обход в глубину также может быть более простым и интуитивно понятным для реализации, особенно для начинающих программистов.
# 8. Обход ориентированных графов, топологическая сортировка
## Обход ориентированных графов

Для ориентированных графов обход в глубину и ширину по идее схожи. Итак, при работе с ориентированными графами часто бывает такая ситуация, что граф не является связным, значит до некоторых вершин алгоритм может не дойти, если мы выберем определенную вершину (мы можем выбирать любую вершину для начала работы алгоритма). Значит, чтобы захватить все вершины, необходимо после отработки алгоритма запустить его ещё раз, но в этот раз взять за корень не посещенную вершину.

### Алгоритм:

1. Проверяем посещена ли вершина
2. Если не посещена, то запускаем DFS(BFS) c корнем в этой вершине
3. повторяем шаги 1, 2 до тех пор, пока все вершины не будут посещены

### Применение обхода в глубину для определения вида ребра в ориентированном графе

Древесный граф - это граф, который является связным и не имеет циклов. Древесное ребро - это ребро, которое принадлежит древесному графу и соединяет две вершины в этом графе, при этом не является частью цикла. То есть, если удалить древесное ребро, граф разобьется на два подграфа.

При обходе в глубину неориентированного графа DFS разделяет все ребра на древесные и обратные. При обходе в глубину для ориентированного графа разделение ребер идет на 4 класса:
![[Pasted image 20240611230114.png]]

Также при помощи этого алгоритма можно определять тип ребра в ориентированном графе, это является очень полезным при разработке алгоритмов для работы с ориентированными графами

### Алгоритм определения типа ребра в графе:
![[Pasted image 20240611230135.png]]
**Пояснение**: во время работы алгоритма мы перешли в какую-то вершину, мы ее зафиксировали и передаем в функцию, как вершину y. Вершина x - вершина, которая имеет с y ребро(может, кстати, быть такая ситуация, что ребро есть, а какая-то вершина не найдена, но она будет найдена потом, поэтому в общем случае мы рассматриваем в качестве x не все вершины, а только найденные).

1. Если x - родитель y, то ребро древесное
2. Если вершина была посещена (метка посещения ставится после вызова функции) до этого, при этом не является обработанной (не все вершины, для который она родитель посещены), то ребро обратное
3. Если обработана и при этом время нахождения вершины y больше x, то ребро прямое
4. Если время y меньше времени x, то ребро поперечное

## Топологическая сортировка

Задача: упорядочить вершины вдоль линии таким образом, что все ориентированные ребра направлены слева направо

_Замечание:_ такое упорядочивание ребер невозможно в графе, содержащем ориентированный цикл, так как в таком графе не существует линейного порядка вершин. Любой, бесконтурный ориентированный граф (не содержит обратных ребер) имеет, по крайней мере, одно топологическое упорядочивание

### Зачем нужна?

Важность топологической сортировки состоит в том, что она позволяет упорядочить вершины графа таким образом, что каждую вершину можно обработать перед обработкой ее потомков. Допустим, что ребра представляют управление очередностью таким образом, что ребро (х, у) означает, что работу х нужно выполнить раньше, чем работу у. Тогда любое топологическое упорядочивание определяет правильное календарное расписание. Более того, бесконтурный орграф может содержать несколько таких упорядочиваний.

### Процедура топологической сортировки

- **Если вершина y не открыта, то начинаем обход в глубину из вершины у, прежде чем можем продолжать исследование вершины х**
- **Если вершина у открыта, но не обработана, то ребро (х,у) является обратным ребром, что запрещено в бесконтурном ориентированном графе**
- **Если вершина у обработана, то она помечается соответствующим образом раньше вершины х**


# 9. Обход взвешенных графов. Минимальное остовное дерево. Алгоритм Прима

### Остовное дерево - подмножество ребер Е, которые создают дерево, содержащее все вершины графа V.
![[Pasted image 20240611232350.png]]
Наибольший интерес представляет минимальное остовное дерево - остовные деревья с минимальной суммой весов ребер
Минимальные остовные деревья позволяют решить задачу, в которой требуется соединить множество точек (представляющих города, дома, перекрестки и другие объекты) наименьшим объемом дорожного полотна, проводов, труб и т. п. Любое дерево - это в сущности, наименьший (по количеству ребер) возможный связный граф, в то время как минимальное остовное дерево является наименьшим связным графом по весу ребер.

### Алгоритм Прима (алгоритм построения минимального остовного дерева):

1) Начинаем с указанной вершины
2) Вставляем в остовное дерево одну новую вершину, исходя из “жадного” принципа (из множества рассмотренных ребер к дереву добавляется ребро с наименьшим весом)

_Пояснение:_
1. Берем первую вершину, ищем ребро, исходящее из нее с минимальным весом
2. Добавляем вершину, с котором ребро связывает исходную вершину, к остову
3. Ищем из множества ребер, которые идут из множества вершин, входящих в остов, минимальное ребро, которое соединяет любую вершину остова с вершиной не из остова и добавляем новую вершину
4. повторяем 3 снова, до тех пор, пока не закончатся вершины
![[Pasted image 20240611232509.png]]
![[Pasted image 20240611232707.png]]
![[Pasted image 20240611232730.png]]
![[Pasted image 20240611232913.png]]
### Анализ эффективности алгоритма Прима

Зависит от используемых структур данных Если n – исполняемых циклов; m – просматриваемых ребер в каждом цикле

O($n^2$) – «наивная» реализация O(m + n*lgn) – применение структуры данных в виде кучи с приоритетами (двоичная куча, Фибоначчиева куча)

_Пояснение:_ если использовать “наивную” реализацию, то при поиске минимального ребра мы каждый раз перебираем все ребра, которые нам доступны. При использовании очереди с приоритетам мы просто закидываем в нее все ребра, которые нам доступны, временная сложно закидывания всех ребер будет n*lgn, а получать минимальное ребро мы будем за константу, ну и соответственно добавить все вершин - m операций
# 10. Алгоритма Крускала построения минимального остовного дерева
### опиСАСАНИЕ 

Алгоритм Крускала изначально помещает каждую вершину в своё дерево, а затем постепенно объединяет эти деревья, объединяя на каждой итерации два некоторых дерева некоторым ребром. Перед началом выполнения алгоритма, все рёбра сортируются по весу (в порядке неубывания). Затем начинается процесс объединения: перебираются все рёбра от первого до последнего (в порядке сортировки), и если у текущего ребра его концы принадлежат разным поддеревьям, то эти поддеревья объединяются, а ребро добавляется к ответу. По окончании перебора всех рёбер все вершины окажутся принадлежащими одному поддереву, и ответ найден.
### Алгоритм:

1) Первоначально каждая вершина – отдельный компонент будущего дерева  
2) Последовательно ищем ребро для добавления в расширяющийся лес путем поиска самого легкого среди соединяющих два дерева в лесу  
3) Выполняется проверка на нахождение обеих конечных точек ребра - кандидата в одной и той же связанной компоненте

_Пояснение:_

1. сортируем все ребра графа от наименьшего к наибольшему(это можно сделать обычной сортировкой или просто закинуть их все в очередь с приоритетом)
2. Если вершины, которые данное ребро соединяет: *
    1. оба находятся в одном множестве, то пропускаем ребро
    2. обе вершины не принадлежат ни одному из множеств - соединяем их и образуем новое множество
    3. если вершины принадлежат разным множествам - сливаем множества в одно
    4. если одна вершина принадлежит какому-то множеству, а вторая не принадлежит, то просто добавляем к множеству новую вершину
3. Повторяем шаг 2 до тех пор, пока не закончатся вершины

*проверку всех случаев можно сделать при помощи обхода в глубину или ширину

### Модификация DSU (система непересекающихся множеств):

Эта структура данных предоставляет следующие возможности. Изначально имеется несколько элементов, каждый из которых находится в отдельном (своём собственном) множестве. За одну операцию можно **объединить два каких-либо множества**, а также можно **запросить, в каком множестве** сейчас находится указанный элемент.
Множества элементов мы будем хранить в виде **деревьев**: одно дерево соответствует одному множеству. Корень дерева — это представитель (лидер) множества.
При реализации это означает, что мы заводим массив parent, в котором для каждого элемента мы храним ссылку на его предка в дерева. Для корней деревьев будем считать, что их предок — они сами (т.е. ссылка зацикливается в этом месте).

**объединение:** Чтобы объединить два множества (операция ), мы сначала найдём лидеров множества, в котором находится , и множества, в котором находится . Если лидеры совпали, то ничего не делаем — это значит, что множества и так уже были объединены. В противном случае можно просто указать, что предок вершины  равен  (или наоборот) — тем самым присоединив одно дерево к другому.
Проверка на принадлежность 1 множеству: операции поиска лидера find() проста: мы поднимаемся по предкам от вершины , пока не дойдём до корня, т.е. пока ссылка на предка не ведёт в себя. Эту операцию удобнее реализовать рекурсивно.

### Анализ эффективности алгоритма Крускала

**Зависит от используемых структур данных Если n – вершин; m – ребер O(m lg m) – время упорядочивания ребер (c применением DSU)**
**O(m n) – время исполнения при реализации поиска в ширину или глубину**
![[Pasted image 20240611233054.png]]
![[Pasted image 20240611233119.png]]
![[Pasted image 20240611233615.png]]
![[Pasted image 20240611233646.png]]

# 11. Поиск кратчайшего пути. Алгоритм Дейкстры
### **Путь – последовательность ребер, соединяющих две вершины**

**Задача:** Для заданного взвешенного графа G = (V, E) найти кратчайшие пути (с минимальным весом) из заданной вершины s до всех остальных вершин. Веса всех рёбер неотрицательны.

**Алгоритм:** В ориентированном взвешенном графе, вес рёбер которого неотрицателен w: E→ℝ, алгоритм Дейкстры находит длины кратчайших путей из заданной вершины s до всех остальных. В алгоритме поддерживается множество вершин U, для которых уже вычислены длины кратчайших путей до них из s. На каждой итерации выбирается вершина u∉U, которой не соответствует минимальная оценка кратчайшего пути. Вершина u добавляется в множество U и производится релаксация всех исходящих из неё рёбер.

_Пояснение:_

Изначально у нас есть 2 точки, надо попасть из одной в другую. Пусть расстояние от начальной вершины, до каждой вершины графа $+\infty$

1. выбираем все ребра, которые соединяют текущую вершину и смежные. Расстояние от начальной вершины до данной равно L. Пусть расстояние от текущей до смежной равно $m_i$, а расстояние от начальной вершины до вершин, смежных с L - $l_i$.
2. Если мы рассматриваем вершину L, то мы уже точно нашли кратчайший путь от начальной вершины до этой и он равен L. для каждой вершины сравниваем L+$m_i$ и $l_i$
3. Если $L+m_i<l_i$, то обновляем минимальное расстояние от начальной вершины до данной смежной вершины
4. Далее либо используем BFS и кидаем все вершины в очередь, либо DFS(обойти все равно придется весь граф, поэтому разница, что использовать - нет)
5. Повторяем 1-4 до тех пор, пока не обойдем весь граф и не дойдем до нужной вершины

_Замечание:_ Алгоритм Дейкстры работает правильно только на графах, в которых нет ребер с отрицательным весом. Дело в том, что при построении пути может встретиться ребро с отрицательным весом настолько большим по модулю, что оно полностью изменит оптимальный путь от вершины s к какой-то другой вершине, которая уже включена в дерево. Образно говоря, самым выгодным путем к соседу по лестничной клетке может оказаться путь через банк на другом конце города, если этот банк выдает за каждое посещение достаточно большое вознаграждение, делающее такой маршрут выгодным.
### Эффективность алгоритма Дейкстры
![[Pasted image 20240611233820.png]]![[Pasted image 20240611234238.png]]



# 12. Вычислительная геометрия. Элементарные задачи вычислительной геометрии
### Вычислительная геометрия — раздел информатики, в котором рассматриваются алгоритмы для решения геометрических задач.

### Области применения вычислительной геометрии:

- Компьютерная графика
    - пересечения примитивов
    - удаление невидимых поверхностей, освещение
- Робототехника
    - кинематика
- Геоинформационные системы
    - интерполяция, хранение данных, работа со слоями
- CAD/CAM/CAE
    - CAD (computer-aided design) - проектирование с помощью ЭВМ
    - CAM (computer-aided manufacturing) - компьютерная поддержка производства
    - CAE (computer-aided engineering) - класс продуктов для компьютерной поддержки расчетов и инженерного анализа
- и другие ….

### Элементарные задачи вычислительной геометрии

- Находится ли точка p на отрезке? Если не находится, то с какой стороны?
- Пересекаются ли два отрезка $l_1$ и $l_2$?
- Триангуляции треугольников
- Поиск ближайшей точки
- Выявление пересечений

### Что важно в задачах вычислительной геометрии

- Понимание геометрических свойств задачи
- Правильное применение алгоритмов и структур данных

### Примеры задач вычислительной геометрии

- Пример 1
Представьте, что вы идете по территории университетского городка (кампуса) и внезапно вспоминаете, что должны срочно позвонить. На территории много телефонных будок и, естественно, вам нужна ближайшая. Но какая из них ближайшая? Хорошо бы иметь карту, на которой можно найти ближайшую будку, в какой бы точке кампуса вы ни находились. На такой карте было бы показано разбиение кампуса на области и для каждой из них – ближайшая телефонная будка. Как выглядят такие области? И как их построить?

- Пример 2
Допустим, вы нашли ближайшую телефонную будку. С картой кампуса в руках вы, надо полагать, без особого труда найдете достаточно короткий путь в обход стен и других препятствий. Но вот запрограммировать робота для решения той же задачи будет посложнее. И в этом случае задача имеет геометрическую природу: при заданном наборе геометрических препятствий найти кратчайший путь между двумя точками, избегающий столкновений с препятствиями.

- Пример 3
Допустим, что у вас не одна карта, а две: на одной нанесены различные здания, в т. ч. и телефонные будки, а на другой – дороги на территории кампуса. Чтобы спланировать маршрут к будке, мы должны наложить карты друг на друга, т. е. объединить содержащуюся в них информацию.

# 13. Базовые алгоритмы вычислительной геометрии. Проблемы реализации

### **Геометрическая вырожденность – частные случаи, требующие специальных подходов.**

Решения:

1. Игнорирование
    Пренебрежение вырожденными случаями, преимущество в том, что нет необходимости совершенствовать и усложнять алгоритм, но могу возникать проблемы с правильностью результатов
    
2. Подделка невырожденности
    Немного подкорректировать случай так, чтобы он оказался вырожденным, преимущества и недостатки аналогичны игнорированию
    
3. Обработка вырожденности
    Создавать определенные условия работы алгоритмов, для обработки вырожденных случаев, преимущества в том, что в таком случае ответ будет наиболее правильным, но может сильно усложниться алгоритм.
    

### **Численная неустойчивость – случаи, когда применение арифметических операций может привести к проблемам переполнения или потери точности.**

Решения:

1. Использование целочисленных арифметических операций
2. Использование действительных чисел двойной точности
3. Использование арифметических операций произвольной точности

## Базовые алгоритмы

### 1. Вычисление площади треугольника

![[Pasted image 20240611234740.png]]
### Почему работает?

Площадь треугольника $(a_x,a_y), (b_x, b_y), (c_x, c_y)$ - координаты вершин треугольника на плоскости. Площадь треугольника, построенного на векторах можно вычислить через определитель, площадь треугольника = $\frac{1}{2}$ площади параллелограмма

Стороны на которых построен параллелограмм: $(a_x - b_x,a_y - b_y), (b_x - c_x, b_y - c_y)$

$$ \begin{vmatrix} a_x - b_x& b_x - c_x\\ a_y - b_y& b_y - c_y\end{vmatrix} = (a_x - b_x)(b_y - c_y) - (b_x - c_x)(a_y - b_y) = \begin{vmatrix} a_x & a_y & 1\\ b_x & b_y & 1 \\ c_x & с_y & 1\end{vmatrix} $$

Объем треугольной пирамиды $(a_x,a_y), (b_x, b_y), (c_x, c_y), (d_x, d_y)$ - координаты вершин пирамиды (это второй определитель)

### 2. Выяснение местоположения точки

![[Pasted image 20240611234805.png]]
_Пояснение:_ если определитель больше 0, то площадь треугольника положительно ориентирована, значит минимальный поворот от одного вектора к другому против часовой стрелки, значит точка с - выше прямой, с < 0 аналогично, если 0, то координаты точек пропорциональны, значит лежат на 1 прямой

### 3. Проверка нахождения точки внутри круга
![[Pasted image 20240611234826.png]]

# 14. Алгоритмы вычисления выпуклой оболочки

### **1. В**ыпуклая оболочка

Множество S на плоскости называется выпуклым, если для любых двух точек p,q∈S весь отрезок pq принадлежит S.

**_Выпуклая оболочка_ – конечное множество Р точек однозначно определенного многоугольника, вершинами которого являются точки, принадлежащие Р, и который содержит все точки Р.**
![[Pasted image 20240611235006.png]]
### Вычисление выпуклой оболочки $O(n^3)$

![[Pasted image 20240611235036.png]]

_Пояснение:_

берем 2 точки, через них проходит ровно 1 прямая, если существует точка, которая находится левее данной прямой, то, очевидно, что данный отрезок не будет принадлежать выпуклой оболочке, значит мы его выбрасываем. Берем следующие 2 точки, и так далее, пока прямые не закончатся. Получим множество отрезков, которые и составляют выпуклую оболочку

### Convex Hull ($nlog_2n)$

![[Pasted image 20240611235112.png]]

Идея: Построить сначала нижнюю оболочку, затем верхнюю и соединить их, получив необходимую нам выпуклую оболочку

Псевдокод инкрементного алгоритма:

![[Pasted image 20240611235245.png]]
_Пояснение:_

1. сортируем точки по координате x
2. начинаем строить верхнюю часть
3. добавляем первые 2 точки в множество выпуклой оболочки
4. обозначим последние 2 добавленные точки p и q
5. берем следующую точку m
6. считаем определитель с точками p, q, m
7. если > 0, то m лежит выше прямой, проходящей через точки p и q, тогда удаляем точку q и за p и q снова берем последние точки принадлежащие множеству
8. Повторяем шаги 6-7 до тех пор, пока в множестве не останется 2 точки или когда определитель не будет ≤ 0
9. добавляем в множество точку m
10. повторяем шаги 3-9 до тех пор, пока не закончатся точки
11. Аналогично, с учетом знака строим нижнюю оболочку
12. объединяем оболочки

### Проблемы при построении выпуклой оболочки

- С каким количеством измерений мы имеем дело?

сложность с увеличением размерности

- Данные представлены в виде вершин или полупространств?

задача поиска пересечения набора n полупространств в d-мерном пространстве двойственна поиску выпуклых оболочек из n точек в d-мерном пространстве

- Сколько точек может содержать оболочка?

удаление точек заведомо находящихся внутри

- Как выяснить форму данного набора точек?

потеря подробности O/G, альфа-очертания

### Теорема выпуклой оболочки. **Выпуклую оболочку множества n точек на плоскости можно вычислить за время О(n logn).**

Такая оценка возникает, когда используется алгоритм "разделяй и властвуй", основанный на рекурсивном делении множества точек на полуплоскости. В этом случае выпуклая оболочка рассматривается как объединение выпуклых многогранников, полученных рекурсивным делением исходного множества. Каждый этап рекурсии занимает O(n), а общее количество таких этапов не превосходит O(log n), поскольку максимальное количество полуплоскостей, на которые может быть разбита выпуклая оболочка, равно n.

# 15. Алгоритмы выявления пересечений


![[Untitled-7.png]]
### Проблемы в задаче выявления пересечений:

- Вычисление местонахождения пересечения или сам его факт ?
- Выявление пересечения прямых или отрезков ?
- Ожидаемое количество пересечений ?
- Видна ли точка x из точки y ?
- Являются ли пересекающиеся объекты выпуклыми ?
- Выполняется ли многократный поиск пересечений с одними и теми же основными объектами ?

### Алгоритм заметания плоскости

![[Untitled-6.png]]
Идея алгоритмов этого типа заключается в представлении себе воображаемой прямой (чаще вертикальной), которая движется по плоскости, останавливаясь в некоторых точках. Геометрические операции ограничены геометрическими объектами, которые или пересекаются, или примыкают к выметающей прямой, а полное решение доступно, когда прямая пройдёт через все объекты.

_Пояснение идеи:_ прямая движется слева направо, если встречает начало или конец отрезка, а также точку пересечения, то обрабатывает это событие, естественно, что мы не можем использовать непрерывную величину, значит будем прыгать по этим трем событиям

Объяснение алгоритма (первые 30 минут): [https://youtu.be/sINIi2mwYls](https://youtu.be/sINIi2mwYls) (новосибирск сила)

Подробное описание алгоритма (со страницы 27): [dmitr (ipo.spb.ru)](http://ipo.spb.ru/journal/content/896/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D0%B9%20%D0%B3%D0%B5%D0%BE%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%B8.%20%D0%9F%D0%B5%D1%80%D0%B5%D1%81%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BE%D1%82%D1%80%D0%B5%D0%B7%D0%BA%D0%BE%D0%B2:%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%20%D0%B7%D0%B0%D0%BC%D0%B5%D1%82%D0%B0%D0%BD%D0%B8%D1%8F%20%D0%BF%D0%BB%D0%BE%D1%81%D0%BA%D0%BE%D1%81%D1%82%D0%B8..pdf)

### Псевдокод(если не понимаешь, посмотри видео)

![[Untitled-5.png]]
![[Untitled-4.png]]
### Структуры данных

- Очередь событий (приоритетная очередь) - очередь, упорядоченная по x(y) координате, всех возможных будущих событий: вставок, удалений, пересечений.
- Структура состояний - динамическая структура, используется для доступа к соседям данного отрезка s, чтобы можно было проверить, пересекаются ли они с s.

![[Untitled-3.png]]
![[Untitled-2.png]]
![[Untitled.png]]
Время работы: O(n*log n+I*logn)
# 16. Триангуляция многоугольников

Входные данные: набор точек или многогранник

Задача: разбить внутреннюю часть набора точек или многогранника на треугольники

![[Pasted image 20240612142540.png]]
### Вопросы при триангуляции

- Триангулируется набор точек или многоугольник?
- Имеет ли значение внешний вид треугольников триангуляции?
- Как можно улучшить результат триангуляции?
- Какова размерность задачи?
- Каковы ограничивающие условия на входные данные?
- Можно ли добавлять дополнительные точки или перемещать существующие?

### Применение триангуляции

_Задача:_ пусть у нас есть здание, его необходимо обставить камерами так, чтобы как можно меньшее число камер покрывали всю площадь видимости помещения. Пусть у нас есть точки, в которые можно поставить камеры, соединим их отрезками и получим углы в которые можно поставить камеры(левый рисунок). Теперь обозначим области видимости камер (треугольники на втором рисунке) Как то наш прямоугольник разбит на треугольники. Теперь необходимо поставить камеры так, чтобы мы всю фигуру разбили на треугольники (камеры могут поворачиваться).

![[Pasted image 20240612142556.png]]

### Теорема Хватала: $\lfloor \frac{n}{3} \rfloor$ охранников(эквиваленты камерам) всегда достаточно, чтобы охранять простой многоугольник с n вершинами
![[Pasted image 20240612142610.png]]
## Метод монотонной триангуляции

Идея: разбить многоугольник на монотонные части, а затем триангулировать каждую из них.

### Основные определения

Определение. Простой прямоугольник P называется монотонным, относительно прямой l, если любая прямая k, перпендикулярная l пересекает многоугольник не более 2-х раз

Определение. Прямоугольник, монотонный, относительно y-оси называется y-монотонным

Рассмотрим самую верхнюю вершину и будем идти вниз до самой минимально вершины, при этом просматривая все вершины. Изначально получаем, что $y_j > y_{j+1}$, назовем вершину поворотной, если это неравенство в ней меняется. Если вершины по y равны, то выше та, у которой по x координата больше.

Пусть $\phi$ - внутренний угол вершины(внутри многоугольника)

Обозначим типы вершин:

start - два соседа вершины лежат ниже ее самой и $\phi < \pi$

split - два соседа лежат ниже ее самой и $\phi > \pi$

end - два соседа лежат выше ее самой и $\phi < \pi$

merge - два соседа лежат выше ее самой и $\phi > \pi$

regular - не является поворотной, в отличие от остальных, другими словами один её сосед находится выше, а другой ниже её самой.
![[Pasted image 20240612142630.png]]
_Лемма:_ прямоугольник P является y-монотонным, если в нем отсутствуют split и merge вершины

### Алгоритм разбиения на split и merge вершины

Чтобы сделать многоугольник монотонным, нужно избавиться от split и merge вершин путём проведения непересекающихся диагоналей из таких вершин.

Рассмотрим горизонтальную заметающую прямую l, будем перемещать её сверху вниз вдоль плоскости на которой лежит исходный многоугольник P. Будем останавливать её в каждой вершине многоугольника. В тот момент, когда на пути заметающей прямой встречается split или merge вершина её нужно соединить с вершиной, у которой расстояние до l минимально, при этом она должна лежать соответственно выше или ниже l.

_**Split вершина**_. Пусть  и  — ближайшее левое и правое ребро относительно split вершины , которые  пересекает в данный момент. Нам нужно найти вершину, лежащую между  $e_j$ и  $e_k$, наиболее приближённую к , либо если такой точки не существует выбрать минимальную из верхних вершин $e_j$ и $e_k$ . Для этого будем хранить указатель на искомую вершину у левого ребра $e_j$, который можно заранее вычислить. Тип вершины, хранящийся в helper не имеет значения. Таким образом, чтобы построить диагональ для split вершины нужно обратиться к указателю $helper(e_j)$ её левого ребра, которое  пересекает в данный момент.
![[Pasted image 20240612142645.png]]
_**Merge вершина**_. В отличие от случая со split вершиной заранее вычислить указатель $helper$  нельзя, поскольку merge вершина $v_i$ должна быть соединена с вершиной, лежащей ниже заметающей прямой l. Для этого в $helper(e_j)$ - левого относительно ребра запишем саму $v_i$. Далее спускаем заметающую прямую вниз к следующей вершине $v_m$, обращаемся к $helper$'у её левого ребра. Проверяем, если там хранится merge вершина, строим диагональ $v_iv_m$. Последняя проверка осуществляется для любого типа вершины, кроме split.
![[Pasted image 20240612142659.png]]
### Триангуляция монотонного многоугольника

Идея: Будем проходить сверху вниз по вершинам многоугольника проводя диагонали где это возможно.

Отсортируем все вершины многоугольника P в порядке убывания их y-координаты. Заведём стек вершин S. В стеке будем хранить вершины в отсортированном порядке, которые были обработаны, но не были отрезаны от многоугольника, то есть находятся в той части многоугольника, которая ещё не была триангулирована. В момент обработки некоторой вершины, будем пытаться провести из неё как можно больше диагоналей к вершинам, содержащимся в стеке. Эти диагонали отрезают треугольники от P. На вершине стека будет храниться вершина, которая будет обрабатываться последней.

_Пояснение:_ берем вершину из стека, строим из нее как можно больше диагоналей, тем самым получая треугольники. И отрезаем от прямоугольника треугольники. Далее переходим к следующей вершине и повторяем то же самое
![[Pasted image 20240612142711.png]]
Каждый раз получаем что-то вроде этого
![[Pasted image 20240612142726.png]]

# 17. Диаграмма Вороного, методы построения

Вход: множество S точек $p_1,...,p_n.$

Задача: разбить пространство на области вокруг каждой точки таким образом, чтобы все точки в этой области вокруг точки $p_i$ были ближе к этой точке, чем к любой другой точке множества S.

_Пояснение:_ провести линии между двумя точками так, чтобы условия выполнялись(очевидно, что каждая линия равноудалена от двух точек, которые она отделяет)
![[Pasted image 20240612143109.png]]
### Применение диаграммы Вороного O(n logn)

1. Поиск ближайшей точки
2. Размещение точек обслуживания
3. Наибольший пустой круг
4. Разработка маршрута
5. Улучшение триангуляции

### Методы построения диаграммы Вороного

- Рандомизированное инкрементальное построение

а) вставка новых точек - поиск содержащих ее ячейки

б) добавление серединных перпендикуляров

- Алгоритм Форчуна

основа - метод заметающей прямой

### Основные определения

Локус - область, в которой присутствуют все точки, которые находятся ближе к данной точке, чем ко всем остальным

Сайт - точка, для которой строим локус

### Как строить локус?

По определению он будет строиться так: пусть дано множество из n точек, для которого мы строим диаграмму. Возьмём конкретную точку **p**, для которой строим локус, и ещё одну точку из данного нам множества — **q** (не равную **p**). Проведём отрезок, соединяющий эти две точки, и проведём прямую, которая будет являться серединным перпендикуляром данного отрезка. Эта прямая делит плоскость на две полуплоскости — в одной лежит точка **p**, в другой лежит точка **q**. В данном случае локусами этих двух точек являются полученные полуплоскости. То есть для того, чтобы построить локус точки **p**, нужно получить пересечение всех таких полуплоскостей (то есть на месте **q** побывают все точки данного множества, кроме **p**).
![[Pasted image 20240612143131.png]]
_Пояснение:_ очевидно, что середину отрезка можем довольно легко найти, затем существует алгоритм, который за константу строит перпендикуляр к прям, тем самым получаем нужную прямую

### Рандомизированное инкрементное построение$O(n^2log_2n)$

Идея: построить диаграмму вороного для нескольких точек, затем добавить новую точку случайно и перестроить диаграмму, учитывая ее.

1. берем 2 любые точки и строим для них серединный перпендикуляр, тем самым построили диаграмму вороного для 2-х точек
2. добавляем новую точку m
3. Определяем, сайт $p_j$ в ячейку которого попала данная точка(перебор)
4. Строим новую ячейку $O(nlog_2n)$
    1. сначала проводим серединный перпендикуляр для $mp_j$, он пересечет границы ячеек, с которыми $p_j$ ячейка была соседом
    2. далее строим серединный перпендикуляр m и соседа $p_j$
    3. повторяем b до тех пор, пока не построим локус для точки m
5. Повторяем 2-4 до тех пор, пока не закончатся точки $O(n)$

### Алгоритм Форчуна $O(nlog_2n)$

Описание алгоритма:

Есть n сайтов (точек на плоскости). Есть заметающая прямая, которая двигается (например) «сверху вниз», то есть от сайта с наибольшей ординатой к сайту с меньшей (от события к событию, если быть точным). Сразу стоит отметить, что влияние на построение диаграммы оказывают _только_ те сайты, которые находятся _выше или на_ заметающей прямой. Когда ЗП попадает на очередной сайт (происходит **событие точки (point event)**

создаётся новая парабола (arch), **фокусом** которой является данный сайт, а **директрисой**— заметающая прямая. Эта парабола делит плоскость на две части — «внутренняя» область параболы соответствует точкам, которые сейчас ближе к сайту, а «внешняя» область — точкам, которые ближе к sweep line, ну а точки, лежащие на параболе — равноудалены от сайта и ЗП. Парабола будет меняться в зависимости от положения ЗП к сайту — чем дальше ЗП уходит от сайта вниз, тем больше расширяется парабола, однако в самом начале она вообще является отрезком («направленным» вверх). Далее парабола расширяется, у неё появляются две **контрольные точки (break points)**— точки её пересечения с остальными параболами («береговой линией»). В «береговой линии» мы храним дуги парабол от одной точки пересечения их друг с другом до другой, так и получается beach line. По сути, в этом алгоритме мы моделируем движение этой «береговой линии». потому как эти самые break point`ы движутся аккурат по рёбрам ячеек Вороного (ведь получается, что контрольные точки равноудалены от обоих сайтов, которым соответствуют эти параболы, да ещё и от ЗП).

И как раз-таки в тот момент, когда две контрольные точки — по одной из разных парабол — «встречаются», то есть как бы превращаются в одну, эта точка и становится вершиной ячейки Вороного (происходит **событие круга (circle event)**), причём в это время та дуга, которая находилась между этими двумя точками — «схлопывается» и удаляется из «береговой линии». Далее мы просто соединяем эту точку с предыдущей соответствующей ей и получаем ребро ячейки Вороного.

Чтобы лучше понять можно зайти на этот сайт и внимательно посмотреть на гифку: [File:Fortunes-algorithm-slowed.gif - Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Fortunes-algorithm-slowed.gif)

_Пояснение:_

По сути мы передвигаем прямую на новую точку, старые параболы расширяются(геометрическое определение параболы - множество точек, равноудаленных от директрисы и фокуса), там образуется парабола, двигаем еще на новую точку, еще образуются параболы и так далее. Если происходи событие, при котором какие-то 2 ветви параболы пересекаются, то мы точку пересечения фиксируем, она будет одной из вершин ячейки для верхней точки. Тем самым получаем диаграмму
### Важные варианты диаграммы Вороного

- Неевклидовы метрики расстояний
- Диаграммы мощности
- Диаграммы k-го порядка и диаграммы для самых дальних точек

# 18. Алгоритмы ЛА. Умножение матрицы на вектор

![[Pasted image 20240612143627.png]]
_Пояснение:_ в первом случае мы как бы просто скалярно перемножаем два вектора и записываем результат(делаем как обычно), во втором случае у наш на каждой итерации есть вектор столбец, мы к нему прибавляем еще один вектор, умноженный на x. Кажется, что алгоритмы по сложности и сути абсолютно одинаковы. Но на самом деле, один из них будет быстрее другого работать, при правильно подобранной архитектуре системы
![[Pasted image 20240612143639.png]]
## Алгоритм умножение матрицы на матрицу

_Замечание:_ по сути алгоритм умножения матрицы на матрицу - это общий случай перемножения матрицы на столбец. Но, кстати, именно здесь уже можем увидеть, что в зависимости от размерности матриц, будет играть роль, какой из 2-х алгоритмов умножения матрицы на столбец применить
![[Pasted image 20240612143714.png]]

_Пояснение:_ дробим матрицы на вектор-строки(матрицу A) и вектор столбцы (матрица B) и просто векторы скалярно перемножаем
![[Pasted image 20240612143731.png]]
_Пояснение:_ в первом виде разбили матрицу B на вектор-столбцы, тем самым по сути свели задачу к перемножению матрицы на столбец и применили второй способ перемножения

Во втором случае разбили A на строки и применили первый способ умножения
![[Pasted image 20240612143741.png]]
_Пояснение:_ тоже самое, как в предыдущем алгоритме, только наборот(является общим случаем второго случая умножения матрицы на столбец)
![[Pasted image 20240612143753.png]]
Пояснение: разбили обе матрицы на вектор-строки и вектор столбцы и перемножаем

### P.S. На сколько я понял, разные реализации алгоритмов нужны для разных реализаций архитектур, для того, чтобы все работало быстро и эффективно. В пояснениях, по поводу их работы. лишь мои додумки, поэтому опираться на них не стоит.

# 19. LU разложение. Различные методы параллельной и последовательной реализации

## Свойства матриц, которые могут помочь
![[Pasted image 20240612145132.png]]
## LU-разложение

_Примечание. Хорошо про LU-разложение расписано у Першина_ [здесь](http://getsomemath.ru/subtopic/computational_mathematics/numerical_linear_algebra/gauss_methods).

LU-разложение — это представление матрицы A в виде A=L•U, где L — нижнетреугольная матрица с единичной диагональю, а U — верхнетреугольная матрица. LU-разложение является модификацией метода Гаусса. Основные применения данного алгоритма — решение систем алгебраических уравнений, вычисление определителя, вычисление обратной матрицы и др.

### Особенности LU-разложения

1. **Легко вычисляется определитель матрицы:**

$$ |A| = |L|*|U| = |U| = \prod_{i=1}^n u_{i,i} $$

1. **Упрощенное решение СЛАУ, поскольку решения больше не зависят от правой части.**

пояснение: если раньше мы решали методом Гаусса, приводя расширенную матрицу $(A|B)$ к треугольному виду (и в зависимости от В мы получали разные матрицы), то с LU-разложением нам неважна правая часть:

$AX= B \to LUX =B$
$Пусть \ Y=UX, \ тогда \ LY=B$.

Тогда так как L - нижнетреугольная матрица, легко находим Y и, поскольку U - верхнетреугольная матрица, легко решаем уравнение

$$ UX=Y $$

1. **Сложность алгоритма LU-разложения: $2n^3 /3+O(n^2 )$**

Последующее же решение систем: $n^2+O(n)$

пояснение: Каждый из шагов требует $O(n^2 )$ операций. Таким образом, один раз разложив матрицу A в нижнюю и верхнюю треугольные матрицы, мы можем находить решение x для различных b в $O(n^2 )$ шагов.

### Алгоритм получения матриц L и U:

пример:
![[Pasted image 20240612145307.png]]
Формула, по которой вычисляем
![[Pasted image 20240612145317.png]]
## Методы параллельной и последовательной реализации

Рассмотрим параллельную систему с локальной памятью. Пусть p = n, i-я строка матрицы А находится в i-м процессоре. Тогда один из возможных вариантов LU-разложения:
![[Pasted image 20240612145327.png]]
Недостатки:

- Значительный объем обмена данных
- Уменьшение на один активный процессор на каждом шагу

Альтернатива:

i-й столбец → в i-й процессор. Тогда $l_{i1}$ вычисляется в Р1 → рассылка → параллельная модификация.

Возникающие сложности – это проблема балансировки нагрузки.

Реально p ≤ n. В этом случае проблема балансировки смягчается. Пусть n = kp и храним данные по строкам
![[Pasted image 20240612145340.png]]
Принцип слоистого хранения оправдан главным образом для систем с локальной памятью. Но его можно применять и в системах с глобальной памятью как средство распределения заданий между процессорами. Однако в системах с глобальной памятью динамическая балансировка может осуществляться и с помощью банка заданий.

### LU разложение, с помощью слоистой схемы
![[Pasted image 20240612145349.png]]
Выход - начать рассылку второй строки, как только она сосчитана.

### Это стратегия опережающей ссылки.

Если можно совместить рассылку и обмен, то пока будет идти рассылка, процессоры будут заниматься модификацией. Насколько выгодна такая стратегия для конкретной машины будет зависеть от топологии межпроцессорных связей.

Стратегию опережающей рассылки можно использовать и в системах с разделяемой памятью. Здесь при прямолинейной реализации потребовалось бы синхронизация после каждого шага (не начать следующий шаг, пока не кончен предыдущий). Стратегия опережающей рассылки теперь носит характер стратегии опережающих вычислений.

Она меняет порядок действий, маркируя k+1строку, как только ее модифицируют, -«ГОТОВО» Теперь другие процессора, завершив свою работу, на k-м шаге, могут немедленно приступить к k+1 шагу, если k+1 строка «готова» Это – **конвейеризация.**

# 20. Прямой ход решения СЛАУ. Параллельная модификация

Среди всех способов нахождения решения СЛАУ традиционно выделяют два типа методов: **прямые** и **итерационные** методы. Прямые методы направлены на получение точного решения систем и являются, как правило, более трудоемкими. Цель итерационных методов – нахождение некоторого приближения к точному решению.

### Прямые методы решения СЛАУ

- _**Метод Гаусса.**_
    - Он заключается в последовательном исключении переменных, когда с помощью элементарных преобразований система уравнений приводится к равносильной системе ступенчатого вида, из которого последовательно, начиная с последних по номеру переменных, находятся все остальные. Преимущества этого метода – доступность алгоритма и наличие большого количества модификаций. Однако метод имеет весьма серьезный недостаток, не позволяющий широко использовать его в расчетах, – высокая трудоемкость метода. Количество арифметических операций связано с размерностью $O(n^3)$, поэтому для больших систем метод Гаусса практически не применим.
- _**Метод на основе LU-разложения**_
    - Известный прямой метод, являющийся одной из модификаций метода Гаусса. Его суть в получении из исходной матрицы коэффициентов двух матриц: _L_ – нижней треугольной матрицы и _U_ – верхней треугольной матрицы. Основная сложность алгоритма в их получении. Решение находится на фазе обратного хода. Преимущество этого метода в том, что при решении более одной системы с одинаковыми матрицами коэффициентов, но разными правыми частями, полученные при решении матрицы _L_ и _U_ будут такими же для всех последующих систем. Основной недостаток метода LU-разложения все в той же высокой трудоемкости.
- _**Метод Крамера**_
    - Метод, основанный на определителях: если определитель системы отличен от нуля, то система линейных уравнений имеет одно единственное решение, причём неизвестное равно отношению определителей. В знаменателе – определитель системы, а в числителе – определитель, полученный из определителя системы путём замены коэффициентов при этом неизвестном свободными членами. Эта теорема имеет место для системы линейных уравнений любого порядка.

### Дополнительные аспекты проблемы хранения информации (проблемы частичного выбора главного элемента)

1. А – циклическая столбцовая схема → поиск ГЭ в одном процессоре

Проста, но чревата опасностью простоя остальных процессоров во время поиска. Эту опасность можно уменьшить, применяя **Стратегию Опережающих Вычислений и Рассылки**

Ведущая строка → передать другим → параллельная работа по перестановке определена (явно или неявно)

2. А – циклическая строчная схема → поиск ГЭ во всех процессорах → ответ в одном процессоре → разослать остальным → перестановка(2 процессора)

Надо принять решение, выполнять ли перестановку физически или переиндексировать – либо простой процессоров при работе двух, либо искажение циклической схемы хранения.

### Параллельная модификация

Один из подходов к параллельной модификации прямых методов LU-разложения - это разделение матрицы на блоки, которые могут быть обработаны независимо друг от друга. В этом случае, матрица разбивается на подматрицы, которые могут быть обработаны на разных вычислительных процессах. Каждый процесс либо вычисляет элементы одной из треугольных матриц L и U, либо работает над решением системы линейных уравнений. В этом случае, каждый процесс должен иметь доступ к той части матрицы, которую ему поручено обрабатывать, а также к результатам вычислений других процессов, необходимых для вычисления следующих элементов матриц L и U.

Другой подход - использование метода перемещения столбцов (Column Mover’s Method) для разделения матрицы на подматрицы, которые обрабатываются параллельно. В этом случае, входная матрица разделена на подматрицы, которые после перемещения столбцов вправо рассматриваются как блоки из треугольных матриц L и U. Далее, каждый столбец матрицы обрабатывается одним из вычислительных процессов, и после завершения обработки столбца, соответствующий элемент матрицы L или U может быть вычислен.

Оба подхода к параллельной модификации прямых методов LU-разложения позволяют ускорить вычислительный процесс за счет распределения нагрузки между несколькими вычислительными процессами. Однако выбор способа зависит от структуры и размера матрицы, а также от доступных вычислительных ресурсов.

Минусы:

1. Необходимость синхронизации процессоров
2. Неравномерность распределения нагрузки: некоторые процессы могут работать более медленно, что создает узкие места во время выполнения.
3. Ошибки: использование параллельных алгоритмов увеличивает возможность ошибок при передаче данных между процессами, синхронизации и обработке результатов.

### Параллельный метод Гаусса

Идея: При внимательном рассмотрении метода Гаусса можно заметить, что все вычисления сводятся к однотипным вычислительным операциям над строками матрицы коэффициентов системы линейных уравнений. Как результат, в основу параллельной реализации алгоритма Гаусса может быть положен принцип распараллеливания по данным. В качестве _базовой подзадачи_ можно принять тогда все вычисления, связанные с обработкой одной строки матрицы A и соответствующего элемента вектора b.

Рассмотрим общую схему параллельных вычислений и возникающие при этом информационные зависимости между базовыми подзадачами.

Для выполнения **прямого хода** метода Гаусса необходимо осуществить (n-1) итерацию по исключению неизвестных для преобразования матрицы коэффициентов A к верхнему треугольному виду.

Выполнение итерации i, 0<=i<n-1, прямого хода метода Гаусса включает ряд последовательных действий. Прежде всего, в самом начале итерации необходимо выбрать ведущую строку, которая при использовании метода главных элементов определяется поиском строки с наибольшим по абсолютной величине значением среди элементов столбца i, соответствующего исключаемой переменной xi. Поскольку строки матрицы A распределены по подзадачам, для поиска максимального значения подзадачи с номерами k, k<i, должны обменяться своими элементами при исключаемой переменной xi. После сбора всех необходимых данных в каждой подзадаче может быть определено, какая из подзадач содержит ведущую строку и какое значение является ведущим элементом.

Далее для продолжения вычислений ведущая подзадача должна разослать свою строку матрицы A и соответствующий элемент вектора b всем остальным подзадачам с номерами k, k<i. Получив ведущую строку, подзадачи выполняют вычитание строк, обеспечивая тем самым исключение соответствующей неизвестной xi.

При выполнении обратного хода метода Гаусса подзадачи выполняют необходимые вычисления для нахождения значения неизвестных. Как только какая-либо подзадача i, 0<=i<n-1, определяет значение своей переменной xi, это значение должно быть разослано всем подзадачам с номерами k, k<i. Далее подзадачи подставляют полученное значение новой неизвестной и выполняют корректировку значений для элементов вектора b.
# 21. Мелкозернистый алгоритм потока данных для решения СЛАУ.

Мелкозернистый алгоритм потока данных для решения систем линейных алгебраических уравнений - это параллельный алгоритм, который используется для решения больших и разреженных систем линейных уравнений.

### Основная идея

Суть мелкозернистого алгоритма потока данных заключается в разделении матрицы на маленькие блоки, называемые "зернами", и решение каждого блока независимо от других блоков. После того, как все блоки решены, результаты объединяются, чтобы получить окончательный ответ.

### Псевдокод

```markdown
Для каждого блока матрицы B, 0<=i< N/p, 0<=j<N/p:
	Решить блок B(i,j)
Объединить результат всех блоков
```

где р - количество процессоров, а N - размер матрицы.

### Достоинства:

1. Эффективность: данный алгоритм позволяет достичь высокой производительности при решении больших систем линейных уравнений.
2. Распараллеливание: алгоритм легко распараллелить, что позволяет использовать для его выполнения многопроцессорные и многопоточные системы.
3. Работа с большими размерами матриц: мелкозернистый алгоритм потока данных позволяет работать с очень большими матрицами, которые могут быть разбиты на множество блоков.
4. Гибкость: алгоритм может быть адаптирован для работы с аппаратным и программным обеспечением различных производителей, что делает его очень гибким.

### Недостатки:

1. Перенос данных: так как каждый блок рассчитывается независимо от других, то при объединении блоков происходит перенос большого количества данных.
2. Стоимость коммуникации: перенос данных может оказаться очень затратным в случае использования удаленной системы, что может снизить эффективность алгоритма.
3. Требование к памяти: требуется большое количество оперативной памяти.

### Сложность:

Зависит от числа блоков, на которые разбита матрица. В идеальном случае, когда число блоков стремится к бесконечности, асимптотическая сложность алгоритма будет O(N), где N - размер матрицы. Однако, на практике, количество блоков определяется ограничениями оборудования. Поэтому сложность может быть выше.
![[Pasted image 20240612145715.png]]
### Сам алгоритм
![[Pasted image 20240612145726.png]]
![[Pasted image 20240612145757.png]]

То есть как бы надвигаемся фронтом, тем самым довольно быстро решаем

## Организация LU-разложения

_(или то, что было в презентации)_

LU-разложение можно организовать следующим образом:

1. Вычислить первый столбец множителей $l_{i1}, i=2,...,n$
2. Вычислить модифицированные элементы $a_{ij}^1= a_{ij}-l_{i1}a_{1j}, i=2,...,n, j=2,...,n.$
3. Вычислить второй столбец множителей $l_{i2}, i=3, ...n.$
4. Вычислить модифицированные элементы $a_{ij}^2= a_{ij}^1-l_{i2}a_{2j}^1, i=3,...,n, j=3,...,n.$

……………….

2n-3. Вычислить последний множитель $l_{n,n-1}$

2n-2. Модифицировать элемент (n,n).

Поскольку для модификации любого элемента требуется две операции (+ и *), то и процесс завершится за 3(n-1) временных шагов.

В этой схеме предполагается, что в системе имеется по крайней мере $(n-1)^2$ процессоров, которые необходимы на шаге 2.

В приведенном примере обойден важный вопрос обменов. Рассмотрим одну из схем, в которой пересылка данных между любыми двумя процессорами происходит за один шаг. Пусть процессора пронумерованы, как элементы матрицы, т.е. процессор $P_{ij}$ производит пересчет элемента $a_{ij}$. Тогда схема модифицируется следующим образом:

1. Вычислить $l_{i1}$ в процессоре $P_{i1}, i=2,...,n.$
2. Переслать $l_{ij}$ в процессоры $P_{ij}, i=2,...,n,j=2,...,n$.
3. Вычислить $a_{ij}^1$ в процессоре $P_{ij}, i=3,...n, j=3,...,n$
4. Переслать $a_{2j}^1$ в процессоры $P_{ij}, i=3,...n, j=3,...n.$

Если считать, что на каждом шаге все необходимые пересылки могут быть сделаны за единицу времени, то общее количество временных шагов увеличивается до O(5n)

# 22. Ленточные матрицы. Проблемы решения. Последовательный вариант алгоритма
### Ленточная матрица - квадратная матрица, все ненулевые элементы которой примыкают к главной диагонали.
![[Pasted image 20240612150124.png]]
### Псевдокод LU-разложения ленточной матрицы для строчно и столбцово ориентированных алгоритмов:

```
# строчный вариант:
for k = 1 to n-1:
    # Вычисляем множители для k-й строки и столбца.
    for i = k+1 to min(k+p,n):
        L[i,k] = A[i,k]/A[k,k]
        A[i,k] = 0
        for j = k+1 to min(k+q,n):
            A[i,j] = A[i,j] - L[i,k]*A[k,j]
    # Обновляем верхнетреугольную матрицу.
    for j = k+1 to min(k+q,n):
        U[k,j] = A[k,j]
```

Здесь n - размерность матрицы, p и q - ширина ленточных матриц L и U соответственно. Алгоритм начинается с вычисления множителей для первого столбца и первой строки матрицы A. Затем применяются эти множители к оставшимся частям матрицы, чтобы получить L и U. В матрице A элементы, которые должны быть равны 0, не удаляются, чтобы сохранить структуру ленточной матрицы.

Псевдокод для столбцово ориентированного алгоритма выглядит аналогичным образом. Единственное отличие состоит в том, что применение множителей происходит не для строк, а для столбцов матрицы A. Это позволяет использовать оптимизации, такие как кэширование, при выполнении алгоритма.

При достаточно больших q псевдокод для полнозаполненных матриц может быть основой как для векторных, так и для параллельных компьютеров. При уменьшении q такой подход совершенно не подходит.

### Проблемы решения

Перестановки, возникающие при выборе главного элемента в ленточной матрице при сохранении LU-разложения увеличивают q. Последствия для векторных и параллельных компьютеров более серьезные, чем для последовательных.

Последовательный вариант алгоритма имеет проблемы с производительностью при работе с большими матрицами. Это связано с тем, что данный алгоритм требует много операций с памятью и производит много вычислений. Кроме того, он не эффективен при работе с матрицами, у которых есть большие свободные области вокруг ленточной формы.

Параллельный вариант алгоритма не очень подходит для решения этой задачи из-за необходимости синхронизации потоков при доступе к общей памяти. Это может привести к задержкам и снижению производительности.

## Метод прогонки: для решения систем Ax = b с трехдиагональной матрицей наиболее часто применяется метод прогонки, являющийся адаптацией метода Гаусса к этому случаю.

Идея: метод прогонки состоит из двух этапов: прямой и обратной прогонки. На первом этапе определяются “прогоночные” коэффициенты, а на втором – находят неизвестные x.

### Алгоритм:
![[Pasted image 20240612150137.png]]
![[Pasted image 20240612150153.png]]

1. Прямой ход метода прогонки (вычисление коэффициентов):
    ![[Pasted image 20240612150203.png]]
    
![[Pasted image 20240612150216.png]]
1. Обратный ход (нахождение решения)
![[Pasted image 20240612150225.png]]

# 23. Блочный метод решения ленточной матрицы для параллельной системы

Рассмотренные методы хороши, когда q большая (см.пред.билет). Если q небольшое, то можно рассмотреть «блочные методы»

Блочный метод решения ленточной матрицы для параллельной системы - это метод решения системы линейных уравнений с ленточной матрицей, который используется для распараллеливания вычислений на несколько процессоров.

### Основная идея

Суть работы алгоритма заключается в разбиении матрицы на блоки, которые могут быть обработаны параллельно. Для этого матрица разбивается на блоки размера n x n, где n - это размер блока. Затем каждый блок решается отдельно, используя метод Гаусса или другой метод решения системы линейных уравнений.

Основная задача блочного метода заключается в том, чтобы уменьшить количество обращений к памяти и увеличить скорость вычислений за счет параллельной обработки блоков матрицы.

А сейчааас слаааайд шооууу
![[Pasted image 20240612150801.png]]
![[Pasted image 20240612150815.png]]
![[Pasted image 20240612150839.png]]
![[Pasted image 20240612150852.png]]
![[Pasted image 20240612150904.png]]
![[Pasted image 20240612150918.png]]

_**Редуцированная система алгебраических уравнений**_ - это система уравнений, полученная из исходной системы путем удаления некоторых уравнений и/или переменных. Это может быть полезным при решении системы уравнений, когда некоторые уравнения или переменные не являются необходимыми для получения решения, или когда их удаление может упростить вычисления.

### Псевдокод блочного метода решения ленточной матрицы:

```markdown
1. Разбить матрицу на блоки размера n x n
2. Для каждого блока:
   1. Выполнить преобразование Гаусса для получения верхнетреугольной матрицы
   2. Решить систему линейных уравнений с помощью обратного хода
3. Объединить результаты решения всех блоков в одну матрицу
```

## Блочный алгоритм Лори-Самеха

```markdown
1. Разбить матрицу на блоки размера w x w, где w - ширина ленты матрицы
2. Для каждого блока:
   1. Выполнить преобразование Гаусса для получения верхнетреугольной матрицы
   2. Решить систему линейных уравнений с помощью обратного хода
   3. Сохранить только диагональные элементы матрицы
3. Объединить результаты решения всех блоков в одну матрицу диагональных элементов
```
![[Pasted image 20240612151000.png]]

Потенциально узким местом блочного алгоритма Лори-Самеха является редуцированная система, которая возникает при удалении из матрицы нулевых элементов. В этом случае размерность системы может значительно уменьшиться, что может привести к неравномерной загрузке процессоров и ухудшению производительности. Для решения этой проблемы можно использовать методы динамического изменения размера блоков или перестройки матрицы.

# 24. Итерационные методы решения СЛАУ

**Итерационные методы решения системы линейных алгебраических уравнений - это методы, которые на каждом шаге приближают решение системы, используя предыдущее приближение. Такие методы могут быть полезными в случаях, когда матрица системы имеет большой размер, и прямые вычисления слишком громоздки.**

## **Метод Якоби**

Метод простейший. Он не является приемлемым для большинства задач, но представляет собой удобную отправную точку для обсуждения итерационных методов

Пусть А – невырожденная матрица n×n и нужно решить систему **Ax = b**

### Алгоритм:

1. Разбить матрицу системы на диагональную и недиагональную части: A = D + R, где D - диагональная матрица, а R - недиагональная матрица.
2. Начать с некоторого начального приближения $x_i^0$.
3. Повторять следующий шаг до тех пор, пока не будет достигнута заданная точность:

$x^{k+1} = D^{-1}(b-Rx^k)$

пояснение: из метода гаусса получаем (в случае $a_{ii} \ne 0):$

$x_i^{k+1} = \frac 1 a_{ii} *(-\sum_{j\ne i} a_{ij} x_j^k+b_i), \ где \ i=1,...,n; k=0,1,....$

в эту формулу мы вставляем наш первый x (начальное приближение), и продолжаем этот процесс до получения равенства из пункта 3.

**Теорема 1.** Если матрица А имеет строгое диагональное преобладание или является неприводимой с диагональным преобладанием, то итерации метода Якоби сходятся при любом начальном приближении x0.

**Теорема 2.** Если матрица A=D-B симметрична и положительно определена, то итерации метода Якоби сходятся при любом начальном приближении x0 тогда и только тогда, когда матрица D+B положительно определена. Главная операция – матрично-векторное умножение (МВУ) Эффективная параллельная или векторная реализация МВУ существенно зависит от структуры H. Итерационные методы применяются, когда A(H)–большие разреженные матрицы.

### Достоинства и недостатки метода Якоби:

- Прост в реализации.
- Сходится для диагонально-преобладающих матриц.
- Медленно сходится для недиагонально-преобладающих матриц.
- Не гарантирует сходимость для произвольных матриц.

## Уравнение Пуассона - пример использования метода Якоби

**Уравнение Пуассона - это уравнение, которое описывает распределение потенциала в области, имеющей равномерно распределенные заряды. Для решения уравнения Пуассона можно использовать итерационный метод, который заключается в следующем:**

1. Разбить область на сетку с шагом h.
2. Начать с некоторого начального приближения u(0).
3. Повторять следующий шаг до тех пор, пока не будет достигнута заданная точность:

$u^{k+1}_{ij} =(u^k_{i+1,j} + u^k_{i-1, j}+u^k_{i, j+1} +u^k_{i, j-1}-h^2f_{i,j})/4$

Эти значения можно вычислять _**параллельно.**_ Метод Якоби иногда рассматривают как прототип параллельного метода.

Предположим, что параллельная система образована сетью из $p=q^2$ процессоров $P_{ij}$, упорядоченных в виде двумерной решетки. Предположим, что $n^2=mp.$ Тогда естественна обработка каждым процессором m неизвестных.
![[Pasted image 20240612151251.png]]
Теоретический случай $p=N^2$ процессоров. Необходима пересылка между процессорами после каждого хода.

Вычисляем $u_{ij}^{k+1}$ в процессоре $P_{ij}$; посылаем значение $u_{ij}^{k+1}$ процессорам $P_{i+1,j}$, $P_{i-1,j}$, $P_{i,j-1},$$P_{i,j+1}$. Более обычной является ситуация, когда $N^2=mp$ и каждый процессор содержит m неизвестных. Например, для предыдущего распределения данных вычисления, выполняемые процессором $P_{22}$ будут:

1. вычислить $u_{33}^{k+1}$, $u_{34}^{k+1}$;
2. передать их в $P_{12}$;
3. вычислить $u_{43}^{k+1}$;
4. передать $u_{33}^{k+1}$, $u_{43}^{k+1}$ в $P_{21}$;

Аналогичные вычисления параллельно выполняются в других процессорах

Будем называть внутренними граничными значениями те значения $u_{ij}$, которые необходимы другим процессорам на следующей итерации и, таким образом, подлежат пересылке.

## Синхронные и асинхронные методы

**Синхронные и асинхронные методы - это методы, которые различаются по способу обновления переменных на каждой итерации. В синхронном методе все переменные обновляются одновременно, а в асинхронном методе каждая переменная обновляется поочередно. Основная идея этих методов заключается в том, что при обновлении переменных можно использовать уже полученные на предыдущих итерациях значения.**

### Достоинства и недостатки:

- Применяются для различных типов матриц
- Оптимизируются для параллельных вычислений
- Для некоторых матриц могут сходиться медленно
- Могут потребовать большого количества памяти для хранения промежуточных значений

Синхронные методы лучше использовать, когда матрица системы имеет малый размер и хранится в памяти целиком. Асинхронные методы лучше использовать, когда матрица системы имеет большой размер или является разреженной, и когда доступ к элементам матрицы может быть организован поочередно.

# 25. Алгоритмы нахождения собственных значений. Проблемы вычисления характеристического полинома.

**Собственным** числом (или **собственным** **значением**) квадратной **матрицы** A называется число λ такое, что система уравнений Ax = λx имеет ненулевое решение x. **Это** решение называется **собственным** вектором **матрицы** A, соответствующим **собственному** **значению** λ.

**Характеристический полином** определяется для произвольной квадратной матрицы A как det(A−λE), где E – единичная матрица одинакового с A порядка.

### Некоторые важные свойства:

- Если А симметрична, то собственные значения вещественны, а собственные векторы ортогональны.
- $\lambda_1*\lambda_2*...*\lambda_n=(-1)^ndet(A).$

### Методы вычисления характеристического полинома:

Неэффективные: разложение на миноры и метод Гаусса

пояснение: минусы этих методов:

- высокая вычислительная сложность, особенно для больших матриц. Это связано с высокой степенью полинома и сложностью операций с определителями.
- чувствительность к погрешностям особенно для матриц с большими числами или близкими кратными собственными значениями. Это может привести к неточным результатам.

### Идеи решения

- предварительное преобразование определителя с удалением $\lambda$ с главной диагонали в крайний ряд
- интерполяция, чтобы аппроксимировать его значения на определенном диапазоне.
- итеративные методы для приближенного нахождения некоторых собственных значений
- использование методов численного решения полиномиальных уравнений
# 26. Методы Леверье и Крылова

## Метод Леверье

**- это итерационный метод для приближенного вычисления собственных значений и собственных векторов симметричной матрицы. Он основан на разложении матрицы в степенной ряд и использовании рекуррентных соотношений для получения приближенных значений собственных чисел.**

### Основная идея

Основная идея метода заключается в том, что если матрица A имеет собственное значение λ и соответствующий ему собственный вектор x, то можно представить матрицу A в виде ряда Леверье:

$$ A=c_0E+c_1A+c_2A^2+...+c_{n-1}A^{n-1} $$

где $c_0=1, c_i=-\frac{1}{i} Sp(Ac_{i-1}) \ \ i > 0$

Затем можно использовать полученное разложение для нахождения собственных значений матрицы A. Для этого можно применять итерационный процесс, начиная с некоторого начального вектора v:

$$ v_0=v \\v_{k+1}=\frac{1}{c_n} (Av_k - c_{n-1}v_{k-1} - ... - c_0v_0) $$

Таким образом, задача сводится к нахождению коэффициентов ряда Леверье и итерационному нахождению собственных значений.

### Псевдокод

```markdown
1. Выбрать начальный вектор v и задать максимальное число итераций k.
2. Вычислить коэффициенты ряда Леверье Ci.
3. Применять итерационный процесс для нахождения собственных значений.
```

### Алгебраическое обоснование (база от Утешева)

примечание. След матрицы - сумма элементов главной диагонали…..

Метод основан на формуле $Sp(A^k)= \lambda_1^k+...+\lambda_n^k=s_k$

то есть след k-й степени матрицы А равен k-й сумме Ньютона ее характеристического полинома $f(\lambda)=det(A-\lambda E)$. Вычисляем последовательные степени матрицы А:

$s_1=Sp(A), s_2=Sp(A^2),...,s_n=Sp(A^n)$

Неизвестные коэффициенты $f(\lambda)=(-1)^n(\lambda^n+a_1\lambda^{n-1}+...+a_n)$ находим по рекурсивным формулам Ньютона:

$a_1=-s_1, a_2=-(s_2+a_1s_1)/2,...,a_k=-(s_k+a_1s_{k-1}+...+a_{k-1}s_1)/k, k$≤$n$

Очевидно, что не имеет смысла вычислять _все_ элементы матрицы $A^n$— достаточно обойтись лишь элементами ее главной диагонали.

Существует модификация метода Леверье, позволяющая организовать одновременное вычисление как самого характеристического полинома матрицы A, так и матрицы взаимной к матрице A − λE (что делает возможным получение универсальной формулы для всех собственных векторов матрицы A); этот метод известен как метод Леверье-Фаддеева.

### Достоинства:

- эффективный метод нахождения собственных значений матрицы, особенно для матриц с небольшим числом собственных значений.
- не требует явного вычисления характеристического полинома и может быть применен для матриц, у которых характеристический полином сложно или невозможно вычислить.
- легко параллелится

### Недостатки:

- может сходиться только к некоторым собственным значениям матрицы, в зависимости от выбора начального вектора и максимального числа итераций
- неустойчив при наличии множественных собственных значений или собственных значений, близких к другим собственным значениям.
- чувствителен к ошибкам округления при вычислениях, особенно при вычислении коэффициентов ряда Леверье

## Метод Крылова

**Метод Крылова - это итерационный метод нахождения собственных значений матрицы, который основан на построении подпространства Крылова, порожденного начальным вектором и матрицей.**

### Основная идея

заключается в том, что если матрица A имеет собственное значение λ и соответствующий ему собственный вектор x, то любой вектор y из подпространства Крылова K(A, v) (где v - начальный вектор) может быть представлен в виде линейной комбинации собственного вектора x и его ортогональной компоненты $y_0:$

$y=c_1x+y_0$

- примечание.
    
    Ортогональная компонента вектора - это вектор, который ортогонален (перпендикулярен) данному вектору и направлен вдоль ортогонального пространства. Ортогональная компонента вектора может быть найдена путем проекции данного вектора на ортогональное пространство.
    

Таким образом, задача сводится к нахождению коэффициентов $c_1 \ и \ y_0$ которые можно найти путем минимизации нормы вектора (A - λE)y

Это можно решить методом наименьших квадратов или методом ортогонализации Грама-Шмидта.

### Достоинства:

- является одним из самых эффективных методов нахождения собственных значений матрицы, особенно для больших матриц.
- не требует явного вычисления характеристического полинома и может быть применен для матриц, у которых характеристический полином сложно или невозможно вычислить.
- легко параллелится

### Недостатки:

- может сходиться только к некоторым собственным значениям матрицы, в зависимости от выбора начального вектора и максимального числа итераций
- неустойчив при наличии множественных собственных значений или собственных значений, близких к другим собственным значениям.
- чувствителен к ошибкам округления при вычислениях, особенно при использовании метода ортогонализации Грама-Шмидта.

### Алгебраическое обоснование (база от Утешева)
![[Pasted image 20240612151548.png]]
![[Pasted image 20240612151556.png]]
![[Pasted image 20240612151602.png]]

# 27. Задача интерполяции. Многочленная интерполяция. Формы Лагранжа и Ньютона

**Интерполяция** или **интерполирование** — приближенное или точное нахождение какой-либо величины по известным отдельным значениям этой же величины, или других величин, с ней связанных.

Различают _интерполяцию_ и _экстраполяцию_: образно говоря, интерполяция занимается восстановлением неизвестной функции в промежутках между точками, где известны ее значения, в то время как экстраполяция предполагает, что мы пытаемся «растянуть» множество задания функции.

### Постановка задачи
![[Pasted image 20240612151744.png]]
## **Интерполяционный полином в форме Лагранжа**
![[Pasted image 20240612151754.png]]
![[Pasted image 20240612151858.png]]

Уравнение для определения _единственного_ интерполяционного полинома можно записать в детерминантной форме:
![[Pasted image 20240612151912.png]]
Представив последний столбец в виде суммы, получим отсюда:
![[Pasted image 20240612151921.png]]
Теперь разложим определитель из числителя по последнему столбцу и вспомним выражение определителя Вандермонда. Обозначим
![[Pasted image 20240612151931.png]]
Тогда **интерполяционный полином в форме Лагранжа** записывается в виде:
![[Pasted image 20240612151939.png]]
## **Интерполяционный полином в форме Ньютона**

Основной недостаток построения интерполяционного полинома по методу (в форме) Лагранжа заключается в том, что при добавлении в таблицу нового узла (новых результатов измерений), в формуле приходится пересчитывать все слагаемые. От этого недостатка свободен метод Ньютона, в котором добавление нового узла ведет к добавлению лишь одного слагаемого к построенному ранее полиному.
![[Pasted image 20240612151948.png]]
![[Pasted image 20240612152054.png]]

### Случай равностоящих узлов
![[Pasted image 20240612152111.png]]
После преобразований получим
![[Pasted image 20240612152127.png]]
# 28. Точки Чебышева. Число обусловленности. Феномен Рунге.
### Многочлен Чебышева

$\begin{cases} T_0\equiv1\\ T_1(x)=x\\ T_{n+1}=2xT_n(x)-T_{n-1}(x), n=2,3.... \end{cases}$

_**Свойства:**_

1. $T_n = 2^{n-1}x^n+....$
2. $T_{2n}$ - четная функция, $T_{2n+1}$ - нечетная
3. На отрезке [-1, 1] имеет место представление $T_n=cos(n \ arccosx)$

### **Точки Чебышева**

Точки Чебышева - это набор точек на отрезке [a,b], которые определяются по формуле:

$$ x_k = \frac{1}{2}(a+b) + \frac{1}{2}(b-a)\cos\left(\frac{(2k-1)\pi}{2n}\right) $$

где k = 1,2,...,n, a и b - концы отрезка, n - количество точек.

Точки Чебышева используются для равномерной аппроксимации функций на отрезке [a,b]. Они позволяют достичь более высокой точности аппроксимации по сравнению с равномерно расположенными точками.

пояснение. Связь между формулой многочлена Чебышева и формулой для точек Чебышева заключается в том, что точки Чебышева являются корнями многочлена Чебышева первого рода $T_n(x)$ - из свойства 3 на интервале [-1,1].

Некоторые полезные свойства:

1. Многочлен Чебышева имеет на отрезке $[-1, 1]$ ровно $n$ действительных корней. Все они задаются формулой $x_k=cos(\frac{2k-1}{2n}\pi)$, $k=0,1,...,n-1$.
    
2. Нули многочленов Чебышёва являются оптимальными узлами в различных _интерполяционных схемах_.
    
    пояснение: нули многочленов Чебышева распределены равномерно на интервале [-1, 1]. Это означает, что они плотно покрывают весь интервал, что особенно полезно для аппроксимации функций, которые могут иметь сложное поведение на краях интервала. Равномерное распределение узлов обеспечивает более равномерную аппроксимацию функции. Еще они минимизируют феномен Рунге (ниже)
    

### Число обусловленности

В области численного анализа **число обусловленности** функции по отношению к аргументу измеряет, насколько может измениться значение функции при небольшом изменении аргумента. Данный параметр отражает, насколько чувствительна функция к изменениям или ошибкам на входе и насколько ошибка на выходе является результатом ошибки на входе.
![[Pasted image 20240612152427.png]]
![[Pasted image 20240612152441.png]]
Пространственные числа обусловленности выводится оцениванием того, насколько чувствительны прогнозируемые значения к небольшим изменениям коэффициентов линейных уравнений интерполяции. Небольшие пространственные числа обусловленности указывают на то, что решение стабильно, тогда как большие значения говорят о том, что решение нестабильно.

### Феномен Рунге

**Феномен (явление) Рунге** — возникновение осцилляций на краях интервала при полиномиальной интерполяции с равноотстоящими узлами.
![[Pasted image 20240612152502.png]]
**Он показывает, что переход к более высоким степеням не всегда повышает точность**

Феномен Рунге можно избежать, используя точки Чебышева для равномерной аппроксимации функции на отрезке [a,b]. Точки Чебышева позволяют достичь более высокой точности аппроксимации при меньшей степени интерполяционного полинома и без больших осцилляций в концах отрезка.

Некоторые методы смягчения:

1. Изменение точек интерполяции
    
    Осцилляции можно минимизировать, используя узлы, более плотно расположенные к краям интервала, в частности, с асимптотической плотностью. Стандартным примером такого набора узлов являются узлы Чебышева, для которых гарантированно уменьшается максимальная ошибка аппроксимации функции Рунге с увеличением степени полинома.
    
2. Использование кусочных полиномов
    
    Этой проблемы можно избежать, используя сплайн-кривые, которые представляют собой кусочные полиномы. При попытке уменьшить ошибку интерполяции можно увеличить количество полиномов, используемых для построения сплайна, вместо увеличения степени используемых полиномов.
    
3. Подгонка методом наименьших квадратов
    
**Другой метод — подбор полинома более низкой степени методом наименьших квадратов.**

# 29. Кусочная интерполяция. Сплайны. Кубическая интерполяция. Граничные условия

_**Кусочная интерполяция - это метод интерполяции, при котором функция аппроксимируется различными многочленами на разных участках интервала.**_

## Кусочная интерполяция

Вместо того, чтобы использовать один многочлен для всего интервала, мы разбиваем его на несколько отрезков и на каждом отрезке используем отдельный многочлен для аппроксимации функции.

Далеко не всегда приходится иметь дело с очень гладкими функциями. В связи с этим часто применяется кусочная интерполяция. Интерполяционной функцией является полином первой степени, то есть узловые точки соединяются прямой.

### Виды кусочных интерполяций:
![[Pasted image 20240612152645.png]]
Можно построить как кусочно-кубические интерполяции так и интерполяции полиномами более высоких степеней, однако на практике такое встречается редко, т. к. уменьшается гладкость получаемых кусочных функций и увеличивается сложность получения коэффициентов полинома.

## Сплайны

_Сплайн – функция, которая вместе с несколькими производными непрерывна на всем заданном отрезке_ $[a,b]$_, а на каждом частичном отрезке_ $[x_i,x_{i+1}]$ _в отдельности является некоторым алгебраическим многочленом._

Существенным недостатком кусочно-полиномиальной интерполяции является отсутствие гладкости в глобальном масштабе. Т.е., несмотря на то, что каждый из отдельных «кусков» обладает непрерывной производной, производная глобальной функции имеет разрывы в узлах перехода от одного «куска» к другому.

Данный недостаток позволяет решить интерполяция _**сплайнами**_.

- пояснение:
    
    интерполяция сплайнами является одним из методов кусочной интерполяции. Кусочная интерполяция описывает общий подход, при котором функция разбивается на отрезки, а на каждом отрезке используется отдельный многочлен для аппроксимации функции. Интерполяция сплайнами является конкретным примером кусочной интерполяции, где используются специфические типы многочленов, называемые сплайнами.
    
    Основное отличие между интерполяцией сплайнами и общей кусочной интерполяцией заключается в выборе многочленов для аппроксимации функции на отрезках. В общей кусочной интерполяции могут использоваться многочлены различных степеней (например, линейные, квадратичные или кубические), и на каждом отрезке может быть использован разный тип многочлена. В этом случае многочлены могут быть выбраны в зависимости от требований задачи или особенностей функции.
    
    В интерполяции сплайнами используются специальные типы многочленов, обычно кубические. Эти многочлены имеют свойство гладкости, то есть они имеют непрерывные производные на всем интервале, что делает их более плавными и естественными для аппроксимации сложных функций.
    

## Кубическая интерполяция

Кубическая интерполяция - это конкретный тип сплайнов, где на каждом отрезке используется кубический многочлен для аппроксимации функции. Кубический сплайн состоит из набора кубических многочленов, каждый из которых определен на своем отрезке. Кубическая интерполяция обладает гладкостью и кривизной, что позволяет более точно аппроксимировать функцию и сохранять ее основные характеристики.

В отличие от кусочно-кубической интерполяции, здесь при переходе от одного участка интерполяции к другому не претерпевают разрыва не только первые производные, но и вторые. Это значит, что сплайновая интерполяция обеспечивает сквозное (на всем участке интерполирования) гладкое приближение к функции $f(x)$ в виде полиномов третьей степени.

![[Pasted image 20240612152657.png]]
![[Pasted image 20240612152710.png]]
## Граничные условия

Граничные условия при кусочной интерполяции определяют значения функции или ее производных на краях интервала, чтобы полностью задать интерполирующую функцию на всем диапазоне.

При кусочной интерполяции обычно имеются два типа граничных условий: условия на конечных точках (граничные точки) и условия на внутренних точках (узлах).
![[Pasted image 20240612152721.png]]
![[Pasted image 20240612152732.png]]
![[Pasted image 20240612152741.png]]
В 1-м типе классических краевых условий задают первые производные (касательные) на краях сплайна; во 2-м типе — задают вторые производные (кривизну); 3-й тип используется для интерполяции замкнутых или периодических линий и заключается в стыковке крайних фрагментов сплайна.

4-й тип используется, когда на краях сплайна не известны ни первая, ни вторая производные и заключается в стыковке соседних пар крайних фрагментов (1-го - со 2-м и последнего - с предпоследним) по третьим производным, что на практике реализуется в проведении по узлам пар соседних крайних фрагментов функции, аналогичной одному фрагменту сплайна (у полиномиального сплайна — полинома той же степени, что и фрагмент сплайна). Используются различные комбинации краевых условий, которые сводятся к данным 4 типам классических условий.

# 30. Задача аппроксимации. Метод наименьших квадратов.

**_Аппроксимация_ - научный метод, состоящий в замене одних объектов другими, в каком-то смысле близкими к исходным, но более простыми. Аппроксимация позволяет исследовать числовые характеристики и качественные свойства объекта, сводя задачу к изучению более простых или более удобных объектов.**

- Пояснение. Интерполяция от аппроксимации (отличия)
    
    Интерполяция:
    
    - Интерполяция предполагает построение функции, которая точно проходит через заданные точки данных (узлы).
    - Цель интерполяции - восстановить исходную функцию, чтобы получить точные значения функции во всех заданных точках.
    - Для интерполяции используются методы, которые стремятся создать полином или другую функцию, проходящую через все узлы данных.
    - Интерполяция может быть полезна, когда требуется точное восстановление значений функции в определенных точках, особенно если эти значения критически важны для дальнейшего анализа или вычислений.
    
    Аппроксимация:
    
    - Аппроксимация предполагает построение функции, которая приближенно описывает данные или поведение исходной функции.
    - Цель аппроксимации - найти более простую и удобную функцию, которая приближает исходную функцию с заданной точностью.
    - Для аппроксимации используются методы, которые ищут более простые формы функции, такие как полиномы низкой степени или сплайны.
    - Аппроксимация может быть полезна, когда точное восстановление значений функции в каждой заданной точке не является критическим, а более общее описание или приближение функции более важны для дальнейшего анализа или упрощения вычислений.
    
    Таким образом, интерполяция и аппроксимация представляют различные методы работы с данными и функциями. Интерполяция стремится точно проходить через заданные точки, в то время как аппроксимация стремится приблизить функцию с помощью более простых и удобных формул или моделей. Выбор между интерполяцией и аппроксимацией зависит от конкретной задачи и требований к точности и удобству работы с функцией.
    

### Метод наименьших квадратов

Предположим , что данные исходной таблицы не являются достоверными: значения обеих переменных подвержены воздействию случайных погрешностей, но эти погрешности имеют _разные_ порядки. Например, будем считать, что в заданной таблице значений
![[Pasted image 20240612152848.png]]

величины x нам известны практически без искажений (т.е. на входе процесса мы имеем абсолютно достоверные данные), а вот измерения величины y подвержены случайным погрешностям.

**Задача**. Построить полином f(x) такой, чтобы величина

$$ \sum_{j=1}^m=(f(x_j)-y_j)^2 $$

стала минимальной

В случае $deg \ f = m-1$ мы возвращаемся к задаче интерполяции в ее классической постановке. Практический интерес, однако, представляет случай $deg \ f < m-1$ экспериментальных данных больше (обычно — много больше) чем значений параметров (коэффициентов полинома f), которые требуется определить.

Так, в случае $deg \ f=1$ речь идет о построении прямой $y=ax + b$ на плоскости (x, y), обеспечивающей

$$ min(\varepsilon_1^2 + \varepsilon_2^2+...+\varepsilon_n^2), \ где \ \varepsilon_j=|ax_j+b-y_j| $$
![[Pasted image 20240612152904.png]]
### Теорема. Если $m \geqslant n \ и \ x_1, ..,x_m$ все различны, то существует единственный набор коэффициентов $a_1, ...,a_n$ , обеспечивающий минимальное значение для

$$ \sum_{j=1}^m(a_0+a_1x_j+...+a_{n-1}x_j^{n-1}-y_j^2) $$

_Этот набор определяется как решение_ **системы нормальных уравнений**
![[Pasted image 20240612152924.png]]
- Доказательство
    ![[Pasted image 20240612152938.png]]
    ![[Pasted image 20240612152945.png]]
    ![[Pasted image 20240612152959.png]]
    

Собственно минимальное значение величины cуммы квадратов невязок, а точнее усреднение по количеству узлов

$$ \sigma =\frac 1 m \sum_{j=1}^m(f(x_j)-y_j)^2 $$

называется **среднеквадратичным отклонением.**
>>>>>>> d6b6954104b91d156a1bb49f1c3d87dfc90b6d9c

# 31. Численное дифференцирование - постановка задачи. Приближение на основе полинома Ньютона
### Численное дифференцирование — постановка задачи.

**Численное дифференцирование** — совокупность методов приближённого вычисления значения производной некоторой функции, заданной таблично или имеющей сложное аналитическое выражение.

Например, необходимость в численном дифференцировании возникает в том случае, когда функция задана не формулой, а таблицей или алгоритмом вычисления в произвольной точке.

Основной подход при построении формул численного дифференцирования — это аппроксимация функции.

Предположим, что в окрестности точки $x$ функция $f(x)$ аппроксимируется некоторой другой функцией $f_n(x)$, причем $k$-тая производная $f_n^{(k)}(x)$(x) легко вычисляется. Естественно в этом случае воспользоваться приближенной формулой

$$ f^{(k)}(x) \approx f_n^{(k)} (x) $$

Приближение $f_n(x)$ может быть построено любым рассмотренным ранее методом. Например, в виде интерполяционного полинома или сплайна.

Если для интерполирующей функции P(x) известна погрешность

$$ R(x)=f(x)-P(x) $$

то погрешность производной P’(x) выражается формулой

$$ r(x)=f'(x)-P'(x)=R'(x) $$

т.е. погрешность производной интерполирующей функции равна производной от погрешности этой функции.

_**Приближенное дифференцирование представляет собой операцию менее точную, чем интерполирование.**_

## **Приближение на основе полинома Ньютона**

При численном дифференцировании на основе полинома Ньютона используется аппроксимация производной функции с использованием интерполяционного полинома Ньютона. Этот метод позволяет приближенно вычислить значение производной функции в заданной точке, используя значения функции вблизи этой точки.

Рассмотрим интерполяционный полином Ньютона P(x) на основе набора узловых точек $\{x_0, x_1, ..., x_n \}$ и соответствующих значениях функции $\{f(x_0), f(x_1), ..., f(x_n)\}$.

Для вычисления приближенного значения производной функции в точке x используется следующая формула:

$$ f'(x) \approx P'(x) = f(x_0)+[x_0;x_1](x-x_0)+[x_0;x_1;x_2](x-x_0)(x-x_1)+...++[x_0;...;x_{n-1}](x-x_0)(x-x_1)...(x-x_{n-2}) $$

где $f'(x)$ - приближенное значение производной функции в точке $x, f[x_0]$ - значение функции в узловой точке $x_0, f[x_0;x_1] -$ разделенная разность первого порядка (по определению, $f[x_0;x_1]= \frac {y_0-y_1} {x_0-x_1}$, если не очень понятно - смотри 27 билет)

По презентации Дегтярева:

![[Pasted image 20240612152956.png]]

![[Pasted image 20240612152946.png]]

Использование односторонних значений функции при численном дифференцировании может привести к неточным результатам, особенно вблизи точек разрыва или экстремумов функции. Это связано с тем, что при использовании односторонних значений производной не учитывается симметричность функции относительно точки, а также не учитываются значения функции на противоположной стороне точки. В результате, производная может оказаться завышенной или заниженной, что может привести к ошибкам в дальнейшем анализе функции. Чтобы избежать этой проблемы, рекомендуется использовать центральные разности при численном дифференцировании, которые учитывают значения функции на обеих сторонах точки.

# 32. Безразностные формулы приближенного дифференцирования
Когда производную аналитически заданной функции по причине ее сложности искать затруднительно, либо выражение для производной приобретает неудобную для применения форму, используется приближенное или численное дифференцирование. Этот метод тем более необходим, если исходная функция задана таблично. Один из способов решения задачи дифференцирования – использование интерполяционных многочленов.

Пусть _f(x)_ – функция, для которой нужно найти производную в заданной точке отрезка [a,b], $F_n(x)$– интерполяционный многочлен для _f(x)_, построенный на отрезке [a,b]. Заменяя _f(x)_ интерполяционным многочленом $F_n(x)$, получим значение производной _f(x)_ на отрезке [a,b] как значение производной интерполяционного многочлена, т.е. примем приближенно $f^\prime(x) \approx F^\prime_n(x)$ (1)

Аналогичным путем можно поступать при нахождении значений производных высших порядков функции _f(x)_.

Полагая, что погрешность интерполирования определяется формулой $R_n(x) = f(x) - F_n(x)$

Получаем подход к оценке погрешности производной $r_n(x) = f^\prime(x) - F^\prime_n(x) = R^\prime_n(x)$ (2)

Применяя для численного дифференцирования на отрезке [_a;b_] интерполяционный полином, естественно строить на этом отрезке систему равноотстоящих узлов

$a = x_0 < x_1 < ... < x_n = b$ и они делятся на одинаковые части $x_{i+1} - x_i = h = const$ (3)

Пусть $t = (x - x_0)/h$

Интерполяционный многочлен в форме Лагранжа имеет вид:

$$ L_n(x) = \sum\limits_{i=1}^ny_i\frac{P_{n+1}(x)}{(x-x_i)P^\prime_{n+1}(x_i)} , (*) $$

где $P_{n+1}(x) = (x-x_0) ... (x - x_n)$

_Пояснение:_ вообще, формула Лагранжа имеет другой вид, но если каждый член домножить и разделить на $(x-x_i)$ и внимательно посмотреть, то получим именно это

$x-x_0 = ht$

$x - x_1 = x - x_0 - (x_1 -x_0) = ht - h = h(t-1)$

$x - x_2 = x - x_0 - (x_2 - x_0) = x - x_0 - (x_2 - x_0) = ht - 2h = h(t-2)$

…

$x - x_i = x - x_0 - ih = h(t - i)$

Пояснение: у нас расстояние $x_1x_0 = h$, при этом $x_2x_1 = h$, значит $x_2 - x_0 = 2h$ из этого следует, что $x_i - x_0 = ih$

имеем $P_{n+1}(x) = h^{n+1}t(t-1)...(t-n)$

$x_i - x_0 = hi$

$x_i - x_1 = x_i - x_0 - h = h(i -1)$

…

$x_i - x_n = x_i - x_0 - nh = h(i-n)$

$P^\prime(x) = \sum\limits_{i=1}^n (x-x_1)...(x-x_{i-1})(x-x_{i+1})...(x-x_n)$ ⇒

$$ P^\prime(x_i) = (x_i - x_0)...(x_i - x_{i-1})(x_i - x_{i+1})...(x_i - x_n) = h^ni(i-1)...1(-1)...(-(n-i)) = h^ni!(n-i)!(-1)^{n-i} $$

$$ L_n(x) = L_n(x_0 + (x - x_0)) = L(x_0 + th) $$

Получаем:

$L_n(x_0 + th) = \sum\limits_{i=1}^n \frac{(-1)^i(t-1)...(t-n)}{i!(n-i)!(t-i)}, (***)$

формула для погрешности интерполяционного полинома:

$$ f(x) - L_n(x) = \frac{f^{(n+1)(\xi)}}{(n+1)!}(x-x_0)...(x-x_n) $$

Объединяя последнее уравнение с $(***)$ умноженное и разделенное на n! получаем:

$$ f(x) = \frac{(-1)^nt(t-1)...(t-n)}{n!}\sum\limits_{i=1}^n \frac{(-1)^iC^i_n}{(t-i)} + \frac{f^{(n+1)(\xi)}}{(n+1)!}h^{n+1}t(t-1)...(t-n) $$

### Дифференцирование интерполяционной функции

перепишем многочлен в более удобном виде

Пусть $t^{[n+1]} = t(t-1)...(t-n)$

$$ f(x) = \frac{(-1)^nt^{[n+1]}}{n!}\sum\limits_{i=1}^n \frac{(-1)^iC^i_n}{(t-i)} + \frac{f^{(n+1)(\xi)}}{(n+1)!}h^{n+1}t^{[n+1]} $$

Будем дифференцировать по x. В данном случае у нас t(x) значит надо рассматривать дифференцирование, как суперпозицию функции

$$ \frac{dx}{dt} = \Big(\frac{x-x_0}{h}\Big)^\prime = \frac{1}{h} $$

перенесем h в левую часть и получим:

$$ hf^\prime(x) = \frac{(-1)^n}{n!}\frac{d}{dt}t^{[n+1]}\sum\limits_{i=1}^n \frac{(-1)^iC^i_n}{(t-i)} + \frac{f^{(n+1)(\xi)}}{(n+1)!}h^{n+1}\frac{d}{dt}t^{[n+1]} $$

![[Pasted image 20240612152808.png]]

![[Pasted image 20240612152801.png]]

![[Pasted image 20240612152754.png]]

### Проблема

Даже при не очень большом шуме могут наблюдаться значительные погрешности
# 33. Метод неопределённых коэффициентов
### Метод неопределённых коэффициентов - метод, используемый в математике для нахождения искомой функции в виде точной или приближённой линейной комбинации конечного или бесконечного набора базовых функций. Указанная линейная комбинация берётся с неизвестными коэффициентами, которые определяются тем или иным способом из условий рассматриваемой задачи. Обычно для них получается система алгебраических уравнений.

В математике для приближённого вычисления производных заданной таблично функции можно искать выражение значений производных через известные значения функции с помощью подходящего набора коэффициентов. Для этого можно использовать различные интерполяционные формулы или метод неопределённых коэффициентов.

## Идея

![[Pasted image 20240612152512.png]]

_Пояснение: Есть задача, в данной точке найти k-ю производную. Мы знаем значение функции в каких-то точках. Саму функцию мы можем аппроксимировать каким-либо полиномом, так, что в этих точках узлы интерполяции (они будут равны):_

$$ f(x) = P_n(x) = \sum\limits_{i=1}^n a_ix^i $$

Но тогда давайте попробуем выразить и k-ю производную функции через значения исходной функции $f(x)$ в в данной точке (6.11) Получим в точке полиномы. Чтобы они были равны необходимо и достаточно, чтобы коэффициенты при соответствующих степенях многочлена были равны

## Формальное объяснение метода

Итак, $f^{(k)}(\xi) \approx  \sum\limits_{i=1}^n C_if(x_i)$ значит можно переписать формулу, со знаком равенства в виде:

$$ f^{(k)}(\xi) =  
\sum\limits_{i=1}^M C_if(x_i) + R(f), (*) $$

где $R(f)$ - остаточный член интерполяции в точке $\xi$ (т.к. в ней исходная формула достигает равенства). Тогда получаем, что в узла интерполяции будет:

$$

\Big(\sum\limits_{j=1}^na_j\xi^i\Big)^{(k)} = \sum\limits_{i=1}^M C_i \sum\limits_{j=1}^na_jx_i^j, (**) $$

Далее мы знаем, что полиномы в данной точки равны <=> коэффициенты при соответствующих степенях равны. Но тогда найдем производные для всех коэффициентов в левой части, приравняем полиномы. и заметим, что $a_j$ при каждом их коэффициентов будут одинаковы, поэтому сократим их. (дифференцируем мы в левой части как полином, это не считается константой. Не забываем, что, если степень одночлена будет меньше k, то при дифференцировании порядком k он обнуляется)

$0 = c_1 + c_2 + ... + c_n$

$0 = c_1x_1 + c_2x_2 + ... + c_nx_n$

…

$0 = c_1x_1^{k-1} + c_2x_2^{k-1} + ... + c_nx_n^{k-1}$

$k! = c_1x_1^k + c_2x_2^k + ... + c_nx_n^k$

$(k+1)!\xi = c_1x_1^{k+1} + c_2x_2^{k+1} + ... + c_nx_n^{k+1}$

$\frac{(k+1)!}{2!}\xi^2 = c_1x_1^{k+2} + c_2x_2^{k+2} + ... + c_nx_n^{k+2}$

…

$n(n-1)... (n-k+1)\xi^{n-k} = c_1x_1^{n} + c_2x_2^{n} + ... + c_nx_n^{n}$

Если количество узлов интерполяции равно степени полинома, то получаем:

![[Pasted image 20240612152447.png]]

откуда находим $C_i$
# 34. Численное интегрирование. Постановка задачи
_Задача:_ есть функция, пусть нам надо найти ее площадь под графиком, при этом мы не можем от нее взять интеграл.

**Численное интегрирование** — вычисление значения определённого интеграла (как правило, приближённое).

Численное интегрирование применяется, когда:

1. Сама подынтегральная функция не задана аналитически. Например, она представлена в виде таблицы (массива) значений в узлах некоторой расчётной сетки.
2. Аналитическое представление подынтегральной функции известно, но её первообразная не выражается через аналитические функции.

Желательно, чтобы метод численного интегрирования обладал следующими свойствами:

- Универсальность. Функция f(x) может быть задана в виде “черного ящика”, как способ вычисления f(x) по данному x.
- Экономичность. Количество вычислений функции f(x) по возможности должно быть сведено к минимуму.
- Хорошая обусловленность. Неустранимые погрешности $\Delta f$ в значениях f(x) не должны приводить к значительной итоговой ошибке $\Delta I$.

Численное интегрирование может применяться для:

- интегрирования функций, известных только в некоторых точках, например, полученных в результате измерений
- Интегрирования сложных выражений, не имеющих элементарных первообразных, либо имеющих слишком громоздкие выражения для них
- построения методов численного решения уравнений в обыкновенных и частных производных (методы конечных элементов, интегро-интерполяционные методы)

![[Pasted image 20240612152254.png]]


Идея: исходя из определения интеграла, взять и разбить подинтегральную площадь на прямоугольники, вычислить их площади и сложить полученные значения. Получим нужную площадь (можно разбить на достаточно малые, чтобы удовлетворять погрешности)

![[Pasted image 20240612152243.png]]

![[Pasted image 20240612152233.png]]

![[Pasted image 20240612152224.png]]

_Пояснение:_

интерполяционный полином Лагранжа

$$ L_n(x) = \sum\limits_{i=1}^ny_i\frac{P_{n+1}(x)}{(x-x_i)P^\prime_{n+1}(x_i)} , (*) $$

в данном случае:

$$ C_k = \frac{P_{n+1}(x)}{(x-x_k)P^\prime_{n+1}(x_k)} $$

Домножили и разделили на (b-a), заменили нужное на $W_k$
# 35. Интерполяционные квадратурные формулы
### _Определение._ Квадратурная формула - формула для приближенного вычисления определенного интеграла

**Основная идея (на словах)**

Интерполяционные квадратурные формулы - это методы численного интегрирования, которые позволяют приближенно вычислить значение определенного интеграла функции на заданном интервале. Основная идея заключается в том, что мы аппроксимируем интегрируемую функцию с помощью интерполяционного полинома и затем вычисляем значение интеграла этого полинома, что дает нам приближенное значение исходного интеграла.

Для использования интерполяционных квадратурных формул мы разбиваем интервал интегрирования на несколько подынтервалов и выбираем точки, называемые узлами, на каждом подынтервале. Затем мы строим интерполяционный полином, который проходит через эти узлы, чтобы аппроксимировать функцию на каждом подынтервале.

После построения интерполяционного полинома мы вычисляем значение интеграла этого полинома на каждом подынтервале. Затем мы суммируем эти значения, чтобы получить приближенное значение исходного интеграла на всем интервале интегрирования.

![[Pasted image 20240612152104.png]]

![[Pasted image 20240612152055.png]]

### Формула Ньютона - Котеса

предположим, что нам необходимо вычислить интеграл:

$$ I = \int_{a}^{b} f(t) \,dt, (*) $$

Разобьем $[a, b]$ на малые равные части: $a = t_0 < t_1 < .. <t_n = b$

Выберем на каждом таком малом промежутке $\xi_i \in [t_{i-1}, t_i]:$

$$ f(\xi_i) \approx f(t), t \in [t_{i-1}, t_i], (**) $$

тогда на каждом промежутке будем считать, что $f(t) = f(\xi) = const$. Тогда:

$$ I_i = \int_{t_{i-1}}^{t_i} f(\xi_i) \,dt = f(\xi_i)(t_i - t_{i-1}) $$

Тогда получаем, что значение интеграла равно:

$$ I =\sum\limits_{i=1}^n I_i = \sum\limits_{i=1}^n f(\xi_i)(t_i - t_{i-1}), (***) $$

Если отрезки достаточно малы, то можно взять:

$$ \xi_i = f(\frac{t_i + t_{i-1}}{2}) $$

Вообще получается, что при таком разбиении мы разбили отрезок на 2n одинаковых частей, тогда за $\xi_i$ можно взять все точки, на которые мы разбили и в итоговом варианте получаем:

$$ \int_{a}^{b} f(x)\,dx \approx \sum\limits_{i=0}^s A_if(x_i) $$

$x_i = a+ ih, h = \frac{b-a}{s}$

Данная формула как раз-таки и называется формулой Ньютона-Котеса

Замечание: такие разбиения можно было и не проводить, можно было сразу расставить точки и сказать, что так будет, но просто для понимания было решено сделать так

![[Pasted image 20240612152031.png]]

Если отрезок достаточно близок к линейному, то:

$$ f(x_i) \approx \frac{f(x_{i+1} - f(x_i)}{2} $$

Откуда и получаем формулу трапеции:
![[Pasted image 20240612152009.png]]

По сути мы малые отрезки аппроксимируем линейной функцией. Если нам этого не достаточно, то мы можем аппроксимировать его параболой и получим формулу Симпсона (для нее берем 3 точки):

![[Pasted image 20240612152000.png]]

Формула 3/8 получается путем аппроксимации подынтегральной функции интерполяционным многочленом Лагранжа второй степени на четырех узлах интегрирования: a, a+h, a+2h, a+3h (по сути для каждого куска берем не 3 точки, а четыре)
![[Pasted image 20240612151944.png]]![[Pasted image 20240612151935.png]]![[Pasted image 20240612151925.png]]
# 36. Правило Рунге
### `Правило Рунге — правило оценки погрешности численных методов`

Идея заключается в следующем: если мы используем численный метод с некоторым шагом (например, шаг интегрирования или шаг сетки), то мы можем получить приближенное значение результата. Однако мы также можем повторить вычисления с более маленьким шагом и получить еще одно приближенное значение. Правило Рунге позволяет оценить погрешность численного метода путем сравнения результатов с разными шагами.

Правило Рунге широко используется в численных методах для проверки и улучшения точности результатов. Оно позволяет выбирать оптимальный шаг и контролировать погрешность численных вычислений.

![[Pasted image 20240612151738.png]]

![[Pasted image 20240612151724.png]]

## Более приближенное объяснение

Интеграл вычисляется по выбранной формуле (прямоугольников, трапеций, парабол Симпсона) при числе шагов, равном n, а затем при числе шагов, равном 2n. Погрешность вычисления значения интеграла при числе шагов, равном 2n, определяется по формуле Рунге:

$$ \Delta \approx \Theta|I_{2n} - I_{n}| $$

для метода левых и правых прямоугольников: $\Theta = 1$

для средних прямоугольников: $\Theta = \frac{1}{3}$

В общем случае;

$$ \Theta = \frac{1}{2^p - 1} $$

p - порядок погрешности используемого метода

Таким образом интеграл вычисляется для последовательности значений числа шагов $N = n_0, 2n_0, ... n_0$ - начальное число шагов, до тех пор, пока не выполнено условие $|\Delta_N|<\epsilon, \epsilon$ - заданная точность
# 37. Метод градиентного спуска
**Градиентный спуск, метод градиентного спуска** — численный метод нахождения _локального_ минимума или максимума функции с помощью движения вдоль градиента, один из основных численных методов современной оптимизации.

### Немного теории

Рассмотрим функцию f, считая для определенности, что она зависит от трех переменных _**x,y,z.**_ Вычислим ее частные производные _дf/дх,_ _дf/ду, дf/дz_ и образуем с их помощью вектор, который называют _градиентом_ функции:

$$ grad \ f(x, y, z) = \frac{\partial f}{\partial x}i + \frac{\partial f}{\partial y}j + \frac{\partial f}{\partial z}k $$

$i, j, k$ - единичные векторы, параллельные координатным осям

Частные производные характеризуют изменение функции f по каждой независимой переменной в отдельности. Образованный с их помощью вектор градиента дает общее представление о поведении функции в окрестности точки (_х, у, z)._ Направление этого вектора является направлением наиболее быстрого возрастания функции в данной точке.

Противоположное ему направление, которое называют _**антиградиентным**,_ представляет собой направление наиболее быстрого убывания функции

$|gradf(x,y,z)|$ - определяет скорость возрастания и убывания функции в направлении градиента и антиградиента. При переходе от одной точки к другой как направление градиента, так и его модуль, вообще говоря, меняются. Понятие градиента естественным образом переносится на функции любого числа переменных.

### Идея: двигаться к минимуму в направлении наиболее быстрого убывания функции, которое определяется антиградиентом.

Выберем каким-либо способом начальную точку, вычислим в ней градиент рассматриваемой функции и сделаем небольшой шаг в обратном, антиградиентном направлении. В результате мы придем в точку, в которой значение функции будет меньше первоначального. В новой точке повторим процедуру: снова вычислим градиент функции и сделаем шаг в обратном направлении. Продолжая этот процесс, мы будем двигаться в сторону убывания функции. Специальный выбор направления движения на каждом шаге позволяет надеяться на то, что в данном случае приближение к наименьшему значению функции будет более быстрым, чем в методе покоординатного спуска.

Метод градиентного спуска требует вычисления градиента целевой функции на каждом шаге. Если она задана аналитически, то это, как правило, не проблема: для частных производных, определяющих градиент, можно получить явные формулы. В противном случае частные производные в нужных точках приходится вычислять приближенно.

### Графическая иллюстрация алгоритма (поиск точки экстремума)

![[Pasted image 20240612151554.png]]
По сути, при применении данного метода мы задаем последовательность, с определенным размером шага, зависящий от какого-то параметра и начинаем спускаться рекурсивно

![[Pasted image 20240612151532.png]]

### Частный случай - квадратичные формы

![[Pasted image 20240612151519.png]]

![[Pasted image 20240612151507.png]]

### Проблема метода

1. Зависимость от исходного приближения: метод градиентного спуска может сходиться к локальному минимуму, а не к глобальному минимуму. Это значит, что выбор неправильного начального приближения может привести к плохому результату.

Более того, есть возможность застрять в локальных минимумах или седловых точках.

1. Проблема скорости сходимости: сходимость метода градиентного спуска может быть медленной, особенно если функция имеет плохо обусловленную или плоскую поверхность. В таких случаях может потребоваться большое количество итераций для достижения сходимости.
2. Чувствительность к выбросам: метод градиентного спуска чувствителен к выбросам в данных. Единичные аномалии могут сильно влиять на направление градиента и приводить к неправильным обновлениям параметров.
3. Вычислительная сложность: метод градиентного спуска требует вычисления градиента функции на каждой итерации, что может быть вычислительно затратным для больших и сложных моделей.

### Модификации
## ![[Pasted image 20240612151449.png]]![[Pasted image 20240612151427.png]]


# Краткое содержание всех билетов

### 1. Поразрядная сортировка MSD

**Описание**: Поразрядная сортировка старшего разряда (MSD - Most Significant Digit) — это алгоритм сортировки, который обрабатывает числа по разрядам, начиная с самого старшего разряда.

**Алгоритм**:

1. Разделение массива на подмассивы в зависимости от значения старшего разряда.
2. Рекурсивная сортировка каждого подмассива по следующему разряду.
3. Повторение процесса до тех пор, пока все разряды не будут обработаны.

**Сложность**: В среднем O(n*k), где n — количество элементов, k — количество разрядов.

### 2. Трехпутевая поразрядная быстрая сортировка

**Описание**: Модификация поразрядной сортировки, использующая трехпутевое разбиение для улучшения производительности при наличии большого количества одинаковых ключей.

**Алгоритм**:

1. Разбиение массива на три части: элементы меньше текущего разряда, равные ему и большие его.
2. Рекурсивная сортировка подмассивов.
3. Объединение отсортированных частей.

**Сложность**: В среднем O(n log n), лучше справляется с большими массивами одинаковых элементов.

### 3. Поразрядная сортировка LSD

**Описание**: Поразрядная сортировка младшего разряда (LSD - Least Significant Digit) — сортировка, которая обрабатывает числа по разрядам, начиная с младшего разряда.

**Алгоритм**:

1. Сортировка массива по младшему разряду с использованием стабильной сортировки (например, подсчетом или сортировкой вставками).
2. Переход к следующему разряду и повторение сортировки.
3. Повторение процесса до тех пор, пока все разряды не будут обработаны.

**Сложность**: O(n*k), где n — количество элементов, k — количество разрядов.

### 4. Графы и их разновидности

**Описание**: Граф — это математическая структура, состоящая из вершин (узлов) и ребер (связей) между ними.

**Разновидности графов**:

- **Неориентированные графы**: Ребра не имеют направления.
- **Ориентированные графы (диграфы)**: Ребра имеют направление.
- **Взвешенные графы**: Ребра имеют вес.
- **Планарные графы**: Графы, которые можно нарисовать на плоскости без пересечений ребер.
- **Связные графы**: В каждом подграфе существует путь между любой парой вершин.

### 5. Обход графов, раскраска графов

**Обход графов**:

- **Обход в глубину (DFS)**: Рекурсивный или стековый алгоритм, посещающий вершины до тех пор, пока не достигнет конца пути.
- **Обход в ширину (BFS)**: Алгоритм, использующий очередь для посещения вершин уровня за уровнем.

**Раскраска графов**:

- **Задача**: Назначение цвета каждой вершине так, чтобы смежные вершины имели разные цвета.
- **Алгоритмы**: Жадные алгоритмы, алгоритмы на основе поиска в глубину/ширину.

### 6. Алгоритм обхода в ширину по сравнению с обходом в глубину

**Обход в ширину (BFS)**:

- **Описание**: Использует очередь, посещая вершины уровня за уровнем.
- **Применение**: Поиск кратчайшего пути в невзвешенных графах, нахождение компонент связности.

**Обход в глубину (DFS)**:

- **Описание**: Использует рекурсию или стек, углубляясь в граф до конца пути.
- **Применение**: Поиск всех путей, топологическая сортировка, обнаружение циклов.

**Сравнение**:

- **Сложность**: O(V + E), где V — количество вершин, E — количество ребер.
- **BFS** лучше для поиска кратчайшего пути, **DFS** — для задач, требующих полного перебора.

### 7. Алгоритм обхода глубину по сравнению с обходом в ширину

**Обход в глубину (DFS)**:

- **Алгоритм**: Использует стек или рекурсию для углубления в граф.
- **Применение**: Топологическая сортировка, нахождение мостов и точек сочленения.

**Обход в ширину (BFS)**:

- **Алгоритм**: Использует очередь для обхода графа по уровням.
- **Применение**: Поиск кратчайшего пути, нахождение всех вершин на определенном расстоянии.

**Сравнение**:

- **DFS** глубже проникает в граф, **BFS** быстрее находит кратчайший путь.
- **Сложность**: O(V + E) для обоих.

### 8. Обход ориентированных графов, топологическая сортировка

**Обход ориентированных графов**:

- **Описание**: Методы обхода графов с направленными ребрами.
- **Алгоритмы**: Обход в глубину (DFS) и обход в ширину (BFS).

**Топологическая сортировка**:

- **Описание**: Линейное упорядочивание вершин ориентированного ациклического графа (DAG).
- **Алгоритм**:
    1. Выполнение обхода в глубину, отслеживание времени выхода из вершины.
    2. Добавление вершин в список в порядке убывания времени выхода.
    3. Результат — упорядоченный список вершин.
- **Сложность**: O(V + E).

### 9. Обход взвешенных графов. Минимальное остовное дерево. Алгоритм Прима

**Обход взвешенных графов**:

- **Описание**: Использование методов обхода с учетом весов ребер.

**Минимальное остовное дерево**:

- **Описание**: Подграф, соединяющий все вершины с минимальной суммой весов ребер.
- **Алгоритм Прима**:
    1. Начало с произвольной вершины.
    2. Добавление наименьшего ребра, соединяющего дерево с новой вершиной.
    3. Повторение до включения всех вершин.
- **Сложность**: O(E log V).

### 10. Алгоритм Крускала построения минимального остовного дерева

**Описание**: Алгоритм для построения минимального остовного дерева на основе сортировки ребер.

**Алгоритм**:

1. Сортировка всех ребер по весу.
2. Добавление ребер в остовное дерево, избегая циклов.
3. Повторение до включения всех вершин.
4. Использование структуры данных "Объединение-поиск" для проверки циклов.

**Сложность**: O(E log E) или O(E log V).

### 11. Поиск кратчайшего пути. Алгоритм Дейкстры

**Поиск кратчайшего пути**:

- **Описание**: Нахождение пути с наименьшей суммой весов ребер между двумя вершинами.

**Алгоритм Дейкстры**:

- **Описание**: Алгоритм для поиска кратчайшего пути от одной вершины до всех остальных в графе с неотрицательными весами ребер.
- **Алгоритм**:
    1. Инициализация расстояний до всех вершин бесконечностью, кроме начальной вершины (0).
    2. Использование приоритетной очереди для выбора вершины с наименьшим текущим расстоянием.
    3. Обновление расстояний до смежных вершин.
    4. Повторение до обработки всех вершин.
- **Сложность**: O(V^2) для матрицы смежности, O(E log V) для очереди с приоритетом.

### 12. Вычислительная геометрия. Элементарные задачи вычислительной геометрии

**Описание**: Раздел математики, изучающий алгоритмы для решения задач, связанных с геометрическими объектами.

**Элементарные задачи**:

- **Определение принадлежности точки многоугольнику**.
- **Вычисление площади многоугольника**.
- **Определение пересечения отрезков**.
- **Вычисление расстояния между точками, отрезками, и многоугольниками**.

### 13. Базовые алгоритмы вычислительной геометрии. Проблемы реализации

**Описание**: Основные алгоритмы, используемые для решения задач вычислительной геометрии.

**Алгоритмы**:

- **Алгоритм Грэхема**: Построение выпуклой оболочки.
- **Алгоритм Джарвиса**: "Обход подарочной упаковки" для выпуклой оболочки.
- **Алгоритм Сазерленда-Ходжмана**: Отсечение многоугольников.
- **Алгоритм Bentley-Ottmann**: Выявление пересечений отрезков.

**Проблемы реализации**:

- **Точность вычислений**: Избежание ошибок округления.
- **Эффективность**: Оптимизация времени выполнения и использования памяти.
- **Обработка вырожденных случаев**: Управление особенными случаями (например, совпадающими точками).

### 14. Алгоритмы вычисления выпуклой оболочки

**Описание**: Алгоритмы для нахождения минимального выпуклого многоугольника, содержащего все заданные точки.

**Основные алгоритмы**:

- **Алгоритм Грэхема**:
    1. Сортировка точек по углу относительно самой левой нижней точки.
    2. Построение оболочки с использованием стека.
    3. Сложность: O(n log n).
- **Алгоритм Джарвиса**:
    1. Выбор начальной точки.
    2. Последовательное добавление точек на оболочку.
    3. Сложность: O(nh), где h — количество точек на оболочке.

### 15. Алгоритмы выявления пересечений

**Описание**: Алгоритмы для нахождения точек пересечения между геометрическими объектами (отрезками, многоугольниками).

**Основные алгоритмы**:

- **Алгоритм Bentley-Ottmann**:
    1. Использование структуры данных "событийная очередь" для отслеживания событий начала и конца отрезков.
    2. Поддержка активного списка отрезков.
    3. Поиск пересечений в упорядоченном виде.
    4. Сложность: O((n + k) log n), где n — количество отрезков, k — количество пересечений.
- **Алгоритм Шеймоса-Хоэя**:
    1. Разделение пространства на вертикальные полосы.
    2. Проверка пересечений в каждой полосе.
    3. Сложность: O(n log n + k), где n — количество отрезков, k — количество пересечений.

### 16. Триангуляция многоугольников

**Описание**: Разбиение многоугольника на невырожденные треугольники.

**Алгоритмы**:

- **Алгоритм на основе ухо-закладывания**:
    1. Выбор и удаление "уха" многоугольника (треугольника, который не пересекает другие ребра).
    2. Повторение до тех пор, пока не останутся только треугольники.
    3. Сложность: O(n^2).
- **Алгоритм Делаунэя**:
    1. Поиск триангуляции, максимизирующей минимальные углы треугольников.
    2. Обеспечение, что никакая точка не находится внутри окружности, описанной вокруг любого треугольника триангуляции.
    3. Сложность: O(n log n).

### 17. Диаграмма Вороного, методы построения

**Описание**: Диаграмма Вороного делит пространство на области, каждая из которых состоит из всех точек, ближайших к одному из заданных объектов.

**Методы построения**:

- **Алгоритм Форчуна**:
    1. Использование "пляжной линии" для обработки событий (добавление новых точек, изменение границ диаграммы).
    2. Построение диаграммы по мере продвижения "пляжной линии".
    3. Сложность: O(n log n).
- **Алгоритм деления и завоевания**:
    1. Рекурсивное деление множества точек на подмножества.
    2. Построение диаграммы для подмножеств.
    3. Объединение поддиаграмм.
    4. Сложность: O(n log n).

### 18. Алгоритмы ЛА. Умножение матрицы на вектор

**Описание**: Линейная алгебра (ЛА) включает операции над матрицами и векторами.

**Умножение матрицы на вектор**:

- **Описание**: Процесс получения нового вектора путем применения матрицы к вектору.
- **Алгоритм**:
    1. Для матрицы A размером m x n и вектора x размером n, результатом является вектор y размером m.
    2. Каждый элемент вектора y вычисляется как скалярное произведение строки матрицы и вектора x.
    3. Сложность: O(mn).

### 19. LU разложение. Различные методы параллельной и последовательной реализации

**LU разложение**:

- **Описание**: Разложение матрицы A на произведение нижней треугольной матрицы L и верхней треугольной матрицы U.
- **Алгоритм**:
    1. Прямой метод: последовательное вычисление элементов L и U.
    2. Итерационные методы: использование подхода крошения для больших матриц.

**Параллельная реализация**:

- **Описание**: Распределение вычислений по нескольким процессорам.
- **Алгоритмы**:
    1. Разделение матрицы на блоки.
    2. Параллельное вычисление блоков L и U.
    3. Объединение результатов.
    4. Сложность: зависит от архитектуры, обычно O(n^3/p) при использовании p процессоров.

### 20. Прямой метод решения СЛАУ. Параллельная модификация

**Прямой метод решения СЛАУ**:

- **Описание**: Нахождение точного решения системы линейных алгебраических уравнений (СЛАУ).
- **Методы**:
    1. Метод Гаусса: пошаговое исключение переменных.
    2. Метод Крамера: использование определителей для решения.

**Параллельная модификация**:

- **Описание**: Ускорение решения за счет параллельных вычислений.
- **Алгоритмы**:
    1. Параллельный метод Гаусса: распределение строк матрицы по процессорам.
    2. Параллельный метод Крамера: параллельное вычисление определителей.
    3. Сложность: зависит от архитектуры, обычно O(n^3/p) при использовании p процессоров.

### 21. Мелкозернистый алгоритм потока данных для решения СЛАУ

**Описание**: Алгоритм, использующий мелкие задачи (grain tasks) для параллельного решения СЛАУ.

**Алгоритм**:

1. Разбиение матрицы и вектора на мелкие блоки.
2. Параллельная обработка каждого блока.
3. Объединение результатов для получения решения.

**Преимущества**:

- Улучшенная параллельная производительность.
- Снижение межпроцессорного взаимодействия.

**Сложность**: Зависит от степени разбиения и количества процессоров, обычно O(n^2) на блок.

### 22. Ленточные матрицы. Проблемы решения. Последовательный вариант алгоритма

**Описание**: Ленточные матрицы — это матрицы, у которых ненулевые элементы сосредоточены вдоль главной диагонали в виде полосы.

**Проблемы решения**:

- Эффективное хранение и обработка ненулевых элементов.
- Учет полосы при алгоритмических операциях.

**Последовательный вариант алгоритма**:

- **Методы**:
    1. Прямой метод: обработка ненулевых элементов по диагоналям.
    2. Итерационные методы: использование шаблонов для обработки блоков матрицы.
    3. Сложность: зависит от ширины полосы b, обычно O(nb^2).

### 23. Блочный метод решения ленточной матрицы для параллельной системы

**Описание**: Распределение ленточной матрицы на блоки для параллельного решения.

**Алгоритм**:

1. Разбиение матрицы на подматрицы (блоки).
2. Параллельное выполнение операций над блоками.
3. Объединение результатов для получения общего решения.

**Преимущества**:

- Улучшенная параллельная производительность.
- Эффективное использование памяти и процессоров.

**Сложность**: Зависит от размера блоков и количества процессоров, обычно O((n/b)^3) при использовании p процессоров.

### 24. Итерационные методы решения СЛАУ

**Описание**: Алгоритмы, использующие последовательные приближения для нахождения решения СЛАУ.

**Основные методы**:

- **Метод Якоби**:
    1. Разделение матрицы на диагональную, нижнюю и верхнюю части.
    2. Итеративное вычисление новых значений на основе предыдущих.
    3. Сложность: O(kn^2), где k — количество итераций.
- **Метод Гаусса-Зейделя**:
    1. Улучшение метода Якоби путем использования новых значений в текущих вычислениях.
    2. Сложность: O(kn^2).
- **Метод сопряженных градиентов**:
    1. Использование градиентного спуска для нахождения решения.
    2. Сложность: O(n^2) на итерацию, обычно сходится за O(n) итераций.

### 25. Алгоритмы нахождения собственных значений. Проблемы вычисления характеристического полинома

**Описание**: Нахождение собственных значений и векторов матрицы.

**Алгоритмы**:

- **Метод степенных итераций**:
    1. Итерационный метод для нахождения наибольшего собственного значения.
    2. Сложность: O(kn^2), где k — количество итераций.
- **QR-алгоритм**:
    1. Последовательное разложение матрицы на произведение ортогональной и верхней треугольной матриц.
    2. Сложность: O(n^3).
- **Метод Якоби**:
    1. Преобразование матрицы в диагональную с использованием вращений.
    2. Сложность: O(n^3).

**Проблемы вычисления характеристического полинома**:

- **Описание**: Определение полинома, корни которого являются собственными значениями.
- **Сложности**: Численные нестабильности, большие вычислительные затраты.

### 26. Методы Леверье и Крылова

**Метод Леверье**:

- **Описание**: Метод для вычисления характеристического полинома матрицы.
- **Алгоритм**:
    1. Последовательное вычисление коэффициентов полинома.
    2. Использование следов степеней матрицы.
    3. Сложность: O(n^4).

**Метод Крылова**:

- **Описание**: Метод для нахождения собственных значений и векторов на основе последовательных матричных векторных произведений.
- **Алгоритм**:
    1. Генерация последовательности Крылова.
    2. Решение системы линейных уравнений для нахождения коэффициентов.
    3. Сложность: O(n^3).

### 27. Задача интерполяции. Многочленная интерполяция. Формы Лагранжа и Ньютона

**Задача интерполяции**:

- **Описание**: Нахождение функции, проходящей через заданные точки.

**Многочленная интерполяция**:

- **Описание**: Использование многочленов для интерполяции.

**Форма Лагранжа**:

- **Описание**: Представление интерполяционного многочлена через базисные многочлены Лагранжа.
- **Алгоритм**:
    1. Вычисление базисных многочленов.
    2. Суммирование базисных многочленов с коэффициентами.
    3. Сложность: O(n^2).

**Форма Ньютона**:

- **Описание**: Представление интерполяционного многочлена через разделенные разности.
- **Алгоритм**:
    1. Вычисление разделенных разностей.
    2. Построение многочлена на основе разделенных разностей.
    3. Сложность: O(n^2).

### 28. Точки Чебышева. Число обусловленности. Феномен Рунге

**Точки Чебышева**:

- **Описание**: Специальные точки, используемые для уменьшения ошибок интерполяции.
- **Алгоритм**:
    1. Вычисление точек Чебышева на интервале.
    2. Использование точек для интерполяции.
    3. Сложность: O(n).

**Число обусловленности**:

- **Описание**: Мера чувствительности решения задачи к изменениям входных данных.
- **Вычисление**: Отношение наибольшего к наименьшему собственному значению матрицы.

**Феномен Рунге**:

- **Описание**: Ошибки интерполяции, возникающие при использовании многочленов высокой степени на равномерной сетке.
- **Решение**: Использование точек Чебышева или кусочно-интерполяционных методов.

### 29. Кусочно интерполяция. Сплайны. Кубическая интерполяция. Граничные условия

**Кусочно-интерполяция**:

- **Описание**: Разбиение интервала на подинтервалы и использование простых функций на каждом подинтервале.

**Сплайны**:

- **Описание**: Гладкие кусочно-интерполяционные функции.
- **Кубические сплайны**: Использование кубических многочленов для интерполяции.
- **Алгоритм**:
    1. Разбиение интервала на подинтервалы.
    2. Вычисление коэффициентов кубических многочленов.
    3. Сложность: O(n).

**Граничные условия**:

- **Типы**:
    1. Натуральные сплайны: вторые производные на концах интервала равны нулю.
    2. Закрепленные сплайны: заданы значения первых производных на концах интервала.
    3. Периодические сплайны: значения и производные на концах интервала совпадают.

### 30. Задача аппроксимации. Метод наименьших квадратов

**Задача аппроксимации**:

- **Описание**: Нахождение функции, минимально отклоняющейся от заданных точек.

**Метод наименьших квадратов**:

- **Описание**: Минимизация суммы квадратов отклонений.
- **Алгоритм**:
    1. Составление системы нормальных уравнений.
    2. Решение системы для нахождения коэффициентов аппроксимирующей функции.
    3. Сложность: O(n^2) для системы нормальных уравнений, O(n^3) для решения системы.

### 31. Численное дифференцирование – постановка задачи. Приближение на основе полинома Ньютона

**Численное дифференцирование**:

- **Описание**: Приближенное вычисление производных функций.

**Постановка задачи**:

- **Описание**: Нахождение производной функции в точке на основе значений функции в соседних точках.

**Приближение на основе полинома Ньютона**:

- **Описание**: Использование интерполяционных полиномов Ньютона для приближенного вычисления производной.
- **Алгоритм**:
    1. Построение интерполяционного многочлена Ньютона.
    2. Вычисление производной многочлена.
    3. Сложность: O(n^2).

### 32. Безразностные формулы приближенного дифференцирования

**Описание**: Методы численного дифференцирования, не использующие конечные разности.

**Алгоритмы**:

- **Метод наименьших квадратов**:
    1. Аппроксимация функции многочленом.
    2. Вычисление производной аппроксимирующего многочлена.
    3. Сложность: O(n^2).
- **Метод сплайнов**:
    1. Аппроксимация функции сплайнами.
    2. Вычисление производной сплайна.
    3. Сложность: O(n).

### 33. Метод неопределенных коэффициентов

**Описание**: Метод для нахождения численных значений производных и интегралов путем аппроксимации функции многочленами с неопределенными коэффициентами.

**Алгоритм**:

1. Выбор формы аппроксимирующего многочлена.
2. Определение коэффициентов многочлена на основе заданных условий.
3. Вычисление производной или интеграла многочлена.
4. Сложность: O(n^2).

### 34. Численное интегрирование. Постановка задачи

**Численное интегрирование**:

- **Описание**: Приближенное вычисление значения интегралов.

**Постановка задачи**:

- **Описание**: Нахождение значения определенного интеграла функции на заданном интервале.

**Методы**:

- **Квадратурные формулы**: Правило трапеций, правило Симпсона.
- **Алгоритмы**:
    1. Разбиение интервала на подинтервалы.
    2. Вычисление значений функции в узловых точках.
    3. Суммирование значений функции с весами.

**Сложность**: O(n) для простых методов, O(n^2) для методов высокой точности.
### 35. Интерполяционные квадратурные формулы

**Описание**: Методы численного интегрирования, основанные на интерполяции функции в узловых точках.

**Алгоритмы**:

- **Метод трапеций**:
    
    1. Интерполяция функции линейными многочленами.
    2. Вычисление интеграла от интерполяционного многочлена.
    3. Сложность: O(n).
- **Правило Симпсона**:
    
    1. Интерполяция функции квадратичными многочленами.
    2. Вычисление интеграла от интерполяционного многочлена.
    3. Сложность: O(n).
- **Методы Гаусса**:
    
    1. Интерполяция функции многочленами более высокой степени.
    2. Вычисление интеграла с использованием специально выбранных узловых точек (корни полиномов Лежандра, Чебышева и т.д.).
    3. Сложность: O(n).

### 36. Правило Рунге

**Описание**: Метод для оценки и уменьшения погрешности численного интегрирования и дифференцирования.

**Алгоритм**:

1. Вычисление результата численного метода с шагом h.
2. Повторное вычисление результата с шагом h/2.
3. Использование разности между результатами для оценки погрешности и корректировки результата.
4. Сложность: O(n).

**Применение**:

- **Для интегрирования**: Улучшение точности интегралов, вычисленных методом трапеций или правилом Симпсона.
- **Для дифференцирования**: Повышение точности численного вычисления производных.

### 37. Метод градиентного спуска

**Описание**: Итеративный метод оптимизации для нахождения минимума функции.

**Алгоритм**:

1. Инициализация начальной точки.
2. Вычисление градиента функции в текущей точке.
3. Перемещение точки в направлении, противоположном градиенту, на заданный шаг.
4. Повторение процесса до достижения критерия остановки (например, малый градиент, незначительное изменение функции).
5. Сложность: O(kn), где k — количество итераций.

**Применение**:

- **Машинное обучение**: Обучение моделей, минимизация функции потерь.
- **Обработка изображений**: Улучшение качества изображений, восстановление изображений.
- **Финансовый анализ**: Оптимизация портфелей, минимизация риска.
