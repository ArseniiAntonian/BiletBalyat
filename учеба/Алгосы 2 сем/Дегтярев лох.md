# 1. Поразрядная сортировка MSD

## **Суть поразрядной сортировки:**

Алгоритмы поразрядной сортировки рассматривают ключи, как числа, которые представлены в системе счисления с основанием R и работают с отдельными цифрами чисел

### MSD (most significant digit radix sort) - поразрядная сортировка по старшей цифре.

Идея состоит в том, чтобы выполнить следующие шаги для каждой цифры **i**, где значение **i** варьируется от старшей цифры до младшей:
- Храните элементы в разных корзинах в соответствии с их **i-м** разрядом.
- сортируйте каждую ячейку, содержащую более одного элемента.
- ![[Pasted image 20240611215607.png]]

### Проблемы настройки

- **если R принимает чрезмерно большое значение, то большая часть стоимости сортировки приходится на инициализацию и проверку корзин;**
- **если R недостаточно велико, то метод не использует своих потенциальных выгод, что достигается, если разделить исходный файл на максимально возможное число фрагментов.**

### Проблема пустых корзин
При сортировке случайных ключей количество ключей в каждом контейнере (размер подфайлов) после первого прохода в среднем будет равно N/R. На практике ключи могут не быть случайными (например, если ключи — это строки, представляющие собой слова на русском языке, то мы знаем, что лишь немногие из них начинаются с буквы й и совсем нет слов, начинающихся с буквы ъ), так что многие контейнеры окажутся пустыми, а некоторые из непустых контейнеров будут содержать больше ключей, чем остальные.
Пути решения:
- **Эвристика в масштабах корзины (bin-span-heuristics) - 
	- **First Fit (Первое подходящее)**:
	    - Элементы рассматриваются последовательно, и каждый элемент помещается в первый контейнер, который может его вместить во всех измерениях. Этот метод прост в реализации, но не всегда находит оптимальное решение.
	- **Best Fit (Лучшее подходящее)**:
	    - Элементы рассматриваются последовательно, и каждый элемент помещается в контейнер, который оставляет наименьшее свободное пространство после его добавления. Этот метод более вычислительно затратен, но часто дает лучшие результаты по сравнению с First Fit.
	- **Next Fit (Следующее подходящее)**:
	    - Элементы рассматриваются последовательно, и каждый элемент помещается в текущий контейнер, если он подходит, иначе открывается новый контейнер. Этот метод ограничивает количество открытых контейнеров, что может быть полезно в некоторых приложениях.**
- **Разработка более сложной реализации абстрактной операции доступа к конкретным байтам, которая учитывает любые специальные знания о сортируемых строках.**

### Сложность

Пусть значения разрядов меньше b, а количество разрядов — k. При сортировке массива из одинаковых элементов MSD-сортировкой на каждом шаге все элементы будут находится в неубывающей по размеру корзине, а так как цикл идет по всем элементам массива, то получим, что время работы MSD-сортировки оценивается величиной **O(nk)**, причем это время нельзя улучшить. Хорошим случаем для данной сортировки будет массив, при котором на каждом шаге каждая корзина будет делиться на b частей. Как только размер корзины станет равен 1, сортировка перестанет рекурсивно запускаться в этой корзине. Таким образом, асимптотика будет Ω(n logb(n)). Это хорошо тем, что не зависит от числа разрядов.

По памяти O(k), по сути O(n)
### **Условие поразрядной сортировки:**

Основное условие поразрядной сортировки состоит в том, чтобы мы могли разделить ключ, строку, последовательность байтов, которые мы могли бы сравнивать по отдельности, то есть выбирать какие-то определенные части и сравнивать их.

### **Абстракции для работы поразрядной сортировки (с чем она может работать):**

1. Байт - последовательность битов фиксированной длины

2. Строка - последовательность байтов переменной длины

3. Слово - последовательность байтов фиксированной длины

4. Ключ - число в системе счисления с основанием R, цифры которого пронумерованы. (организованы как последовательности байтов)

Все эти абстракции можно разделить на части, которые можно сравнивать

### Частный случай: двоичная поразрядная сортировка:

1. У каждого элемента изначально имеется ключ в двоичной системе счисления, выбираем самый большой разряд, по нему будем сортировать
2. Где единица - вниз, они больше, с 0 вверх, они меньше
3. Получили два блока, далее в каждом блоке берем второй разряд, он будет страшим и уже к каждому из блоков рекурсивно применяем алгоритм сортировки

Суть: Делим на 2 блока, с нулями и единицами в старшем разряде и рекурсивно сортируем по старшему разряду, пока не дойдем до конца ключа

Пример:
![[Pasted image 20240611222130.png]]

Суть: модификация двоичной поразрядной сортировки, только вместо двоичной СС, у нас СС, с основанием R, получается, что наш алгоритм делит на k частей, называемых корзинами и затем в каждой из этих корзин сортирует рекурсивно также разделяя на k частей, пока не дойдет до конца

### Преимущества и недостатки:

- Хорошо параллелится, при этом даже без этого является очень быстрой
- Требует дополнительную память и не всегда работает. Например, на знаковых числах (просто для разных знаков сортировать - неэффективно). Также не очень эффективен, когда много одинаковых элементов
# 2. Трех-путевая поразрядная быстрая сортировка
### Постановка задачи:

Допустим, мы выполняем сортировку строк.

Использовать qsort, который активно сравнивает элементы, выглядит слишком накладным — сравнение строк операция долгая. Да, мы можем написать свой компаратор, который будет несколько эффективнее. Но все же.

Использовать radix, который требует дополнительную память, тоже не слишком мотивирует — строки могут быть большими. Да и большая длина строк, т.е. число разрядов, удручающе сказываются на эффективности.
### Основная идея

**Приспособить быструю сортировку для MSD, используя трехпутевое разделение ключей по старшим байтам с переходом к следующему байту только в среднем подфайле.**

Это комбинация быстрой и поразрядной сортировки.

1. Берем опорный элемент.
2. Разделяем массив на три части, сравнивая элементы с опорным по старшему разряду — на меньшие, равные и большие.
3. В каждой из трех частей процедуру повторяем, начиная с шага №1, до тех пор, не дойдем до пустых частей или частей с 1 элементом.

Только в средней части (т.е. где старший разряд равен старшему разряду опорного элемента) переходим к следующему разряду. В остальных частях операция начинается без изменения «рабочего» разряда.

Пример:
![[Pasted image 20240611222618.png]]

### Сложность:

По сути работает схожим образом, как и быстрая сортировка, по методу “разделяй и властвуй”, поэтому сложность:

**Сложность сортировки — O(n*logn).**

**Дополнительная память — O(1).**

### Преимущества:

- По сравнению с быстрой в том, что нам не нужно выполнять сравнение каждого элемента целиком, что естественно ускоряет процесс
- По сравнению с поразрядной - не требует дополнительной памяти

### Недостатки:

- Более сложна в написании, чем быстрая сортировка
- Хуже параллелится по сравнению с поразрядной сортировкой 
# 3. Поразрядная сортировка LSD
### LSD (least significant digit radix sort) - поразрядная сортировка сначала по младшей цифре.

**Альтернативный метод поразрядной сортировки
› Сортируем по последней букве (используем метод подсчета индексных ключей)
› Сортируем по средней букве


**Работает только в том случае, если доказана устойчивость метода сортировки**

Идея: сортировать элементы не от большего разряда к меньшему, а наоборот

### Алгоритм:

1. Берем последние разряды чисел и поводим сортировку по ним, любой сортировкой, главное, чтобы она была устойчивой(наиболее эффективно, конечно, будет использовать сортировку подсчетом. Именно для нее будет потом рассчитана сложность)
2. Переходим к предпоследнему элементу, также сортируем
3. Производим операции до тех пор, пока не дойдем до конца.

### Почему сортировка должна быть устойчивой?

Если значения разрядов чисел не равны между собой, то элементы могут перемещаться между группами на каждом шаге. Поэтому при реализации LSD sort необходимо использовать стабильную сортировку внутри каждой группы.

Пояснение:

Предположим, что мы используем, неустойчивую сортировку, тогда элементы не сохраняют относительный порядок. Из этого следует, что относительный порядок, который мы задали, никак не будет задействован, из чего следует, что элементы, у которых, все одинаковы разряды, кроме, например, последнего, стоят так, что не удовлетворяют отсортированности, из чего можно сделать вывод, что сортировка не работает

### Сложность

Пусть m — количество разрядов, n — количество объектов, которые нужно отсортировать, T(n) — время работы устойчивой сортировки. Цифровая сортировка выполняет k итераций, на каждой из которой выполняется устойчивая сортировка и не более O(1) других операций. Следовательно время работы цифровой сортировки — O(kT(n)).

Рассмотрим отдельно случай сортировки чисел. Пусть в качестве аргумента сортировке передается массив, в котором содержатся n m-значных чисел, и каждая цифра может принимать значения от 0 до k−1. Тогда цифровая сортировка позволяет отсортировать данный массив за время O(m(n+k)), если устойчивая сортировка имеет время работы O(n+k). Если k небольшое, то оптимально выбирать в качестве устойчивой сортировки сортировку подсчетом.

Если количество разрядов — константа, а k=O(n), то сложность цифровой сортировки составляет O(n), то есть она линейно зависит от количества сортируемых чисел.

### Преимущества и недостатки:

1. Устойчива, довольно быстрая
2. Требует дополнительные затраты памяти и не всегда работает или делает это неэффективно(аналогично MSD)
# 4. Графы и их разновидности
### Граф - совокупность узлов и ребер, соединяющих эти узлы, как структура данных.

### Разновидности графов:

- **Неориентированный/ориентированный**

	G=(V,E) – неориентированный, если из (x,y)∈E следует, что (y,x) также является членом E. В противном случае граф – ориентированный.

- **Взвешенный/Невзвешенный**

	Каждому ребру/вершине взвешенного графа G присваивается числовое значение или вес.

- **Простые/Сложные**
	![[Pasted image 20240611223823.png]]
- **Разреженные/Плотные**
	![[Pasted image 20240611223930.png]]

- **Циклические/Ациклические**
	![[Pasted image 20240611224040.png]]

- **Явные/Неявные**
	![[Pasted image 20240611224112.png]]

- **Вложенные/Топологические**
	![[Pasted image 20240611224133.png]]

- **Помеченные/Непомеченные**
	![[Pasted image 20240611224258.png]]
- **Дерево**
	Дерево - любой связанный граф, не содержащий циклов
	
	Связанный граф - граф, между любой парой вершин которого существует хотя бы 1 путь
# 5. Обход графов, раскраска графов
## Обход графов
### **Обход графа - систематизированное посещение каждой вершины и каждого ребра графа.**

Каждая из вершин находится в одном из состояний: 
- Неоткрытая – первоначальное, нетронутое состояние вершины
- Открытая – вершина обнаружена, но не проверены все ее ребра
- Обработанная – все инцидентные данной вершине ребра были посещены

### Обход в ширину (breadth-first search, BFS)

Идея: берем, изначально, какую-нибудь вершину, помечаем ее. Далее мы берем для этой вершины все вершины, смежные с ней. Заходим в каждую. Если были в ней, то ничего не делаем, если в ней не были, то берем все вершины, смежные с данной. Далее то же самое повторяем, для вершин, которые взяли (т.е. мы каждый раз берем все смежные вершины данной, тем самым каждый раз расширяемся в ширину).

### **Обход в глубину (depth-first search, DFS)**

Идея: выбрать изначально любую вершину графа, и идти по какому-нибудь одному пути, помечая при этом все вершины, в которых были, пока не дойдем до вершины, которую уже посетили, или до вершины, из которой нельзя попасть в другие вершины. (т.е. постоянно спускаемся в глубь). Затем поднимаемся на одну вершину вверх, от той, которая оказалась конечной и повторяем то же самое

Замечание: оба алгоритма являются взаимозаменяемыми, но иногда какой-то из них использовать более выгодно чем другой.

## Раскраска графов

пусть G - некоторый граф, k - натуральное число. тогда раскраской графа называется функция f, которая каждой вершине графа G ставит в соответствие определенные номер {1,…, k}

Раскраска называется правильной, если любым двум смежным вершинам не соответствует одно и то же число, при этом k - минимальное число для этого графа.

_Пояснение:_ раскраска будет правильной, если у нас смежные вершины не раскрашены в один и тот же цвет, при этом мы задействовали минимальное количество цветов.(граф помечен определенным числом, можно визуализировать как граф раскрашен в определенный цвет)
![[Pasted image 20240611224857.png]]
## Двудольная раскраска графов

Задача: раскрасить граф в 2 цвета

Заметим, что если такая раскраска существует, и если зафиксировать цвет одной вершины, то все цвета всех достижимых из неё вершин определяются однозначно: пусть цвет этой вершины белый, тогда все её соседи будут иметь черный цвет, все вершины на расстоянии 2 будут иметь снова белый цвет, все вершины на расстоянии 3 снова черный, и так далее.

Раскрашивать граф можно обходом в глубину. На этот раз наш DFS будет принимать параметром цвет, в который нужно покрасить вершину, и он будет рекурсивно запускаться от всех соседей, крася их в противоположный цвет. По окончании работы алгоритма мы либо обнаружим, что граф не двудолен (мы когда-то посмотрели на две соседние вершины, которым нужно присвоить один и тот же цвет), либо найдём разбиение вершин графа на две доли.
**Пример**
 - Предположим, что в ВУЗе есть N преподавателей. Каждый должен прочитать определенное количество лекций по своему предмету, притом разные преподаватели могут читать свои лекции параллельно, один преподаватель так не может. Каждая лекция пусть занимает 1 час.
- Задача: составить расписание лекций так, чтобы они были прочтены для всех групп за минимальное время (т.е. по сути были прочтены за минимальное количество дней, значит надо сделать так, чтобы было как можно меньше окон)
- Пусть каждая лекция - это вершина графа, смежные вершины те, которые читает один и тот же преподаватель, смежные вершины раскрашиваем в разные цвета. Получаем некоторую раскраску, при этом, если вершины имеют 1 раскраску, то они могут читаться параллельно, значит их можно поставить в расписание на 1 время, если сделаем правильную раскраску, то по сути найдем максимальное количество лекций, которые можно ставить параллельно, при этом получим, что если мы так расставим лекции, то все лекции будут прочтены за минимальное количество часов

подгон от Коли:

[](https://www.youtube.com/watch?v=qoIEi0KA5Xc&t=336s&ab_channel=%D0%A3%D1%87%D0%B8%D0%BC%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%D0%B4%D0%B8%D1%81%D0%BA%D1%80%D0%B5%D1%82%D0%BD%D0%BE%D0%B9%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B8)[https://www.youtube.com/watch?v=qoIEi0KA5Xc&t=336s&ab_channel=Учималгоритмыдискретнойматематики](https://www.youtube.com/watch?v=qoIEi0KA5Xc&t=336s&ab_channel=%D0%A3%D1%87%D0%B8%D0%BC%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%D0%B4%D0%B8%D1%81%D0%BA%D1%80%D0%B5%D1%82%D0%BD%D0%BE%D0%B9%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B8)
# 6. Алгоритм обхода в ширину по сравнению с обходом в глубину
### **Обход в ширину (breadth-first search, BFS)**

**Идея**: берем, изначально, какую-нибудь вершину, помечаем ее. Далее мы берем для этой вершины все вершины, смежные с ней. Заходим в каждую. Если были в ней, то ничего не делаем, если в ней не были, то берем все вершины, смежные с данной. Далее то же самое повторяем, для вершин, которые взяли (т.е. мы каждый раз берем все смежные вершины данной, тем самым каждый раз расширяемся в ширину).

Получается, что при каждой операции мы проверяем, является ли вершина посещенной и, если нет, то собираем все вершины, смежные ей, и даем им самый низкий приоритет. Далее берем еще одну, с нее собираем смежные и даем им самый низкий приоритет, значит приоритет вершин, добавленных до этого надо увеличить. Таким образом, мы приходим к выводу, что для реализации обхода в ширину мы будем использовать структуру данных - очередь, в отличие от обхода в глубину, где мы использовать будем стек.

**Очередь** - обход в ширину. Помещая вершины в очередь типа FIFO, мы исследуем самые старые неисследованные вершины первыми. Таким образом, наше исследование медленно распространяется вширь, начиная от стартовой вершины.

### Итеративный алгоритм

1. Выбираем вершину и кладем ее в очередь
2. Достаем из очереди вершину, помечаем ее как открытую
3. Если вершина изначально не была открытой, то кладем в очередь все вершины, смежные ей
4. Повторяем шаги 2, 3 до тех пор, пока очередь не окажется пустой

### Свойства при обходе в ширину

- _Сохранение порядка, в котором мы открывали вершины графа, и пути позволяет определить минимальный путь от исходной вершины до любой заданной (поскольку у каждой вершины есть только один родитель)._
- Ребра графа, которые не включены в дерево обхода в ширину, также имеют особые свойства. Для неориентированных графов, не попавшие в дерево ребра могут указывать только на вершины на том же уровне, что и родительская вершина, или на вершины, расположенные на уровень ниже. Эти свойства естественно следуют из того факта, что каждое ребро в дереве должно быть кратчайшим путем в графе (есть вершина, если с вершиной, смежной с ней, изначально нет ребра в дереве вершин, то тогда значит, что смежная вершина или на том же уровне с деревом или на 1 уровень ниже).
- Для ориентированных графов ребро (u, v), указывающее в обратном направлении, может существовать в любом случае, когда вершина v расположена ближе к корню, чем вершина u.(у нас есть ребро (u,v). Если v ближе к корню чем u, то ребро (u, v) обязательно может существовать, хотя это не гарантируется)

### Преимущества обхода в ширину, по сравнению с обходом в глубину

- Одним из основных преимуществ обхода в ширину по сравнению с обходом в глубину является то, что он может быть использован для поиска кратчайшего пути в невзвешенном графе. Обход в ширину гарантирует, что кратчайший путь будет найден, если все ребра имеют одинаковый вес. Кроме того, обход в ширину может быть более эффективным при работе с графами, у которых много ребер, но мало уровней.
- Еще одним преимуществом обхода в ширину является то, что он может быть использован для поиска компонент связности в неориентированном графе. Обход в ширину позволяет найти все вершины, которые достижимы из заданной вершины, и таким образом определить компонент связности (максимального связного подграфа).
- Кроме того, обход в ширину может быть более удобным для работы с графами, у которых нет определенной структуры или которые содержат циклы. Обход в ширину позволяет посетить все вершины графа, не застревая в циклах или повторно посещая вершины.
# 7. Алгоритм обхода в глубину по сравнению с обходом в ширину
### **Обход в глубину (depth-first search, DFS)**

**Идея**: выбрать изначально любую вершину графа, и идти по какому-нибудь одному пути, помечая при этом все вершины, в которых были, пока не дойдем до вершины, которую уже посетили, или до вершины, из которой нельзя попасть в другие вершины. (т.е. постоянно спускаемся вглубь). Затем поднимаемся на одну вершину вверх, от той, которая оказалась конечной и повторяем то же самое.

Получается, что мы берем одну вершину, далее, собираем все смежные вершины и даем им максимальный приоритет, переходи-м в одну из них, если не открытая, то снова собираем все смежные вершины и даем им максимальный приоритет. Тем самым приходим к выводу, что нам необходимо использовать стек (или рекурсию, как замену ему)

**Стек** – обход в глубину. Помещая вершины в стек с порядком извлечения LIFO, мы исследуем их, отклоняясь от пути для посещения очередного соседа, и возвращаясь назад, только если оказываемся в окружении ранее открытых вершин. Таким образом, мы в своем исследовании быстро удаляемся от стартовой вершины.

### Итеративный алгоритм

1. Берем вершину и кладем (!!!) ее в стек
2. Вытаскиваем из стека и отмечаем, как открытую
3. Если до этого вершина была закрытой, то кладем в стек все вершины, смежные с ней
4. Повторяем шаги 2-3 до тех пор, пока стек не будет пуст

### Рекурсивный алгоритм

1. берем первую вершину, и запускаем для нее функцию проверки на открытость
2. Если вершина открытая, то завершаем, если нет, то помечаем, как открытую и для каждой вершины, смежной ей запускаем рекурсивно ту же функцию
3. Алгоритм завершится, когда стек рекурсий будет пуст

### Свойства при обходе в глубину

- Посещение предшественника. Если вершина х является предшественником вершины у в дереве обхода в глубину, то временной интервал посещения у должен быть корректно учтен его предшественником
- Количество потомков. Разница во времени выхода и входа для вершины у свидетельствует о количестве потомков этой вершины в дереве обхода в глубину.

**Обход в глубину разбивает ребра на два класса:**

- древесные (tree edges) - используются при открытии новых вершин и закодированы в родительском отношении
- обратные (back edges) - второй конец является предшественником расширяемой вершины

### Преимущество обхода в глубину по сравнению с обходом в ширину

- Один из главных преимуществ обхода в глубину по сравнению с обходом в ширину заключается в том, что он использует меньше памяти. В обходе в ширину необходимо хранить все вершины на текущем уровне, а также все вершины на предыдущих уровнях, что может потребовать значительных объемов памяти при работе с большими графами. В обходе в глубину же используется стек для хранения вершин, что позволяет эффективно использовать память и работать с графами большого размера.
- Кроме того, обход в глубину может быть более эффективен при поиске определенных типов путей или структур в графе, таких как циклы, деревья или компоненты связности. Обход в глубину также может быть более простым и интуитивно понятным для реализации, особенно для начинающих программистов.
# 8. Обход ориентированных графов, топологическая сортировка
## Обход ориентированных графов

Для ориентированных графов обход в глубину и ширину по идее схожи. Итак, при работе с ориентированными графами часто бывает такая ситуация, что граф не является связным, значит до некоторых вершин алгоритм может не дойти, если мы выберем определенную вершину (мы можем выбирать любую вершину для начала работы алгоритма). Значит, чтобы захватить все вершины, необходимо после отработки алгоритма запустить его ещё раз, но в этот раз взять за корень не посещенную вершину.

### Алгоритм:

1. Проверяем посещена ли вершина
2. Если не посещена, то запускаем DFS(BFS) c корнем в этой вершине
3. повторяем шаги 1, 2 до тех пор, пока все вершины не будут посещены

### Применение обхода в глубину для определения вида ребра в ориентированном графе

Древесный граф - это граф, который является связным и не имеет циклов. Древесное ребро - это ребро, которое принадлежит древесному графу и соединяет две вершины в этом графе, при этом не является частью цикла. То есть, если удалить древесное ребро, граф разобьется на два подграфа.

При обходе в глубину неориентированного графа DFS разделяет все ребра на древесные и обратные. При обходе в глубину для ориентированного графа разделение ребер идет на 4 класса:
![[Pasted image 20240611230114.png]]

Также при помощи этого алгоритма можно определять тип ребра в ориентированном графе, это является очень полезным при разработке алгоритмов для работы с ориентированными графами

### Алгоритм определения типа ребра в графе:
![[Pasted image 20240611230135.png]]
**Пояснение**: во время работы алгоритма мы перешли в какую-то вершину, мы ее зафиксировали и передаем в функцию, как вершину y. Вершина x - вершина, которая имеет с y ребро(может, кстати, быть такая ситуация, что ребро есть, а какая-то вершина не найдена, но она будет найдена потом, поэтому в общем случае мы рассматриваем в качестве x не все вершины, а только найденные).

1. Если x - родитель y, то ребро древесное
2. Если вершина была посещена (метка посещения ставится после вызова функции) до этого, при этом не является обработанной (не все вершины, для который она родитель посещены), то ребро обратное
3. Если обработана и при этом время нахождения вершины y больше x, то ребро прямое
4. Если время y меньше времени x, то ребро поперечное

## Топологическая сортировка

Задача: упорядочить вершины вдоль линии таким образом, что все ориентированные ребра направлены слева направо

_Замечание:_ такое упорядочивание ребер невозможно в графе, содержащем ориентированный цикл, так как в таком графе не существует линейного порядка вершин. Любой, бесконтурный ориентированный граф (не содержит обратных ребер) имеет, по крайней мере, одно топологическое упорядочивание

### Зачем нужна?

Важность топологической сортировки состоит в том, что она позволяет упорядочить вершины графа таким образом, что каждую вершину можно обработать перед обработкой ее потомков. Допустим, что ребра представляют управление очередностью таким образом, что ребро (х, у) означает, что работу х нужно выполнить раньше, чем работу у. Тогда любое топологическое упорядочивание определяет правильное календарное расписание. Более того, бесконтурный орграф может содержать несколько таких упорядочиваний.

### Процедура топологической сортировки

- **Если вершина y не открыта, то начинаем обход в глубину из вершины у, прежде чем можем продолжать исследование вершины х**
- **Если вершина у открыта, но не обработана, то ребро (х,у) является обратным ребром, что запрещено в бесконтурном ориентированном графе**
- **Если вершина у обработана, то она помечается соответствующим образом раньше вершины х**


# 9. Обход взвешенных графов. Минимальное остовное дерево. Алгоритм Прима

### Остовное дерево - подмножество ребер Е, которые создают дерево, содержащее все вершины графа V.
![[Pasted image 20240611232350.png]]
Наибольший интерес представляет минимальное остовное дерево - остовные деревья с минимальной суммой весов ребер
Минимальные остовные деревья позволяют решить задачу, в которой требуется соединить множество точек (представляющих города, дома, перекрестки и другие объекты) наименьшим объемом дорожного полотна, проводов, труб и т. п. Любое дерево - это в сущности, наименьший (по количеству ребер) возможный связный граф, в то время как минимальное остовное дерево является наименьшим связным графом по весу ребер.

### Алгоритм Прима (алгоритм построения минимального остовного дерева):

1) Начинаем с указанной вершины
2) Вставляем в остовное дерево одну новую вершину, исходя из “жадного” принципа (из множества рассмотренных ребер к дереву добавляется ребро с наименьшим весом)

_Пояснение:_
1. Берем первую вершину, ищем ребро, исходящее из нее с минимальным весом
2. Добавляем вершину, с котором ребро связывает исходную вершину, к остову
3. Ищем из множества ребер, которые идут из множества вершин, входящих в остов, минимальное ребро, которое соединяет любую вершину остова с вершиной не из остова и добавляем новую вершину
4. повторяем 3 снова, до тех пор, пока не закончатся вершины
![[Pasted image 20240611232509.png]]
![[Pasted image 20240611232707.png]]
![[Pasted image 20240611232730.png]]
![[Pasted image 20240611232913.png]]
### Анализ эффективности алгоритма Прима

Зависит от используемых структур данных Если n – исполняемых циклов; m – просматриваемых ребер в каждом цикле

O($n^2$) – «наивная» реализация O(m + n*lgn) – применение структуры данных в виде кучи с приоритетами (двоичная куча, Фибоначчиева куча)

_Пояснение:_ если использовать “наивную” реализацию, то при поиске минимального ребра мы каждый раз перебираем все ребра, которые нам доступны. При использовании очереди с приоритетам мы просто закидываем в нее все ребра, которые нам доступны, временная сложно закидывания всех ребер будет n*lgn, а получать минимальное ребро мы будем за константу, ну и соответственно добавить все вершин - m операций
# 10. Алгоритма Крускала построения минимального остовного дерева
### опиСАСАНИЕ 

Алгоритм Крускала изначально помещает каждую вершину в своё дерево, а затем постепенно объединяет эти деревья, объединяя на каждой итерации два некоторых дерева некоторым ребром. Перед началом выполнения алгоритма, все рёбра сортируются по весу (в порядке неубывания). Затем начинается процесс объединения: перебираются все рёбра от первого до последнего (в порядке сортировки), и если у текущего ребра его концы принадлежат разным поддеревьям, то эти поддеревья объединяются, а ребро добавляется к ответу. По окончании перебора всех рёбер все вершины окажутся принадлежащими одному поддереву, и ответ найден.
### Алгоритм:

1) Первоначально каждая вершина – отдельный компонент будущего дерева  
2) Последовательно ищем ребро для добавления в расширяющийся лес путем поиска самого легкого среди соединяющих два дерева в лесу  
3) Выполняется проверка на нахождение обеих конечных точек ребра - кандидата в одной и той же связанной компоненте

_Пояснение:_

1. сортируем все ребра графа от наименьшего к наибольшему(это можно сделать обычной сортировкой или просто закинуть их все в очередь с приоритетом)
2. Если вершины, которые данное ребро соединяет: *
    1. оба находятся в одном множестве, то пропускаем ребро
    2. обе вершины не принадлежат ни одному из множеств - соединяем их и образуем новое множество
    3. если вершины принадлежат разным множествам - сливаем множества в одно
    4. если одна вершина принадлежит какому-то множеству, а вторая не принадлежит, то просто добавляем к множеству новую вершину
3. Повторяем шаг 2 до тех пор, пока не закончатся вершины

*проверку всех случаев можно сделать при помощи обхода в глубину или ширину

### Модификация DSU (система непересекающихся множеств):

Эта структура данных предоставляет следующие возможности. Изначально имеется несколько элементов, каждый из которых находится в отдельном (своём собственном) множестве. За одну операцию можно **объединить два каких-либо множества**, а также можно **запросить, в каком множестве** сейчас находится указанный элемент.
Множества элементов мы будем хранить в виде **деревьев**: одно дерево соответствует одному множеству. Корень дерева — это представитель (лидер) множества.
При реализации это означает, что мы заводим массив parent, в котором для каждого элемента мы храним ссылку на его предка в дерева. Для корней деревьев будем считать, что их предок — они сами (т.е. ссылка зацикливается в этом месте).

**объединение:** Чтобы объединить два множества (операция ), мы сначала найдём лидеров множества, в котором находится , и множества, в котором находится . Если лидеры совпали, то ничего не делаем — это значит, что множества и так уже были объединены. В противном случае можно просто указать, что предок вершины  равен  (или наоборот) — тем самым присоединив одно дерево к другому.
Проверка на принадлежность 1 множеству: операции поиска лидера find() проста: мы поднимаемся по предкам от вершины , пока не дойдём до корня, т.е. пока ссылка на предка не ведёт в себя. Эту операцию удобнее реализовать рекурсивно.

### Анализ эффективности алгоритма Крускала

**Зависит от используемых структур данных Если n – вершин; m – ребер O(m lg m) – время упорядочивания ребер (c применением DSU)**
**O(m n) – время исполнения при реализации поиска в ширину или глубину**
![[Pasted image 20240611233054.png]]
![[Pasted image 20240611233119.png]]
![[Pasted image 20240611233615.png]]
![[Pasted image 20240611233646.png]]

# 11. Поиск кратчайшего пути. Алгоритм Дейкстры
### **Путь – последовательность ребер, соединяющих две вершины**

**Задача:** Для заданного взвешенного графа G = (V, E) найти кратчайшие пути (с минимальным весом) из заданной вершины s до всех остальных вершин. Веса всех рёбер неотрицательны.

**Алгоритм:** В ориентированном взвешенном графе, вес рёбер которого неотрицателен w: E→ℝ, алгоритм Дейкстры находит длины кратчайших путей из заданной вершины s до всех остальных. В алгоритме поддерживается множество вершин U, для которых уже вычислены длины кратчайших путей до них из s. На каждой итерации выбирается вершина u∉U, которой не соответствует минимальная оценка кратчайшего пути. Вершина u добавляется в множество U и производится релаксация всех исходящих из неё рёбер.

_Пояснение:_

Изначально у нас есть 2 точки, надо попасть из одной в другую. Пусть расстояние от начальной вершины, до каждой вершины графа $+\infty$

1. выбираем все ребра, которые соединяют текущую вершину и смежные. Расстояние от начальной вершины до данной равно L. Пусть расстояние от текущей до смежной равно $m_i$, а расстояние от начальной вершины до вершин, смежных с L - $l_i$.
2. Если мы рассматриваем вершину L, то мы уже точно нашли кратчайший путь от начальной вершины до этой и он равен L. для каждой вершины сравниваем L+$m_i$ и $l_i$
3. Если $L+m_i<l_i$, то обновляем минимальное расстояние от начальной вершины до данной смежной вершины
4. Далее либо используем BFS и кидаем все вершины в очередь, либо DFS(обойти все равно придется весь граф, поэтому разница, что использовать - нет)
5. Повторяем 1-4 до тех пор, пока не обойдем весь граф и не дойдем до нужной вершины

_Замечание:_ Алгоритм Дейкстры работает правильно только на графах, в которых нет ребер с отрицательным весом. Дело в том, что при построении пути может встретиться ребро с отрицательным весом настолько большим по модулю, что оно полностью изменит оптимальный путь от вершины s к какой-то другой вершине, которая уже включена в дерево. Образно говоря, самым выгодным путем к соседу по лестничной клетке может оказаться путь через банк на другом конце города, если этот банк выдает за каждое посещение достаточно большое вознаграждение, делающее такой маршрут выгодным.
### Эффективность алгоритма Дейкстры
![[Pasted image 20240611233820.png]]![[Pasted image 20240611234238.png]]



# 12. Вычислительная геометрия. Элементарные задачи вычислительной геометрии
### Вычислительная геометрия — раздел информатики, в котором рассматриваются алгоритмы для решения геометрических задач.

### Области применения вычислительной геометрии:

- Компьютерная графика
    - пересечения примитивов
    - удаление невидимых поверхностей, освещение
- Робототехника
    - кинематика
- Геоинформационные системы
    - интерполяция, хранение данных, работа со слоями
- CAD/CAM/CAE
    - CAD (computer-aided design) - проектирование с помощью ЭВМ
    - CAM (computer-aided manufacturing) - компьютерная поддержка производства
    - CAE (computer-aided engineering) - класс продуктов для компьютерной поддержки расчетов и инженерного анализа
- и другие ….

### Элементарные задачи вычислительной геометрии

- Находится ли точка p на отрезке? Если не находится, то с какой стороны?
- Пересекаются ли два отрезка $l_1$ и $l_2$?
- Триангуляции треугольников
- Поиск ближайшей точки
- Выявление пересечений

### Что важно в задачах вычислительной геометрии

- Понимание геометрических свойств задачи
- Правильное применение алгоритмов и структур данных

### Примеры задач вычислительной геометрии

- Пример 1
Представьте, что вы идете по территории университетского городка (кампуса) и внезапно вспоминаете, что должны срочно позвонить. На территории много телефонных будок и, естественно, вам нужна ближайшая. Но какая из них ближайшая? Хорошо бы иметь карту, на которой можно найти ближайшую будку, в какой бы точке кампуса вы ни находились. На такой карте было бы показано разбиение кампуса на области и для каждой из них – ближайшая телефонная будка. Как выглядят такие области? И как их построить?

- Пример 2
Допустим, вы нашли ближайшую телефонную будку. С картой кампуса в руках вы, надо полагать, без особого труда найдете достаточно короткий путь в обход стен и других препятствий. Но вот запрограммировать робота для решения той же задачи будет посложнее. И в этом случае задача имеет геометрическую природу: при заданном наборе геометрических препятствий найти кратчайший путь между двумя точками, избегающий столкновений с препятствиями.

- Пример 3
Допустим, что у вас не одна карта, а две: на одной нанесены различные здания, в т. ч. и телефонные будки, а на другой – дороги на территории кампуса. Чтобы спланировать маршрут к будке, мы должны наложить карты друг на друга, т. е. объединить содержащуюся в них информацию.

# 13. Базовые алгоритмы вычислительной геометрии. Проблемы реализации

### **Геометрическая вырожденность – частные случаи, требующие специальных подходов.**

Решения:

1. Игнорирование
    Пренебрежение вырожденными случаями, преимущество в том, что нет необходимости совершенствовать и усложнять алгоритм, но могу возникать проблемы с правильностью результатов
    
2. Подделка невырожденности
    Немного подкорректировать случай так, чтобы он оказался вырожденным, преимущества и недостатки аналогичны игнорированию
    
3. Обработка вырожденности
    Создавать определенные условия работы алгоритмов, для обработки вырожденных случаев, преимущества в том, что в таком случае ответ будет наиболее правильным, но может сильно усложниться алгоритм.
    

### **Численная неустойчивость – случаи, когда применение арифметических операций может привести к проблемам переполнения или потери точности.**

Решения:

1. Использование целочисленных арифметических операций
2. Использование действительных чисел двойной точности
3. Использование арифметических операций произвольной точности

## Базовые алгоритмы

### 1. Вычисление площади треугольника

![[Pasted image 20240611234740.png]]
### Почему работает?

Площадь треугольника $(a_x,a_y), (b_x, b_y), (c_x, c_y)$ - координаты вершин треугольника на плоскости. Площадь треугольника, построенного на векторах можно вычислить через определитель, площадь треугольника = $\frac{1}{2}$ площади параллелограмма

Стороны на которых построен параллелограмм: $(a_x - b_x,a_y - b_y), (b_x - c_x, b_y - c_y)$

$$ \begin{vmatrix} a_x - b_x& b_x - c_x\\ a_y - b_y& b_y - c_y\end{vmatrix} = (a_x - b_x)(b_y - c_y) - (b_x - c_x)(a_y - b_y) = \begin{vmatrix} a_x & a_y & 1\\ b_x & b_y & 1 \\ c_x & с_y & 1\end{vmatrix} $$

Объем треугольной пирамиды $(a_x,a_y), (b_x, b_y), (c_x, c_y), (d_x, d_y)$ - координаты вершин пирамиды (это второй определитель)

### 2. Выяснение местоположения точки

![[Pasted image 20240611234805.png]]
_Пояснение:_ если определитель больше 0, то площадь треугольника положительно ориентирована, значит минимальный поворот от одного вектора к другому против часовой стрелки, значит точка с - выше прямой, с < 0 аналогично, если 0, то координаты точек пропорциональны, значит лежат на 1 прямой

### 3. Проверка нахождения точки внутри круга
![[Pasted image 20240611234826.png]]

# 14. Алгоритмы вычисления выпуклой оболочки

### **1. В**ыпуклая оболочка

Множество S на плоскости называется выпуклым, если для любых двух точек p,q∈S весь отрезок pq принадлежит S.

**_Выпуклая оболочка_ – конечное множество Р точек однозначно определенного многоугольника, вершинами которого являются точки, принадлежащие Р, и который содержит все точки Р.**
![[Pasted image 20240611235006.png]]
### Вычисление выпуклой оболочки $O(n^3)$

![[Pasted image 20240611235036.png]]

_Пояснение:_

берем 2 точки, через них проходит ровно 1 прямая, если существует точка, которая находится левее данной прямой, то, очевидно, что данный отрезок не будет принадлежать выпуклой оболочке, значит мы его выбрасываем. Берем следующие 2 точки, и так далее, пока прямые не закончатся. Получим множество отрезков, которые и составляют выпуклую оболочку

### Convex Hull ($nlog_2n)$

![[Pasted image 20240611235112.png]]

Идея: Построить сначала нижнюю оболочку, затем верхнюю и соединить их, получив необходимую нам выпуклую оболочку

Псевдокод инкрементного алгоритма:

![[Pasted image 20240611235245.png]]
_Пояснение:_

1. сортируем точки по координате x
2. начинаем строить верхнюю часть
3. добавляем первые 2 точки в множество выпуклой оболочки
4. обозначим последние 2 добавленные точки p и q
5. берем следующую точку m
6. считаем определитель с точками p, q, m
7. если > 0, то m лежит выше прямой, проходящей через точки p и q, тогда удаляем точку q и за p и q снова берем последние точки принадлежащие множеству
8. Повторяем шаги 6-7 до тех пор, пока в множестве не останется 2 точки или когда определитель не будет ≤ 0
9. добавляем в множество точку m
10. повторяем шаги 3-9 до тех пор, пока не закончатся точки
11. Аналогично, с учетом знака строим нижнюю оболочку
12. объединяем оболочки

### Проблемы при построении выпуклой оболочки

- С каким количеством измерений мы имеем дело?

сложность с увеличением размерности

- Данные представлены в виде вершин или полупространств?

задача поиска пересечения набора n полупространств в d-мерном пространстве двойственна поиску выпуклых оболочек из n точек в d-мерном пространстве

- Сколько точек может содержать оболочка?

удаление точек заведомо находящихся внутри

- Как выяснить форму данного набора точек?

потеря подробности O/G, альфа-очертания

### Теорема выпуклой оболочки. **Выпуклую оболочку множества n точек на плоскости можно вычислить за время О(n logn).**

Такая оценка возникает, когда используется алгоритм "разделяй и властвуй", основанный на рекурсивном делении множества точек на полуплоскости. В этом случае выпуклая оболочка рассматривается как объединение выпуклых многогранников, полученных рекурсивным делением исходного множества. Каждый этап рекурсии занимает O(n), а общее количество таких этапов не превосходит O(log n), поскольку максимальное количество полуплоскостей, на которые может быть разбита выпуклая оболочка, равно n.

# 15. Алгоритмы выявления пересечений


![[Untitled-7.png]]
### Проблемы в задаче выявления пересечений:

- Вычисление местонахождения пересечения или сам его факт ?
- Выявление пересечения прямых или отрезков ?
- Ожидаемое количество пересечений ?
- Видна ли точка x из точки y ?
- Являются ли пересекающиеся объекты выпуклыми ?
- Выполняется ли многократный поиск пересечений с одними и теми же основными объектами ?

### Алгоритм заметания плоскости

![[Untitled-6.png]]
Идея алгоритмов этого типа заключается в представлении себе воображаемой прямой (чаще вертикальной), которая движется по плоскости, останавливаясь в некоторых точках. Геометрические операции ограничены геометрическими объектами, которые или пересекаются, или примыкают к выметающей прямой, а полное решение доступно, когда прямая пройдёт через все объекты.

_Пояснение идеи:_ прямая движется слева направо, если встречает начало или конец отрезка, а также точку пересечения, то обрабатывает это событие, естественно, что мы не можем использовать непрерывную величину, значит будем прыгать по этим трем событиям

Объяснение алгоритма (первые 30 минут): [https://youtu.be/sINIi2mwYls](https://youtu.be/sINIi2mwYls) (новосибирск сила)

Подробное описание алгоритма (со страницы 27): [dmitr (ipo.spb.ru)](http://ipo.spb.ru/journal/content/896/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B%20%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D0%B9%20%D0%B3%D0%B5%D0%BE%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%B8.%20%D0%9F%D0%B5%D1%80%D0%B5%D1%81%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BE%D1%82%D1%80%D0%B5%D0%B7%D0%BA%D0%BE%D0%B2:%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%20%D0%B7%D0%B0%D0%BC%D0%B5%D1%82%D0%B0%D0%BD%D0%B8%D1%8F%20%D0%BF%D0%BB%D0%BE%D1%81%D0%BA%D0%BE%D1%81%D1%82%D0%B8..pdf)

### Псевдокод(если не понимаешь, посмотри видео)

![[Untitled-5.png]]
![[Untitled-4.png]]
### Структуры данных

- Очередь событий (приоритетная очередь) - очередь, упорядоченная по x(y) координате, всех возможных будущих событий: вставок, удалений, пересечений.
- Структура состояний - динамическая структура, используется для доступа к соседям данного отрезка s, чтобы можно было проверить, пересекаются ли они с s.

![[Untitled-3.png]]
![[Untitled-2.png]]
![[Untitled.png]]
Время работы: O(n*log n+I*logn)

# 31. Численное дифференцирование - постановка задачи. Приближение на основе полинома Ньютона
### Численное дифференцирование — постановка задачи.

**Численное дифференцирование** — совокупность методов приближённого вычисления значения производной некоторой функции, заданной таблично или имеющей сложное аналитическое выражение.

Например, необходимость в численном дифференцировании возникает в том случае, когда функция задана не формулой, а таблицей или алгоритмом вычисления в произвольной точке.

Основной подход при построении формул численного дифференцирования — это аппроксимация функции.

Предположим, что в окрестности точки $x$ функция $f(x)$ аппроксимируется некоторой другой функцией $f_n(x)$, причем $k$-тая производная $f_n^{(k)}(x)$(x) легко вычисляется. Естественно в этом случае воспользоваться приближенной формулой

$$ f^{(k)}(x) \approx f_n^{(k)} (x) $$

Приближение $f_n(x)$ может быть построено любым рассмотренным ранее методом. Например, в виде интерполяционного полинома или сплайна.

Если для интерполирующей функции P(x) известна погрешность

$$ R(x)=f(x)-P(x) $$

то погрешность производной P’(x) выражается формулой

$$ r(x)=f'(x)-P'(x)=R'(x) $$

т.е. погрешность производной интерполирующей функции равна производной от погрешности этой функции.

_**Приближенное дифференцирование представляет собой операцию менее точную, чем интерполирование.**_

## **Приближение на основе полинома Ньютона**

При численном дифференцировании на основе полинома Ньютона используется аппроксимация производной функции с использованием интерполяционного полинома Ньютона. Этот метод позволяет приближенно вычислить значение производной функции в заданной точке, используя значения функции вблизи этой точки.

Рассмотрим интерполяционный полином Ньютона P(x) на основе набора узловых точек $\{x_0, x_1, ..., x_n \}$ и соответствующих значениях функции $\{f(x_0), f(x_1), ..., f(x_n)\}$.

Для вычисления приближенного значения производной функции в точке x используется следующая формула:

$$ f'(x) \approx P'(x) = f(x_0)+[x_0;x_1](x-x_0)+[x_0;x_1;x_2](x-x_0)(x-x_1)+...++[x_0;...;x_{n-1}](x-x_0)(x-x_1)...(x-x_{n-2}) $$

где $f'(x)$ - приближенное значение производной функции в точке $x, f[x_0]$ - значение функции в узловой точке $x_0, f[x_0;x_1] -$ разделенная разность первого порядка (по определению, $f[x_0;x_1]= \frac {y_0-y_1} {x_0-x_1}$, если не очень понятно - смотри 27 билет)

По презентации Дегтярева:

![[Pasted image 20240612152956.png]]

![[Pasted image 20240612152946.png]]

Использование односторонних значений функции при численном дифференцировании может привести к неточным результатам, особенно вблизи точек разрыва или экстремумов функции. Это связано с тем, что при использовании односторонних значений производной не учитывается симметричность функции относительно точки, а также не учитываются значения функции на противоположной стороне точки. В результате, производная может оказаться завышенной или заниженной, что может привести к ошибкам в дальнейшем анализе функции. Чтобы избежать этой проблемы, рекомендуется использовать центральные разности при численном дифференцировании, которые учитывают значения функции на обеих сторонах точки.

# 32. Безразностные формулы приближенного дифференцирования
Когда производную аналитически заданной функции по причине ее сложности искать затруднительно, либо выражение для производной приобретает неудобную для применения форму, используется приближенное или численное дифференцирование. Этот метод тем более необходим, если исходная функция задана таблично. Один из способов решения задачи дифференцирования – использование интерполяционных многочленов.

Пусть _f(x)_ – функция, для которой нужно найти производную в заданной точке отрезка [a,b], $F_n(x)$– интерполяционный многочлен для _f(x)_, построенный на отрезке [a,b]. Заменяя _f(x)_ интерполяционным многочленом $F_n(x)$, получим значение производной _f(x)_ на отрезке [a,b] как значение производной интерполяционного многочлена, т.е. примем приближенно $f^\prime(x) \approx F^\prime_n(x)$ (1)

Аналогичным путем можно поступать при нахождении значений производных высших порядков функции _f(x)_.

Полагая, что погрешность интерполирования определяется формулой $R_n(x) = f(x) - F_n(x)$

Получаем подход к оценке погрешности производной $r_n(x) = f^\prime(x) - F^\prime_n(x) = R^\prime_n(x)$ (2)

Применяя для численного дифференцирования на отрезке [_a;b_] интерполяционный полином, естественно строить на этом отрезке систему равноотстоящих узлов

$a = x_0 < x_1 < ... < x_n = b$ и они делятся на одинаковые части $x_{i+1} - x_i = h = const$ (3)

Пусть $t = (x - x_0)/h$

Интерполяционный многочлен в форме Лагранжа имеет вид:

$$ L_n(x) = \sum\limits_{i=1}^ny_i\frac{P_{n+1}(x)}{(x-x_i)P^\prime_{n+1}(x_i)} , (*) $$

где $P_{n+1}(x) = (x-x_0) ... (x - x_n)$

_Пояснение:_ вообще, формула Лагранжа имеет другой вид, но если каждый член домножить и разделить на $(x-x_i)$ и внимательно посмотреть, то получим именно это

$x-x_0 = ht$

$x - x_1 = x - x_0 - (x_1 -x_0) = ht - h = h(t-1)$

$x - x_2 = x - x_0 - (x_2 - x_0) = x - x_0 - (x_2 - x_0) = ht - 2h = h(t-2)$

…

$x - x_i = x - x_0 - ih = h(t - i)$

Пояснение: у нас расстояние $x_1x_0 = h$, при этом $x_2x_1 = h$, значит $x_2 - x_0 = 2h$ из этого следует, что $x_i - x_0 = ih$

имеем $P_{n+1}(x) = h^{n+1}t(t-1)...(t-n)$

$x_i - x_0 = hi$

$x_i - x_1 = x_i - x_0 - h = h(i -1)$

…

$x_i - x_n = x_i - x_0 - nh = h(i-n)$

$P^\prime(x) = \sum\limits_{i=1}^n (x-x_1)...(x-x_{i-1})(x-x_{i+1})...(x-x_n)$ ⇒

$$ P^\prime(x_i) = (x_i - x_0)...(x_i - x_{i-1})(x_i - x_{i+1})...(x_i - x_n) = h^ni(i-1)...1(-1)...(-(n-i)) = h^ni!(n-i)!(-1)^{n-i} $$

$$ L_n(x) = L_n(x_0 + (x - x_0)) = L(x_0 + th) $$

Получаем:

$L_n(x_0 + th) = \sum\limits_{i=1}^n \frac{(-1)^i(t-1)...(t-n)}{i!(n-i)!(t-i)}, (***)$

формула для погрешности интерполяционного полинома:

$$ f(x) - L_n(x) = \frac{f^{(n+1)(\xi)}}{(n+1)!}(x-x_0)...(x-x_n) $$

Объединяя последнее уравнение с $(***)$ умноженное и разделенное на n! получаем:

$$ f(x) = \frac{(-1)^nt(t-1)...(t-n)}{n!}\sum\limits_{i=1}^n \frac{(-1)^iC^i_n}{(t-i)} + \frac{f^{(n+1)(\xi)}}{(n+1)!}h^{n+1}t(t-1)...(t-n) $$

### Дифференцирование интерполяционной функции

перепишем многочлен в более удобном виде

Пусть $t^{[n+1]} = t(t-1)...(t-n)$

$$ f(x) = \frac{(-1)^nt^{[n+1]}}{n!}\sum\limits_{i=1}^n \frac{(-1)^iC^i_n}{(t-i)} + \frac{f^{(n+1)(\xi)}}{(n+1)!}h^{n+1}t^{[n+1]} $$

Будем дифференцировать по x. В данном случае у нас t(x) значит надо рассматривать дифференцирование, как суперпозицию функции

$$ \frac{dx}{dt} = \Big(\frac{x-x_0}{h}\Big)^\prime = \frac{1}{h} $$

перенесем h в левую часть и получим:

$$ hf^\prime(x) = \frac{(-1)^n}{n!}\frac{d}{dt}t^{[n+1]}\sum\limits_{i=1}^n \frac{(-1)^iC^i_n}{(t-i)} + \frac{f^{(n+1)(\xi)}}{(n+1)!}h^{n+1}\frac{d}{dt}t^{[n+1]} $$

![[Pasted image 20240612152808.png]]

![[Pasted image 20240612152801.png]]

![[Pasted image 20240612152754.png]]

### Проблема

Даже при не очень большом шуме могут наблюдаться значительные погрешности
# 33. Метод неопределённых коэффициентов
### Метод неопределённых коэффициентов - метод, используемый в математике для нахождения искомой функции в виде точной или приближённой линейной комбинации конечного или бесконечного набора базовых функций. Указанная линейная комбинация берётся с неизвестными коэффициентами, которые определяются тем или иным способом из условий рассматриваемой задачи. Обычно для них получается система алгебраических уравнений.

В математике для приближённого вычисления производных заданной таблично функции можно искать выражение значений производных через известные значения функции с помощью подходящего набора коэффициентов. Для этого можно использовать различные интерполяционные формулы или метод неопределённых коэффициентов.

## Идея

![[Pasted image 20240612152512.png]]

_Пояснение: Есть задача, в данной точке найти k-ю производную. Мы знаем значение функции в каких-то точках. Саму функцию мы можем аппроксимировать каким-либо полиномом, так, что в этих точках узлы интерполяции (они будут равны):_

$$ f(x) = P_n(x) = \sum\limits_{i=1}^n a_ix^i $$

Но тогда давайте попробуем выразить и k-ю производную функции через значения исходной функции $f(x)$ в в данной точке (6.11) Получим в точке полиномы. Чтобы они были равны необходимо и достаточно, чтобы коэффициенты при соответствующих степенях многочлена были равны

## Формальное объяснение метода

Итак, $f^{(k)}(\xi) \approx  \sum\limits_{i=1}^n C_if(x_i)$ значит можно переписать формулу, со знаком равенства в виде:

$$ f^{(k)}(\xi) =  
\sum\limits_{i=1}^M C_if(x_i) + R(f), (*) $$

где $R(f)$ - остаточный член интерполяции в точке $\xi$ (т.к. в ней исходная формула достигает равенства). Тогда получаем, что в узла интерполяции будет:

$$

\Big(\sum\limits_{j=1}^na_j\xi^i\Big)^{(k)} = \sum\limits_{i=1}^M C_i \sum\limits_{j=1}^na_jx_i^j, (**) $$

Далее мы знаем, что полиномы в данной точки равны <=> коэффициенты при соответствующих степенях равны. Но тогда найдем производные для всех коэффициентов в левой части, приравняем полиномы. и заметим, что $a_j$ при каждом их коэффициентов будут одинаковы, поэтому сократим их. (дифференцируем мы в левой части как полином, это не считается константой. Не забываем, что, если степень одночлена будет меньше k, то при дифференцировании порядком k он обнуляется)

$0 = c_1 + c_2 + ... + c_n$

$0 = c_1x_1 + c_2x_2 + ... + c_nx_n$

…

$0 = c_1x_1^{k-1} + c_2x_2^{k-1} + ... + c_nx_n^{k-1}$

$k! = c_1x_1^k + c_2x_2^k + ... + c_nx_n^k$

$(k+1)!\xi = c_1x_1^{k+1} + c_2x_2^{k+1} + ... + c_nx_n^{k+1}$

$\frac{(k+1)!}{2!}\xi^2 = c_1x_1^{k+2} + c_2x_2^{k+2} + ... + c_nx_n^{k+2}$

…

$n(n-1)... (n-k+1)\xi^{n-k} = c_1x_1^{n} + c_2x_2^{n} + ... + c_nx_n^{n}$

Если количество узлов интерполяции равно степени полинома, то получаем:

![[Pasted image 20240612152447.png]]

откуда находим $C_i$
# 34. Численное интегрирование. Постановка задачи
_Задача:_ есть функция, пусть нам надо найти ее площадь под графиком, при этом мы не можем от нее взять интеграл.

**Численное интегрирование** — вычисление значения определённого интеграла (как правило, приближённое).

Численное интегрирование применяется, когда:

1. Сама подынтегральная функция не задана аналитически. Например, она представлена в виде таблицы (массива) значений в узлах некоторой расчётной сетки.
2. Аналитическое представление подынтегральной функции известно, но её первообразная не выражается через аналитические функции.

Желательно, чтобы метод численного интегрирования обладал следующими свойствами:

- Универсальность. Функция f(x) может быть задана в виде “черного ящика”, как способ вычисления f(x) по данному x.
- Экономичность. Количество вычислений функции f(x) по возможности должно быть сведено к минимуму.
- Хорошая обусловленность. Неустранимые погрешности $\Delta f$ в значениях f(x) не должны приводить к значительной итоговой ошибке $\Delta I$.

Численное интегрирование может применяться для:

- интегрирования функций, известных только в некоторых точках, например, полученных в результате измерений
- Интегрирования сложных выражений, не имеющих элементарных первообразных, либо имеющих слишком громоздкие выражения для них
- построения методов численного решения уравнений в обыкновенных и частных производных (методы конечных элементов, интегро-интерполяционные методы)

![[Pasted image 20240612152254.png]]


Идея: исходя из определения интеграла, взять и разбить подинтегральную площадь на прямоугольники, вычислить их площади и сложить полученные значения. Получим нужную площадь (можно разбить на достаточно малые, чтобы удовлетворять погрешности)

![[Pasted image 20240612152243.png]]

![[Pasted image 20240612152233.png]]

![[Pasted image 20240612152224.png]]

_Пояснение:_

интерполяционный полином Лагранжа

$$ L_n(x) = \sum\limits_{i=1}^ny_i\frac{P_{n+1}(x)}{(x-x_i)P^\prime_{n+1}(x_i)} , (*) $$

в данном случае:

$$ C_k = \frac{P_{n+1}(x)}{(x-x_k)P^\prime_{n+1}(x_k)} $$

Домножили и разделили на (b-a), заменили нужное на $W_k$
# 35. Интерполяционные квадратурные формулы
### _Определение._ Квадратурная формула - формула для приближенного вычисления определенного интеграла

**Основная идея (на словах)**

Интерполяционные квадратурные формулы - это методы численного интегрирования, которые позволяют приближенно вычислить значение определенного интеграла функции на заданном интервале. Основная идея заключается в том, что мы аппроксимируем интегрируемую функцию с помощью интерполяционного полинома и затем вычисляем значение интеграла этого полинома, что дает нам приближенное значение исходного интеграла.

Для использования интерполяционных квадратурных формул мы разбиваем интервал интегрирования на несколько подынтервалов и выбираем точки, называемые узлами, на каждом подынтервале. Затем мы строим интерполяционный полином, который проходит через эти узлы, чтобы аппроксимировать функцию на каждом подынтервале.

После построения интерполяционного полинома мы вычисляем значение интеграла этого полинома на каждом подынтервале. Затем мы суммируем эти значения, чтобы получить приближенное значение исходного интеграла на всем интервале интегрирования.

![[Pasted image 20240612152104.png]]

![[Pasted image 20240612152055.png]]

### Формула Ньютона - Котеса

предположим, что нам необходимо вычислить интеграл:

$$ I = \int_{a}^{b} f(t) \,dt, (*) $$

Разобьем $[a, b]$ на малые равные части: $a = t_0 < t_1 < .. <t_n = b$

Выберем на каждом таком малом промежутке $\xi_i \in [t_{i-1}, t_i]:$

$$ f(\xi_i) \approx f(t), t \in [t_{i-1}, t_i], (**) $$

тогда на каждом промежутке будем считать, что $f(t) = f(\xi) = const$. Тогда:

$$ I_i = \int_{t_{i-1}}^{t_i} f(\xi_i) \,dt = f(\xi_i)(t_i - t_{i-1}) $$

Тогда получаем, что значение интеграла равно:

$$ I =\sum\limits_{i=1}^n I_i = \sum\limits_{i=1}^n f(\xi_i)(t_i - t_{i-1}), (***) $$

Если отрезки достаточно малы, то можно взять:

$$ \xi_i = f(\frac{t_i + t_{i-1}}{2}) $$

Вообще получается, что при таком разбиении мы разбили отрезок на 2n одинаковых частей, тогда за $\xi_i$ можно взять все точки, на которые мы разбили и в итоговом варианте получаем:

$$ \int_{a}^{b} f(x)\,dx \approx \sum\limits_{i=0}^s A_if(x_i) $$

$x_i = a+ ih, h = \frac{b-a}{s}$

Данная формула как раз-таки и называется формулой Ньютона-Котеса

Замечание: такие разбиения можно было и не проводить, можно было сразу расставить точки и сказать, что так будет, но просто для понимания было решено сделать так

![[Pasted image 20240612152031.png]]

Если отрезок достаточно близок к линейному, то:

$$ f(x_i) \approx \frac{f(x_{i+1} - f(x_i)}{2} $$

Откуда и получаем формулу трапеции:
![[Pasted image 20240612152009.png]]

По сути мы малые отрезки аппроксимируем линейной функцией. Если нам этого не достаточно, то мы можем аппроксимировать его параболой и получим формулу Симпсона (для нее берем 3 точки):

![[Pasted image 20240612152000.png]]

Формула 3/8 получается путем аппроксимации подынтегральной функции интерполяционным многочленом Лагранжа второй степени на четырех узлах интегрирования: a, a+h, a+2h, a+3h (по сути для каждого куска берем не 3 точки, а четыре)
![[Pasted image 20240612151944.png]]![[Pasted image 20240612151935.png]]![[Pasted image 20240612151925.png]]
# 36. Правило Рунге
### `Правило Рунге — правило оценки погрешности численных методов`

Идея заключается в следующем: если мы используем численный метод с некоторым шагом (например, шаг интегрирования или шаг сетки), то мы можем получить приближенное значение результата. Однако мы также можем повторить вычисления с более маленьким шагом и получить еще одно приближенное значение. Правило Рунге позволяет оценить погрешность численного метода путем сравнения результатов с разными шагами.

Правило Рунге широко используется в численных методах для проверки и улучшения точности результатов. Оно позволяет выбирать оптимальный шаг и контролировать погрешность численных вычислений.

![[Pasted image 20240612151738.png]]

![[Pasted image 20240612151724.png]]

## Более приближенное объяснение

Интеграл вычисляется по выбранной формуле (прямоугольников, трапеций, парабол Симпсона) при числе шагов, равном n, а затем при числе шагов, равном 2n. Погрешность вычисления значения интеграла при числе шагов, равном 2n, определяется по формуле Рунге:

$$ \Delta \approx \Theta|I_{2n} - I_{n}| $$

для метода левых и правых прямоугольников: $\Theta = 1$

для средних прямоугольников: $\Theta = \frac{1}{3}$

В общем случае;

$$ \Theta = \frac{1}{2^p - 1} $$

p - порядок погрешности используемого метода

Таким образом интеграл вычисляется для последовательности значений числа шагов $N = n_0, 2n_0, ... n_0$ - начальное число шагов, до тех пор, пока не выполнено условие $|\Delta_N|<\epsilon, \epsilon$ - заданная точность
# 37. Метод градиентного спуска
**Градиентный спуск, метод градиентного спуска** — численный метод нахождения _локального_ минимума или максимума функции с помощью движения вдоль градиента, один из основных численных методов современной оптимизации.

### Немного теории

Рассмотрим функцию f, считая для определенности, что она зависит от трех переменных _**x,y,z.**_ Вычислим ее частные производные _дf/дх,_ _дf/ду, дf/дz_ и образуем с их помощью вектор, который называют _градиентом_ функции:

$$ grad \ f(x, y, z) = \frac{\partial f}{\partial x}i + \frac{\partial f}{\partial y}j + \frac{\partial f}{\partial z}k $$

$i, j, k$ - единичные векторы, параллельные координатным осям

Частные производные характеризуют изменение функции f по каждой независимой переменной в отдельности. Образованный с их помощью вектор градиента дает общее представление о поведении функции в окрестности точки (_х, у, z)._ Направление этого вектора является направлением наиболее быстрого возрастания функции в данной точке.

Противоположное ему направление, которое называют _**антиградиентным**,_ представляет собой направление наиболее быстрого убывания функции

$|gradf(x,y,z)|$ - определяет скорость возрастания и убывания функции в направлении градиента и антиградиента. При переходе от одной точки к другой как направление градиента, так и его модуль, вообще говоря, меняются. Понятие градиента естественным образом переносится на функции любого числа переменных.

### Идея: двигаться к минимуму в направлении наиболее быстрого убывания функции, которое определяется антиградиентом.

Выберем каким-либо способом начальную точку, вычислим в ней градиент рассматриваемой функции и сделаем небольшой шаг в обратном, антиградиентном направлении. В результате мы придем в точку, в которой значение функции будет меньше первоначального. В новой точке повторим процедуру: снова вычислим градиент функции и сделаем шаг в обратном направлении. Продолжая этот процесс, мы будем двигаться в сторону убывания функции. Специальный выбор направления движения на каждом шаге позволяет надеяться на то, что в данном случае приближение к наименьшему значению функции будет более быстрым, чем в методе покоординатного спуска.

Метод градиентного спуска требует вычисления градиента целевой функции на каждом шаге. Если она задана аналитически, то это, как правило, не проблема: для частных производных, определяющих градиент, можно получить явные формулы. В противном случае частные производные в нужных точках приходится вычислять приближенно.

### Графическая иллюстрация алгоритма (поиск точки экстремума)

![[Pasted image 20240612151554.png]]
По сути, при применении данного метода мы задаем последовательность, с определенным размером шага, зависящий от какого-то параметра и начинаем спускаться рекурсивно

![[Pasted image 20240612151532.png]]

### Частный случай - квадратичные формы

![[Pasted image 20240612151519.png]]

![[Pasted image 20240612151507.png]]

### Проблема метода

1. Зависимость от исходного приближения: метод градиентного спуска может сходиться к локальному минимуму, а не к глобальному минимуму. Это значит, что выбор неправильного начального приближения может привести к плохому результату.

Более того, есть возможность застрять в локальных минимумах или седловых точках.

1. Проблема скорости сходимости: сходимость метода градиентного спуска может быть медленной, особенно если функция имеет плохо обусловленную или плоскую поверхность. В таких случаях может потребоваться большое количество итераций для достижения сходимости.
2. Чувствительность к выбросам: метод градиентного спуска чувствителен к выбросам в данных. Единичные аномалии могут сильно влиять на направление градиента и приводить к неправильным обновлениям параметров.
3. Вычислительная сложность: метод градиентного спуска требует вычисления градиента функции на каждой итерации, что может быть вычислительно затратным для больших и сложных моделей.

### Модификации
![[Pasted image 20240612151449.png]]
![[Pasted image 20240612151427.png]]